[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mastering Health Data Science Using R",
    "section": "",
    "text": "Preface\nThis book serves as an interactive introduction to R for public health and health data science students. Topics include data structures in R, exploratory analysis, distributions, hypothesis testing, regression analysis, and larger scale programming with functions and control flows. The presentation assumes knowledge with the underlying methodology and focuses instead on how to use R to implement your analysis.\nThis book is written using Quarto Book. You can download the Quarto files used to generate this book or a corresponding Jupyter notebook from the github repository. The github repository also contains a few cheat sheets.\nThis work is licensed under the Creative Commons Attribution 4.0 International CC BY 4.0.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Mastering Health Data Science Using R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book was written with the support of a Data Science Institute Seed Grant. Thanks to students Hannah Eglinton, Jialin Liu, Joanna Walsh, and Xinbei Yu for their help and feedback. Please contact Dr. Paul (alice_paul@brown.edu) with questions, suggested edits, or feedback.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/intro_to_r.html#why-r",
    "href": "book/intro_to_r.html#why-r",
    "title": "1  Getting Started with R",
    "section": "1.1 Why R?",
    "text": "1.1 Why R?\nWhat are some of the benefits of using R?\n\nR is built for statisticians and data analysts.\n\nR is open source.\n\nR has most of the latest statistical methods available.\n\nR is flexible.\n\nSince R is built for statisticians, it is built with data in mind. This comes in handy when we want to streamline how we process and analyze data. It also means that many statisticians working on new methods are publishing user-created packages in R, so R users have access to most methods of interest. R is also an interpreted language, which means that we do not have to compile our code into machine language first: this allows for simpler syntax and more flexibility when writing our code, which also makes it a great first programming language to learn.\nPython is another interpreted language often used for data analysis. Both languages feature simple and flexible syntax, but while python is more broadly developed for usage outside of data science and statistical analyses, R is a great programming language for those in health data science. I use both languages and find switching between them to be straightforward, but I do prefer R for anything related to data or statistical analysis.\n\n1.1.1 Installation of R and RStudio\nTo run R on your computer, you need to download and install R. This allows you to open the R application and run R code interactively. However, to get the most out of programming with R, you should install RStudio, which is an integrated development environment (IDE) for R. RStudio offers a nice environment for writing, editing, running, and debugging R code.\nEach chapter in this book is written as a Quarto document and can also be downloaded as a Jupyter notebook. You can open Quarto files in RStudio to run the code as you read and complete the practice questions and exercises.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "book/intro_to_r.html#the-r-console",
    "href": "book/intro_to_r.html#the-r-console",
    "title": "1  Getting Started with R",
    "section": "1.2 The R Console",
    "text": "1.2 The R Console\nThe R console provides our first intro to code in R. Figure 1.1 shows the console appearance when opened. You should see a blinking cursor - this where we can write our first line of code!\n\n\n\n\n\n\nFigure 1.1: The R Console.\n\n\n\nTo start, type 2+3 and press ENTER. You should see that 5 is printed below that code and that your cursor is moved to the next line.\n\n1.2.1 Basic Computations and Objects\nIn the previous example, we coded a simple addition. Try out some other basic calculations using the following operators:\n\nAddition: 5+6\n\nSubtraction: 7-2\n\nMultiplication: 2*3\n\nDivision: 6/3\n\nExponentiation: 4^2\n\nModulo: 100 %% 4\n\nFor example, use the modulo operator to find what 100 mod 4 is. It should return 0 since 100 is divisible by 4.\nIf we want to save the result of any computation, we need to create an object to store our value of interest. An object is simply a named data structure that allows us to reference that data structure. Objects are also commonly called variables. In the following code, we create an object x which stores the value 5 using the assignment operator &lt;-. The assignment operator assigns whatever is on the right hand side of the operator to the name on the left hand side. We can now reference x by calling its name. Additionally, we can update its value by adding 1. In the second line of code, the computer first finds the value of the right hand side by finding the current value of x before adding 1 and assigning it back to x.\n\nx &lt;- 2+3\nx &lt;- x+1\nx\n#&gt; [1] 6\n\nWe can create and store multiple objects by using different names. The following code creates a new object y that is one more than the value of x. We can see that the value of x is still 5 after running this code.\n\nx &lt;- 2+3\ny &lt;- x\ny &lt;- y + 1\nx\n#&gt; [1] 5\n\n\n\n1.2.2 Naming Conventions\nAs we start creating objects, we want to make sure we use good object names. Here are a few tips for naming objects effectively:\n\nStick to a single format. We use snake_case, which uses underscores between words (e.g. my_var, class_year).\n\nMake your names useful. Try to avoid using names that are too long\n(e.g. which_day_of_the_week) or do not contain enough information (e.g., x1, x2, x3).\n\nReplace unexplained numeric values with named objects. For example, if you need to do some calculations using 100 as the number of participants, create an object n_part with value 100 rather than repeatedly using the number. This makes the code easy to update and helps the user avoid possible errors.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "book/intro_to_r.html#rstudio-and-quarto",
    "href": "book/intro_to_r.html#rstudio-and-quarto",
    "title": "1  Getting Started with R",
    "section": "1.3 RStudio and Quarto",
    "text": "1.3 RStudio and Quarto\nIf we made a mistake in the code we type in the console, we would have to re-enter everything from the beginning. However, when we write code, we often want to be able to run it multiple times and develop it in stages. R scripts and R markdown files allow us to save all of our R code in files that we can update and re-run, which allows us to create reproducible and easy-to-share analyses. We now move to RStudio as our development environment to demonstrate creating an R script. When you open RStudio, there are multiple windows. Start by opening a new R file by going to File -&gt; New File -&gt; R Script. You should now see several windows as outlined in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: RStudio Layout and Panes.\n\n\n\n\n1.3.1 Panes\nThere are four panes shown by default:\n\nSource Pane - used for editing code files such as R scripts or Quarto documents.\nConsole Pane - used to show the live R session.\nEnvironment Pane - containing the Environment and History tabs, used to keep track of the current state.\nOutput Pane - containing the Plots and Packages tabs.\n\nThe source pane is the code editor window in the top left. This shows your currently blank R script. Add the following code to your .R file and save the file as “test.R”. Note that here we used snake_case to name our objects!\n\n# Calculate primary care physician to specialist ratio\npcp_phys &lt;- c(6300, 1080, 9297, 16433)\nspec_phys &lt;- c(6750, 837, 10517, 22984)\npcp_spec_ratio &lt;- 1000 * pcp_phys / spec_phys\n\nThe first line starts with # and does not contain any code. This is a comment line, which allows us to add context, intent, or extra information to help the reader understand our code. A good rule of thumb is that we want to write enough comments so that we could open our code in six months and be able to understand what we were doing. As we develop longer chunks of code, this will become more important.\nUnlike when we type code into the console, we can write multiple lines of code in our R script without running them. In order to run the code in the script, we need to tell RStudio we are ready to run it. To run a single line of code, we can either hit Ctrl+Enter when on that line or we can hit the Run button  button at the top right of the source pane. This copies the code to the R Console. Try this out to run the first line of code that defines pcp_phys. You can see that the line of code has been run in the console pane. Now check your environment pane. You should see that you have a new object representing the one we just created. This pane keeps track of all current objects. Run the second line of code and see how the environment updates. If you look at the History tab within this pane, you see the history of R commands run.\nIf we want to run all lines of code in our script, we can use the Source button . Before we do this, we will clear our environment. You can do this by clicking the broom  in the environment pane, which deletes all objects in the environment. Alternatively, you can go to Session -&gt; Restart R in the main menu, which restarts your whole R session. After clearing your environment, click the source button. You will see that in the R console it shows that it sourced this file. This means that it runs through all lines of code in this file. You can see that our objects have been added back into our environment.\n\nsource(\"test.R\")\n\nNow suppose we want to update our script by adding a plot. Copy the code in subsequent code chunk, save your updated file, and then source your file. You will see that the generated plot will appear in your output pane.\n\nplot(spec_phys, pcp_phys)\n\n\n\n\n\n\n\n\nUnlike R scripts which only contain R code, Quarto documents allow us to intersperse text and code. This breaks our code into chunks surrounded by text written in markdown. Every chapter in this book is available as a Quarto document. Try opening the Quarto file for Chapter 2 of this book. You will see the first code chunk as in Figure 1.3. In order to run the code in a code chunk, we can again use Ctrl+Enter to run a single line or selected lines. Additionally, we can use the Play button . This runs all the code within the chunk. We recommend using the available Quarto documents to follow along with the text. Writing your own Quarto documents is covered in Chapter 22.\n\n\n\n\n\n\nFigure 1.3: Example Code Chunk.\n\n\n\n\n\n1.3.2 Video Tour of RStudio and R Markdown\nThis video reviews the layout of RStudio as well as how to run and write R scripts and Quarto files.\n\n\n\n1.3.3 Calling Functions\nWhen we use R, we have access to all the functions available in base R. A function takes in one or more inputs and returns a single output object. Let’s first use the simple function exp(). This exponential function takes in one (or more) numeric values and exponentiates them. The following code computes \\(e^3\\).\n\nexp(3)\n#&gt; [1] 20.1\n\nSome other simple functions are shown that all convert a numeric input to an integer value. The ceiling() and floor() functions returns the ceiling and floor of your input, and the round() function round your input to the closest integer. Note that the round() function rounds a number ending in 0.5 to the closest even integer.\n\nceiling(3.7)\n#&gt; [1] 4\n\n\nfloor(3.7)\n#&gt; [1] 3\n\n\nround(2.5)\n#&gt; [1] 2\nround(3.5)\n#&gt; [1] 4\n\nIf we want to learn about a function, we can use the help operator ? by typing it in front of the function you are interested in: this brings up the documentation for that particular function. This documentation often tells you the usage of the function, the arguments (the object inputs), the value (information about the returned object), and gives some examples of how to use the function. For example, if we want to understand the difference between floor() and ceiling(), we can call ?floor and ?ceiling. This should bring up the documentation in your help window. We can then read that the floor function rounds a numeric input down to the nearest integer whereas the ceiling function rounds a numeric input up to the nearest integer.\n\n\n1.3.4 Working Directories and Paths\nLet’s try using another example function: read.csv(). This function reads in a comma-delimited file and returns the information as a data frame (try typing ?read.csv in the console to read more about this function). We learn more about data frames in Chapter 2. The first argument to this function is a file, which can be expressed as either a file name or a path to a file. First, download the file fake_names.csv from this book’s github repository. By default, R looks for the file in your current working directory. To find the working directory, you can run getwd(). You can see in the following output that my current working directory is where the book content is on my computer.\n\ngetwd()\n#&gt; [1] \"/Users/Alice/Dropbox/health-data-science-using-r/book\"\n\nYou can either move the .csv file to your current working directory and load it in, or you can specify the path to the .csv file. Another option is to update your working directory by using the setwd() function.\n\nsetwd('/Users/Alice/Dropbox/health-data-science-using-r/book/data')\n\nIf you receive an error that a file cannot be found, you most likely have the wrong path to the file or the wrong file name. In the following code, I chose to specify the path to the downloaded .csv file, saved this file to an object called df, and then printed that df object.\n\n# update this with the path to your file\ndf &lt;- read.csv(\"data/fake_names.csv\") \ndf\n#&gt;                  Name Age     DOB            City State\n#&gt; 1           Ken Irwin  37 6/28/85      Providence    RI\n#&gt; 2 Delores Whittington  56 4/28/67      Smithfield    RI\n#&gt; 3       Daniel Hughes  41 5/22/82      Providence    RI\n#&gt; 4         Carlos Fain  83  2/2/40          Warren    RI\n#&gt; 5        James Alford  67 2/23/56 East Providence    RI\n#&gt; 6        Ruth Alvarez  34 9/22/88      Providence    RI\n\nWe can see that df contains the information from the .csv file and that R has printed the first few observations of the data.\n\n\n1.3.5 Installing and Loading Packages\nWhen working with data frames, we often use the tidyverse package (Wickham 2023), which is actually a collection of R packages for data science applications. An R package is a collection of functions and/or sample data that allow us to expand on the functionality of R beyond the base functions. You can check whether you have the tidyverse package installed by going to the package tab in the Output Pane in RStudio or by running the following command, which displays all your installed packages.\ninstalled.packages()\nIf you don’t already have a package installed, you can install it using the install.packages() function. Note that you have to include single or double quotes around the package name when using this function. You only have to install a package one time.\ninstall.packages('tidyverse')\nThe function read_csv() is another function to read in comma-delimited files that is part of the readr package in the tidyverse (Wickham, Hester, and Bryan 2023). However, if we tried to use this function to load in our data, we would get an error that the function cannot be found. That is because we haven’t loaded in this package. To do so, we use the library() function. Unlike the install.packages() function, we do not have to use quotes around the package name when calling this library() function. When we load in a package, we see some messages. For example, in the following output we see that this package contains the functions filter() and lag() that are also functions in base R. In future chapters, we suppress these messages to make the chapter presentation nicer. After loading the tidyverse package, we can now use the read_csv() function.\n\nlibrary(tidyverse)\n\n\ndf &lt;- read_csv(\"data/fake_names.csv\", show_col_types=FALSE)\ndf\n#&gt; # A tibble: 6 × 5\n#&gt;   Name                  Age DOB     City            State\n#&gt;   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;\n#&gt; 1 Ken Irwin              37 6/28/85 Providence      RI   \n#&gt; 2 Delores Whittington    56 4/28/67 Smithfield      RI   \n#&gt; 3 Daniel Hughes          41 5/22/82 Providence      RI   \n#&gt; 4 Carlos Fain            83 2/2/40  Warren          RI   \n#&gt; 5 James Alford           67 2/23/56 East Providence RI   \n#&gt; 6 Ruth Alvarez           34 9/22/88 Providence      RI\n\nAlternatively, we could have told R where to locate the function by adding readr:: before the function. This tells it to find read_csv() function in the readr package. This can be helpful even if we have already loaded in the package, since sometimes multiple packages have functions with the same name.\n\ndf &lt;- readr::read_csv(\"data/fake_names.csv\", show_col_types = FALSE)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "book/intro_to_r.html#rstudio-projects-and-rstudio-global-options",
    "href": "book/intro_to_r.html#rstudio-projects-and-rstudio-global-options",
    "title": "1  Getting Started with R",
    "section": "1.4 RStudio Projects and RStudio Global Options",
    "text": "1.4 RStudio Projects and RStudio Global Options\nYou have now had a basic tour of RStudio. Once you close RStudio, you have the option of whether to store your current R environment. We highly recommend that you update your RStudio options to not save your workspace on exiting or load it on starting. This ensures that you have a fresh environment every time you open RStudio and helps you to create fully reproducible code and avoid possible errors or confusion.\n\n\n\n\n\n\nFigure 1.4: RStudio Global Options.\n\n\n\nNow when you re-open RStudio, it opens the files you had open previously and has your history of commands. This may become confusing when you are working on different files. RStudio projects allow us to create a folder that are associated with a single project. This means that when we open our project it sets the appropriate work directory for us and only open files related to that project. In order to create a new R project, such as one associated with this book, you can go to File -&gt; New Project. You can then choose whether to create a new directory or existing directory before selecting to create an empty project as in Figure 1.5. Within this directory you should see a .RProj file that allows you to re-open your project.\n\n\n\n\n\n\nFigure 1.5: Creating a New RStudio Project.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "book/intro_to_r.html#tips-and-reminders",
    "href": "book/intro_to_r.html#tips-and-reminders",
    "title": "1  Getting Started with R",
    "section": "1.5 Tips and Reminders",
    "text": "1.5 Tips and Reminders\nWe end this chapter with some final tips and reminders.\n\nKeyboard Shortcuts: RStudio has several useful keyboard shortcuts that make your programming experience more streamlined. It is worth getting familiar with some of the most common keyboard shortcuts using this book’s cheatsheet.\nAsking for help: Within R, you can use the ? operator or the help() function to pull up documentation on a given function. This documentation is also available online.\nFinding all objects: You can use the environment pane or ls() function to find all current objects. If you have an error that an object you are calling does not exist, take a look to find where you defined it.\nChecking packages: If you get an error that a function does not exist, check to make sure you have loaded that package using the library() function. The list of packages used in this book is given on the github repository homepage.\n\n\n\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#data-types",
    "href": "book/data_structures.html#data-types",
    "title": "2  Data Structures in R",
    "section": "2.1 Data Types",
    "text": "2.1 Data Types\nEach individual value in R has a type: logical, integer, double, or character. We can think of these as the building blocks of all data structures. We use the typeof() function to find the type of our vector ex_num, which shows that the value of ex_num is a double. A double is a numeric value with a stored decimal.\n\ntypeof(ex_num)\n#&gt; [1] \"double\"\n\nOn the other hand, an integer is a whole number that does not contain a decimal. We now create an integer object ex_int. To indicate to R that we want to restrict our values to integer values, we use an L after the number.\n\nex_int &lt;- 4L\ntypeof(ex_int)\n#&gt; [1] \"integer\"\n\nBoth ex_num and ex_int are numeric objects, but we can also work with two other types of objects: characters (e.g. “php”, “stats”) and booleans (e.g. TRUE, FALSE), also known as logicals.\n\nex_bool &lt;- TRUE\nex_char &lt;- \"Alice\"\n\ntypeof(ex_bool)\n#&gt; [1] \"logical\"\ntypeof(ex_char)\n#&gt; [1] \"character\"\n\nOne important characteristic of logical objects is that R also interprets them as 0/1. This means they can be added as in the following example: each TRUE has a value of 1 and each FALSE has a value of 0.\n\nTRUE + FALSE + TRUE\n#&gt; [1] 2\n\nTo create all of these objects, we used the assignment operator &lt;-, which we discussed in Chapter 1. You may see code elsewhere that uses an = instead. While = can also be used for assignment, it is more standard practice to use &lt;-.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#vectors",
    "href": "book/data_structures.html#vectors",
    "title": "2  Data Structures in R",
    "section": "2.2 Vectors",
    "text": "2.2 Vectors\nIn the previous examples, we created objects with a single value. R actually uses a vector of length 1 to store this information. Vectors are 1-dimensional data structures that can store multiple data values of the same type (e.g. character, boolean, or numeric).\n\n\n\n\n\n\nFigure 2.2: Vector Examples.\n\n\n\nWe can confirm this by using the is.vector() function, which returns whether or not the inputted argument is a vector.\n\nis.vector(ex_bool)\n#&gt; [1] TRUE\n\nOne way to create a vector with multiple values is to use the combine function c(). In the following code, we create two vectors: one with the days of the week and one with the amount of rain on each day. The first vector has all character values, and the second one has all numeric values.\n\ndays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nrain &lt;- c(5, 0.1, 0, 0, 0.4)\n\nRemember, a vector can only store values of the same type. Because of this, in the following code, R automatically converts the numeric value to be a character in order to store these values in a vector together.\n\nc(\"Monday\", 5)\n#&gt; [1] \"Monday\" \"5\"\n\nThe class() function returns the data structure of an object. If we check the classes of these two objects using the class() function, we see that R tells us that the first is a character vector and the second is a numeric vector. This matches the data type in this case.\n\nclass(days)\n#&gt; [1] \"character\"\nclass(rain)\n#&gt; [1] \"numeric\"\n\nWhat happens when we create an empty vector? What is the class?\n\nex_empty &lt;- c()\nclass(ex_empty)\n#&gt; [1] \"NULL\"\n\nIn this case, there is no specified type yet. If we wanted to specify the type, we could make an empty vector using the vector() function.\n\nex_empty &lt;- vector(mode = \"numeric\")\nclass(ex_empty)\n#&gt; [1] \"numeric\"\n\nAnother way to create a vector is with the rep() or seq() functions. The first function rep(x, times) takes in a vector x and a number of times times and outputs x repeated that many times. Let’s try this with a single value. The second function seq(from, to, step) takes in a numeric starting value from, end value to, and step size step and returns a sequence from from in increments of step until a maximum value of to is reached.\n\nrep(0, 5)\n#&gt; [1] 0 0 0 0 0\nrep(\"Monday\", 4)\n#&gt; [1] \"Monday\" \"Monday\" \"Monday\" \"Monday\"\nseq(1, 5, 1)\n#&gt; [1] 1 2 3 4 5\nseq(0, -10, -2)\n#&gt; [1]   0  -2  -4  -6  -8 -10\n\n\n2.2.1 Indexing a Vector\nOnce we have a vector, we may want to access certain values stored in that vector. To do so, we index the vector using the position of each value: the first value in the vector has index 1, the second value has index 2, etc. When we say a vector is 1-dimensional, we mean that we can define the position of each value by a single index. To index the vector, we then use square brackets [] after the vector name and provide the position. We use these indices to find the value at index 1 and the value at index 4.\n\ndays[1]\n#&gt; [1] \"Monday\"\ndays[4]\n#&gt; [1] \"Thursday\"\n\nWe can either access a single value or a subset of values using a vector of indices. Let’s see what happens when we use a vector of indices c(1,4) and then try using -c(1,4) and see what happens then. In the first case, we get the values at index 1 and at index 4. In the second case, we get all values except at those indices. The - indicates that we want to remove rather than select these indices.\n\ndays[c(1, 4)]\n#&gt; [1] \"Monday\"   \"Thursday\"\ndays[-c(1, 4)]\n#&gt; [1] \"Tuesday\"   \"Wednesday\" \"Friday\"\n\nHowever, always indexing by the index value can sometimes be difficult or inefficient. One extra feature of vectors is that we can associate a name with each value. In the subsequent code, we update the names of the vector rain to be the days of the week and then find Friday’s rain count by indexing with the name.\n\nnames(rain) &lt;- days\nprint(rain)\n#&gt;    Monday   Tuesday Wednesday  Thursday    Friday \n#&gt;       5.0       0.1       0.0       0.0       0.4\nrain[\"Friday\"]\n#&gt; Friday \n#&gt;    0.4\n\nThe last way to index a vector is to use TRUE and FALSE values. If we have a vector of booleans that is the same length as our original vector, then this returns all the values that correspond to a TRUE value. For example, indexing the days vector by the logical vector ind_bools returns its first and fourth values. We will see more about using logic to access certain values later on.\n\nind_bools &lt;- c(TRUE, FALSE, FALSE, TRUE, FALSE)\ndays[ind_bools]\n#&gt; [1] \"Monday\"   \"Thursday\"\n\n\n\n2.2.2 Editing a Vector and Calculations\nThe mathematical operators we saw in the last chapter (+, -, *, /, ^, %%) can all be applied to numeric vectors and are applied element-wise. That is, in the code examples, the two vectors are added together by index. This holds true for some of the built-in math functions as well:\n\nexp() - exponential\nlog() - log\nsqrt() - square root\nabs() - absolute value\nround() - round to nearest integer value\nceiling() - round up to the nearest integer value\nfloor() - round down to the nearest integer value\nsignif(, dig) - round to dig number of significant digits\n\n\nc(1, 2, 3) + c(1, 1, 1)\n#&gt; [1] 2 3 4\nc(1, 2, 3) + 1 # equivalent to the code above\n#&gt; [1] 2 3 4\nsqrt(c(1, 4, 16))\n#&gt; [1] 1 2 4\nsignif(c(0.23, 0.19), dig = 1)\n#&gt; [1] 0.2 0.2\n\nAfter we create a vector, we may need to update its values. For example, we may want to change a specific value. We can do so using indexing. We then update the rain value for Friday using the assignment operator.\n\nrain[\"Friday\"] &lt;- 0.5\nrain\n#&gt;    Monday   Tuesday Wednesday  Thursday    Friday \n#&gt;       5.0       0.1       0.0       0.0       0.5\n\nFurther, we may need to add extra entries. We can do so using the c() function again but this time passing in the vector we want to add to as our first argument. This creates a single vector with all previous and new values. In the following code, we add two days to both vectors and then check the length of the updated vector rain. The length() function returns the length of a vector.\n\nlength(rain)\n#&gt; [1] 5\ndays &lt;- c(days, \"Saturday\", \"Sunday\") # add the weekend with no rain\nrain &lt;- c(rain,0,0)\nlength(rain)\n#&gt; [1] 7\n\nWe can also call some useful functions on vectors. For example, the sum(), max(), and min() functions returns the sum, maximum value, and minimum value of a vector, respectively.\n\n\n2.2.3 Practice Question\nCreate a vector of the odd numbers from 1 to 11 using the seq() function. Then, find the third value in the vector using indexing, which should have value 5.\n\n# Insert your solution here:\n\n\n\n2.2.4 Common Vector Functions\nThe following list contains some of the most common vector functions that are available in base R. All of these functions assume that the vector is numeric. If we pass the function a logical vector, R converts the vector to 0/1 first, and if we pass the function a character vector, R gives us an error message.\n\nsum() - summation\nmedian() - median value\nmean() - mean\nsd() - standard deviation\nvar() - variance\nmax() - maximum value\nwhich.max() - index of the first element with the maximum value\nmin() - minimum value\nwhich.min() - index of the first element with the minimum value\n\nTry these out using the vector rain. Note that R is case sensitive - Mean() is considered different from mean(), so if we type Mean(rain) R tells us that it cannot find this function.\n\nmean(rain)  \n#&gt; [1] 0.8\nmin(rain) \n#&gt; [1] 0\nwhich.min(rain) \n#&gt; Wednesday \n#&gt;         3\n\nWe may also be interested in the order of the values. The sort() function sorts the values of a vector, whereas the order() function returns the permutation of the elements to be in sorted order. The last line of following code sorts the days of the week from smallest to largest rain value.\n\nrain\n#&gt;    Monday   Tuesday Wednesday  Thursday    Friday                     \n#&gt;       5.0       0.1       0.0       0.0       0.5       0.0       0.0\norder(rain)\n#&gt; [1] 3 4 6 7 2 5 1\ndays[order(rain)]\n#&gt; [1] \"Wednesday\" \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Tuesday\"  \n#&gt; [6] \"Friday\"    \"Monday\"\n\nBoth of these functions have an extra possible argument decreasing, which has a default value of FALSE. We can specify this to be TRUE to find the days of the week sorted from largest to smallest rainfall. Note that in the case of ties, the first occurrence gets the higher rank.\n\ndays[order(rain, decreasing=TRUE)]\n#&gt; [1] \"Monday\"    \"Friday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\" \n#&gt; [6] \"Saturday\"  \"Sunday\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#factors",
    "href": "book/data_structures.html#factors",
    "title": "2  Data Structures in R",
    "section": "2.3 Factors",
    "text": "2.3 Factors\nA factor is a special kind of vector that behaves like a regular vector except that it represents values from a category. In particular, a factor keeps track of all possible values of that category, which are called the levels of the factor. Factors are especially helpful when we start getting into data analysis and have categorical columns. The as.factor() function converts a vector to a factor.\n\ndays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Monday\", \n          \"Thursday\", \"Wednesday\")\ndays_fct &lt;- as.factor(days)\n\nclass(days_fct)\n#&gt; [1] \"factor\"\nlevels(days_fct)\n#&gt; [1] \"Monday\"    \"Thursday\"  \"Tuesday\"   \"Wednesday\"\n\nIn the previous example, we did not specify the possible levels for our column. Instead, R found all values in the vector days and set these equal to the levels of the factor. Because of this, if we try to change one of the levels to ‘Friday’, we get an error. Uncomment the following line to see the error message.\n\n#days_fct[2] &lt;- \"Friday\"   \n\nWe can avoid this error by specifying the levels using the factor() function instead of the as.factor() function.\n\ndays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Monday\", \"Thursday\", \n          \"Wednesday\")\ndays_fct &lt;- factor(days, \n               levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \n                          \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\n\nclass(days_fct)\n#&gt; [1] \"factor\"\nlevels(days_fct)\n#&gt; [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n#&gt; [6] \"Saturday\"  \"Sunday\"\ndays_fct[2] &lt;- \"Friday\"\n\nFactors can also be used for numeric vectors. For example, we might have a vector that is 0/1 that represents whether or not a day is a weekend. This can also only take on certain values (0 or 1).\n\nweekend &lt;- as.factor(c(1, 0, 0, 0, 1, 1))\nlevels(weekend)\n#&gt; [1] \"0\" \"1\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#matrices",
    "href": "book/data_structures.html#matrices",
    "title": "2  Data Structures in R",
    "section": "2.4 Matrices",
    "text": "2.4 Matrices\nMatrices are similar to vectors in that they store data of the same type. However, matrices are two-dimensional consisting of both rows and columns, as opposed to one-dimensional vectors.\n\n\n\n\n\n\nFigure 2.3: Matrix Example.\n\n\n\nIn the following code, we create a matrix reporting the daily rainfall over multiple weeks. We can create a matrix using the matrix(data, nrow, ncol, byrow) function. This creates a nrow by ncol matrix from the vector data values filling in by row if byrow is TRUE and by column otherwise. Run the code. Then, change the last argument to byrow=FALSE and see what happens to the values.\n\nrainfall &lt;- matrix(c(5, 6, 0.1, 3, 0, 1, 0, 1, 0.4, 0.2, \n                     0.5, 0.3, 0, 0), \n                   ncol=7, nrow=2, byrow=TRUE)\nrainfall\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#&gt; [1,]    5  6.0  0.1  3.0  0.0    1    0\n#&gt; [2,]    1  0.4  0.2  0.5  0.3    0    0\n\nWe can find the dimensions of a matrix using the nrow(), ncol(), or dim() functions, which return the number of rows, the number of columns, and both the number of rows and columns, respectively.\n\nnrow(rainfall)\n#&gt; [1] 2\nncol(rainfall)\n#&gt; [1] 7\ndim(rainfall)\n#&gt; [1] 2 7\n\n\n2.4.1 Indexing a Matrix\nSince matrices are two-dimensional, a single value is indexed by both its row number and its column number. This means that to access a subset of values in a matrix, we need to provide row and column indices. In the subsequent code, we access a single value in the 1st row and the 4th column. The first value is always the row index and the second value is always the column index.\n\nrainfall[1, 4]\n#&gt; [1] 3\n\nAs before, we can also provide multiple indices to get multiple values. In the subsequent example, we choose multiple columns but we can also choose multiple rows (or multiple rows and multiple columns).\n\nrainfall[1, c(4, 5, 7)]\n#&gt; [1] 3 0 0\n\nAs with vectors, we can also use booleans to index a matrix by providing boolean values for the rows and/or columns. Note that in the following example we give a vector for the row indices and no values for the columns. Since we did not specify any column indices, this selects all of them.\n\nrainfall[c(FALSE, TRUE), ]\n#&gt; [1] 1.0 0.4 0.2 0.5 0.3 0.0 0.0\n\nLet’s do the opposite and select some columns and all rows.\n\nrainfall[ ,c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE)]\n#&gt;      [,1] [,2]\n#&gt; [1,]    5  6.0\n#&gt; [2,]    1  0.4\n\nAs with vectors, we can specify row names and column names to access entries instead of using indices. The colnames() and rownames() functions allow us to specify the column and row names, respectively.\n\ncolnames(rainfall) &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \n                        \"Friday\", \"Saturday\", \"Sunday\")\nrownames(rainfall) &lt;- c(\"Week1\", \"Week2\")\nrainfall[\"Week1\", c(\"Friday\",\"Saturday\")]\n#&gt;   Friday Saturday \n#&gt;        0        1\n\n\n\n2.4.2 Editing a Matrix\nIf we want to change the values in a matrix, we need to first index those values and then assign them the new value(s). In the subsequent code chunks, we change a single entry to be 3 and then update several values to all be 0. Note that we do not provide multiple 0’s on the right-hand side, as R infers that all values should be set to 0.\n\nrainfall[\"Week1\", \"Friday\"] &lt;- 3\n\n\nrainfall[\"Week1\", c(\"Monday\", \"Tuesday\")] &lt;- 0\nprint(rainfall)\n#&gt;       Monday Tuesday Wednesday Thursday Friday Saturday Sunday\n#&gt; Week1      0     0.0       0.1      3.0    3.0        1      0\n#&gt; Week2      1     0.4       0.2      0.5    0.3        0      0\n\nFurther, we can append values to our matrix by adding rows or columns through the rbind() and cbind() functions. The first function appends a row (or multiple rows) to a matrix and the second appends a column (or multiple columns). Note that in the following example I provide a row and column name when passing in the additional data. If I hadn’t specified these names, then those rows and columns would not be named.\n\nrainfall &lt;- rbind(rainfall, \"Week3\" = c(0.4, 0.0, 0.0, 0.0, 1.2, 2.2, \n                                        0.0))\nrainfall &lt;- cbind(rainfall, \"Total\" = c(7.1, 2.4, 3.8))\nprint(rainfall)\n#&gt;       Monday Tuesday Wednesday Thursday Friday Saturday Sunday Total\n#&gt; Week1    0.0     0.0       0.1      3.0    3.0      1.0      0   7.1\n#&gt; Week2    1.0     0.4       0.2      0.5    0.3      0.0      0   2.4\n#&gt; Week3    0.4     0.0       0.0      0.0    1.2      2.2      0   3.8\n\nHere is an example where we bind two matrices by column. Note that whenever we bind two matrices together, we have to be sure that their dimensions are compatible and that they are of the same type.\n\nA &lt;- matrix(c(1, 2, 3, 4), nrow=2)\nB &lt;- matrix(c(5, 6, 7, 8), nrow=2)\nC &lt;- cbind(A, B)\nC\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    3    5    7\n#&gt; [2,]    2    4    6    8\n\nAs with vectors, most mathematical operators (+, -, *, / etc.) are applied element-wise in R.\n\nA+B\n#&gt;      [,1] [,2]\n#&gt; [1,]    6   10\n#&gt; [2,]    8   12\n\n\nexp(C)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,] 2.72 20.1  148 1097\n#&gt; [2,] 7.39 54.6  403 2981\n\n\n\n2.4.3 Practice Question\nCreate a 3x4 matrix of all 1’s using the rep() and matrix() functions. Then select the first and third columns using indexing which returns a 3x2 matrix of all ones.\n\n# Insert your solution here:",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#data-frames",
    "href": "book/data_structures.html#data-frames",
    "title": "2  Data Structures in R",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\nMatrices can store data like the rainfall data, where everything is of the same type. However, if we want to capture more complex data records, we also want to allow for different measurement types: this is where data frames come in. A data frame is like a matrix in that data frames are two-dimensional, but unlike matrices, data frames allow for each column to be a different type. In this case, each row corresponds to a single data entry (or observation) and each column corresponds to a different variable.\n\n\n\n\n\n\nFigure 2.4: Data Frame Example.\n\n\n\nFor example, suppose that, for every day in a study, we want to record the temperature, rainfall, and day of the week. Temperature and rainfall can be numeric values, but day of the week is character type. We create a data frame using the data.frame() function. Note that I am providing column names for each vector (column).\nThe head() function prints the first six rows of a data frame (to avoid printing very large datasets). In our case, all the data is shown because we only created four rows. The column names are displayed as well as their type. By contrast, the tail() function prints the last six rows of a data frame.\n\nweather_data &lt;- data.frame(day_of_week = c(\"Monday\", \"Tuesday\",\n                                           \"Wednesday\", \"Monday\"), \n                           temp = c(70, 62, 75, 50), \n                           rain = c(5, 0.1, 0.0, 0.5))\nhead(weather_data)\n#&gt;   day_of_week temp rain\n#&gt; 1      Monday   70  5.0\n#&gt; 2     Tuesday   62  0.1\n#&gt; 3   Wednesday   75  0.0\n#&gt; 4      Monday   50  0.5\n\nThe dim(), nrow(), and ncol() functions return the dimensions, number of rows, and number of columns of a data frame, respectively.\n\ndim(weather_data)\n#&gt; [1] 4 3\nnrow(weather_data)\n#&gt; [1] 4\nncol(weather_data)\n#&gt; [1] 3\n\nThe column names can be found (or assigned) using the colnames() or names() function. These were specified when I created the data. On the other hand, the row names are currently the indices.\n\ncolnames(weather_data)\n#&gt; [1] \"day_of_week\" \"temp\"        \"rain\"\nrownames(weather_data)\n#&gt; [1] \"1\" \"2\" \"3\" \"4\"\nnames(weather_data)\n#&gt; [1] \"day_of_week\" \"temp\"        \"rain\"\n\nWe update the row names to be more informative as with a matrix using the rownames() function.\n\nrownames(weather_data) &lt;- c(\"6/1\", \"6/2\", \"6/3\", \"6/8\")\nhead(weather_data)\n#&gt;     day_of_week temp rain\n#&gt; 6/1      Monday   70  5.0\n#&gt; 6/2     Tuesday   62  0.1\n#&gt; 6/3   Wednesday   75  0.0\n#&gt; 6/8      Monday   50  0.5\n\n\n2.5.1 Indexing a Data Frame\nWe can select elements of the data frame using its indices in the same way as we did with matrices. In the subsequent code, we access a single value and then a subset of our data frame. The subset returned is itself a data frame. Note that the second line returns a data frame.\n\nweather_data[1, 2]\n#&gt; [1] 70\nweather_data[1, c(\"day_of_week\", \"temp\")]\n#&gt;     day_of_week temp\n#&gt; 6/1      Monday   70\n\nAnother useful way to access the columns of a data frame is by using the $ accessor and the column name.\n\nweather_data$day_of_week\n#&gt; [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Monday\"\nweather_data$temp\n#&gt; [1] 70 62 75 50\n\nThe column day_of_week is a categorical column, but it can only take on a limited number of values. For this kind of column, it is often useful to convert that column to a factor as we did before.\n\nweather_data$day_of_week &lt;- factor(weather_data$day_of_week)\nlevels(weather_data$day_of_week)\n#&gt; [1] \"Monday\"    \"Tuesday\"   \"Wednesday\"\n\n\n\n2.5.2 Editing a Data Frame\nAs with matrices, we can change values in a data frame by indexing those entries.\n\nweather_data[1, \"rain\"] &lt;- 2.2\nweather_data\n#&gt;     day_of_week temp rain\n#&gt; 6/1      Monday   70  2.2\n#&gt; 6/2     Tuesday   62  0.1\n#&gt; 6/3   Wednesday   75  0.0\n#&gt; 6/8      Monday   50  0.5\n\nThe rbind() functions and cbind() functions also work for data frames in the same way as for matrices. However, another way to add a column is to directly use the $ accessor. We add a categorical column called aq_warning, indicating whether there was an air quality warning that day.\n\nweather_data$aq_warning &lt;- as.factor(c(1, 0, 0, 0))\nweather_data\n#&gt;     day_of_week temp rain aq_warning\n#&gt; 6/1      Monday   70  2.2          1\n#&gt; 6/2     Tuesday   62  0.1          0\n#&gt; 6/3   Wednesday   75  0.0          0\n#&gt; 6/8      Monday   50  0.5          0\n\n\n\n2.5.3 Practice Question\nAdd a column to weather_data called air_quality_index using the rep() function so that all values are NA (the missing value in R). Then, index the second value of this column and set the value to be 57. The result should look like Figure 2.5.\n\n\n\n\n\n\nFigure 2.5: Air Quality Data.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#lists",
    "href": "book/data_structures.html#lists",
    "title": "2  Data Structures in R",
    "section": "2.6 Lists",
    "text": "2.6 Lists\nA data frame is actually a special type of another data structure called a list, which is a collection of objects under the same name. These objects can be vectors, matrices, data frames, or even other lists! There does not have to be any relation in size, type, or other attribute between different members of the list. We create an example list using the list() function, which takes in a series of objects. What are the types of each element of the following list?\n\nex_list &lt;- list(\"John\", c(\"ibuprofen\", \"metformin\"), \n                c(136, 142, 159))\nprint(ex_list)\n#&gt; [[1]]\n#&gt; [1] \"John\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"ibuprofen\" \"metformin\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 136 142 159\n\nWe can access each element using the index. Note unlike indexing vectors, using single brackets will return another list which is a sub-list containing the object at that index.\n\nprint(class(ex_list[2]))\n#&gt; [1] \"list\"\nex_list[2]\n#&gt; [[1]]\n#&gt; [1] \"ibuprofen\" \"metformin\"\n\nWe can access the actual numeric vector at this index using double brackets.\n\nex_list[[2]]\n#&gt; [1] \"ibuprofen\" \"metformin\"\n\nMore often, however, it is useful to name the elements of the list for easier access. Let’s create this list again but this time give names to each object.\n\nex_list &lt;- list(name=\"John\", \n                medications = c(\"ibuprofen\", \"metformin\"), \n                past_weights = c(136, 142, 159))\nprint(ex_list)\n#&gt; $name\n#&gt; [1] \"John\"\n#&gt; \n#&gt; $medications\n#&gt; [1] \"ibuprofen\" \"metformin\"\n#&gt; \n#&gt; $past_weights\n#&gt; [1] 136 142 159\nex_list$medications\n#&gt; [1] \"ibuprofen\" \"metformin\"\n\nTo edit a list, we can use indexing to access different objects in the list and then assign them to new values. Additionally, we can add objects to the list using the $ accessor.\n\nex_list$supplements &lt;- c(\"vitamin D\", \"biotin\")\nex_list$supplements[2] &lt;- \"collagen\"\nex_list\n#&gt; $name\n#&gt; [1] \"John\"\n#&gt; \n#&gt; $medications\n#&gt; [1] \"ibuprofen\" \"metformin\"\n#&gt; \n#&gt; $past_weights\n#&gt; [1] 136 142 159\n#&gt; \n#&gt; $supplements\n#&gt; [1] \"vitamin D\" \"collagen\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#recap-video",
    "href": "book/data_structures.html#recap-video",
    "title": "2  Data Structures in R",
    "section": "2.7 Recap Video",
    "text": "2.7 Recap Video",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/data_structures.html#exercises",
    "href": "book/data_structures.html#exercises",
    "title": "2  Data Structures in R",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\nRecreate the data frame in Figure 2.6 in R, where temperature and co2 represent the average temperature in Fahrenheit and the average \\(\\text{CO}_2\\) concentrations in \\(\\text{mg}/\\text{m}^3\\) for the month of January 2008, and name it city_air_quality.\n\n\n\n\n\n\n\nFigure 2.6: City Air Quality Data.\n\n\n\n\nCreate a character vector named precipitation with entries Yes or No indicating whether or not there was more precipitation than average in January 2008 in these cities (you can make this information up yourself). Then, append this vector to the city_air_quality data frame as a new column.\nConvert the categorical column precipitation to a factor. Then, add a row to the data frame city_air_quality using the rbind() function to match Figure 2.7.\n\n\n\n\n\n\n\nFigure 2.7: Updated City Air Quality Data.\n\n\n\n\nUse single square brackets to access the precipitation and \\(\\text{CO}_2\\) concentration entries for San Francisco and Paris in your data frame. Then, create a list city_list which contains two lists, one for San Francisco and one for Paris, where each inner list contains the city name, precipitation, and \\(\\text{CO}_2\\) concentration information for that city.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Structures in R</span>"
    ]
  },
  {
    "objectID": "book/working_data_files.html#importing-and-exporting-data",
    "href": "book/working_data_files.html#importing-and-exporting-data",
    "title": "3  Working with Data Files in R",
    "section": "3.1 Importing and Exporting Data",
    "text": "3.1 Importing and Exporting Data\nThe data we use in this chapter contains information about patients who visited one of the University of Pittsburgh’s seven pain management clinics. This includes patient-reported pain assessments using the Collaborative Health Outcomes Information Registry (CHOIR) at baseline and at a 3-month follow-up (Alter et al. 2021). You can use the help operator ?pain to learn more about the source of this data and to read its column descriptions. Since this data is available in our R package, we can use the data() function to load this data into our environment. Note that this data has 21,659 rows and 92 columns.\n\ndata(pain)\ndim(pain)\n#&gt; [1] 21659    92\n\nIn general, the data you will be using is not available in R packages and will instead exist in one or more data files on your personal computer. In order to load in this data to R, you need to use the function that corresponds to the file type you have. For example, you can load a .csv file using the read.csv() function in base R or using the read_csv() function from the readr package, both of which were shown in Chapter 1. As an example, we load the fake_names.csv dataset using both of these functions. Looking at the print output, we can see that there is slight difference in the data structure and data types storing the data between these two functions. The function read.csv() loads the data as a data frame, whereas the function read_csv() loads the data as a spec_tbl_df, a special type of data frame called a tibble that is used by the tidyverse packages. We cover this data structure in more detail in Chapter 5. For now, note that you can use either function to read in a .csv file.\n\nread.csv(\"data/fake_names.csv\")\n#&gt;                  Name Age     DOB            City State\n#&gt; 1           Ken Irwin  37 6/28/85      Providence    RI\n#&gt; 2 Delores Whittington  56 4/28/67      Smithfield    RI\n#&gt; 3       Daniel Hughes  41 5/22/82      Providence    RI\n#&gt; 4         Carlos Fain  83  2/2/40          Warren    RI\n#&gt; 5        James Alford  67 2/23/56 East Providence    RI\n#&gt; 6        Ruth Alvarez  34 9/22/88      Providence    RI\n\n\nreadr::read_csv(\"data/fake_names.csv\", show_col_types=FALSE)\n#&gt; # A tibble: 6 × 5\n#&gt;   Name                  Age DOB     City            State\n#&gt;   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;\n#&gt; 1 Ken Irwin              37 6/28/85 Providence      RI   \n#&gt; 2 Delores Whittington    56 4/28/67 Smithfield      RI   \n#&gt; 3 Daniel Hughes          41 5/22/82 Providence      RI   \n#&gt; 4 Carlos Fain            83 2/2/40  Warren          RI   \n#&gt; 5 James Alford           67 2/23/56 East Providence RI   \n#&gt; 6 Ruth Alvarez           34 9/22/88 Providence      RI\n\nIn addition to loading data into R, you may also want to save data from R into a data file you can access later or share with others. To write a data frame from R to a .csv file, you can use the write.csv() function. This function has three key arguments: the first argument is the data frame in R that you want to write to a file, the second argument is the file name or the full file path where you want to write the data, and the third argument is whether or not you want to include the row names as an extra column. In this case, we do not include row names. If you do not specify a file path, R saves the file in our current working directory.\n\ndf &lt;- data.frame(x = c( 1, 0, 1), y = c(\"A\", \"B\", \"C\"))\nwrite.csv(df, \"data/test.csv\", row.names=FALSE)\n\nIf your data is not in a .csv file, you may need to use another package to read in the file. The two most common packages are the readxl package (Wickham and Bryan 2023), which makes it easy to read in Excel files, and the haven package (Wickham, Miller, and Smith 2023), which can import SAS, SPSS, and Stata files. For each function, you need to specify the file path to the data file.\n\nTab Delimited Files: You can read in a tab-separated .txt file using the read.delim() function in base R.\nExcel Files: You can read in a .xls or .xlsx file using readxl::read_excel(), which allows you to specify a sheet and/or cell range within a file (e.g. read_excel('test.xlsx', sheet=\"Sheet1\")).\nSAS: haven::read_sas() reads in .sas7bdat or .sas7bcat files, haven::read_xpt() reads in SAS transport files.\nStata: haven::read_dta() reads in .dta files.\nSPSS: haven::read_spss() reads in .spss files.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Working with Data Files in R</span>"
    ]
  },
  {
    "objectID": "book/working_data_files.html#summarizing-and-creating-data-columns",
    "href": "book/working_data_files.html#summarizing-and-creating-data-columns",
    "title": "3  Working with Data Files in R",
    "section": "3.2 Summarizing and Creating Data Columns",
    "text": "3.2 Summarizing and Creating Data Columns\nWe now look at the data we have loaded into the data frame called pain. We use the head() function to print the first six rows. However, note that we have so many columns that all not of the columns are displayed! For those that are displayed, we can see the data type for each column under the column name. For example, we can see that the column PATIENT_NUM is a numeric column of type dbl. Because patients identification numbers are technically nominal in nature, we might consider whether we should make convert this column to a factor or a character representation later on. We can use the names() function to print all the column names. Note that columns X101 to X238 correspond to numbers on a body pain map (see the data documentation for the image of this map). Each of these columns has a 1 if the patient indicated that they have pain in that corresponding body part and a 0 otherwise.\n\nhead(pain)\n#&gt; # A tibble: 6 × 92\n#&gt;   PATIENT_NUM  X101  X102  X103  X104  X105  X106  X107  X108  X109\n#&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       13118     0     0     0     0     0     0     0     0     0\n#&gt; 2       21384     0     0     0     0     0     0     0     0     0\n#&gt; 3        6240     0     0     0     0     0     0     0     0     0\n#&gt; 4        1827     0     0     0     0     0     0     0     0     0\n#&gt; 5       11309     0     0     0     0     0     0     0     0     0\n#&gt; 6       11093     0     0     0     0     0     0     0     0     0\n#&gt; # ℹ 82 more variables: X110 &lt;dbl&gt;, X111 &lt;dbl&gt;, X112 &lt;dbl&gt;, X113 &lt;dbl&gt;,\n#&gt; #   X114 &lt;dbl&gt;, X115 &lt;dbl&gt;, X116 &lt;dbl&gt;, X117 &lt;dbl&gt;, X118 &lt;dbl&gt;,\n#&gt; #   X119 &lt;dbl&gt;, X120 &lt;dbl&gt;, X121 &lt;dbl&gt;, X122 &lt;dbl&gt;, X123 &lt;dbl&gt;,\n#&gt; #   X124 &lt;dbl&gt;, X125 &lt;dbl&gt;, X126 &lt;dbl&gt;, X127 &lt;dbl&gt;, X128 &lt;dbl&gt;,\n#&gt; #   X129 &lt;dbl&gt;, X130 &lt;dbl&gt;, X131 &lt;dbl&gt;, X132 &lt;dbl&gt;, X133 &lt;dbl&gt;,\n#&gt; #   X134 &lt;dbl&gt;, X135 &lt;dbl&gt;, X136 &lt;dbl&gt;, X201 &lt;dbl&gt;, X202 &lt;dbl&gt;,\n#&gt; #   X203 &lt;dbl&gt;, X204 &lt;dbl&gt;, X205 &lt;dbl&gt;, X206 &lt;dbl&gt;, X207 &lt;dbl&gt;, …\nnames(pain)\n#&gt;  [1] \"PATIENT_NUM\"                     \n#&gt;  [2] \"X101\"                            \n#&gt;  [3] \"X102\"                            \n#&gt;  [4] \"X103\"                            \n#&gt;  [5] \"X104\"                            \n#&gt;  [6] \"X105\"                            \n#&gt;  [7] \"X106\"                            \n#&gt;  [8] \"X107\"                            \n#&gt;  [9] \"X108\"                            \n#&gt; [10] \"X109\"                            \n#&gt; [11] \"X110\"                            \n#&gt; [12] \"X111\"                            \n#&gt; [13] \"X112\"                            \n#&gt; [14] \"X113\"                            \n#&gt; [15] \"X114\"                            \n#&gt; [16] \"X115\"                            \n#&gt; [17] \"X116\"                            \n#&gt; [18] \"X117\"                            \n#&gt; [19] \"X118\"                            \n#&gt; [20] \"X119\"                            \n#&gt; [21] \"X120\"                            \n#&gt; [22] \"X121\"                            \n#&gt; [23] \"X122\"                            \n#&gt; [24] \"X123\"                            \n#&gt; [25] \"X124\"                            \n#&gt; [26] \"X125\"                            \n#&gt; [27] \"X126\"                            \n#&gt; [28] \"X127\"                            \n#&gt; [29] \"X128\"                            \n#&gt; [30] \"X129\"                            \n#&gt; [31] \"X130\"                            \n#&gt; [32] \"X131\"                            \n#&gt; [33] \"X132\"                            \n#&gt; [34] \"X133\"                            \n#&gt; [35] \"X134\"                            \n#&gt; [36] \"X135\"                            \n#&gt; [37] \"X136\"                            \n#&gt; [38] \"X201\"                            \n#&gt; [39] \"X202\"                            \n#&gt; [40] \"X203\"                            \n#&gt; [41] \"X204\"                            \n#&gt; [42] \"X205\"                            \n#&gt; [43] \"X206\"                            \n#&gt; [44] \"X207\"                            \n#&gt; [45] \"X208\"                            \n#&gt; [46] \"X209\"                            \n#&gt; [47] \"X210\"                            \n#&gt; [48] \"X211\"                            \n#&gt; [49] \"X212\"                            \n#&gt; [50] \"X213\"                            \n#&gt; [51] \"X214\"                            \n#&gt; [52] \"X215\"                            \n#&gt; [53] \"X216\"                            \n#&gt; [54] \"X217\"                            \n#&gt; [55] \"X218\"                            \n#&gt; [56] \"X219\"                            \n#&gt; [57] \"X220\"                            \n#&gt; [58] \"X221\"                            \n#&gt; [59] \"X222\"                            \n#&gt; [60] \"X223\"                            \n#&gt; [61] \"X224\"                            \n#&gt; [62] \"X225\"                            \n#&gt; [63] \"X226\"                            \n#&gt; [64] \"X227\"                            \n#&gt; [65] \"X228\"                            \n#&gt; [66] \"X229\"                            \n#&gt; [67] \"X230\"                            \n#&gt; [68] \"X231\"                            \n#&gt; [69] \"X232\"                            \n#&gt; [70] \"X233\"                            \n#&gt; [71] \"X234\"                            \n#&gt; [72] \"X235\"                            \n#&gt; [73] \"X236\"                            \n#&gt; [74] \"X237\"                            \n#&gt; [75] \"X238\"                            \n#&gt; [76] \"PAIN_INTENSITY_AVERAGE\"          \n#&gt; [77] \"PROMIS_PHYSICAL_FUNCTION\"        \n#&gt; [78] \"PROMIS_PAIN_BEHAVIOR\"            \n#&gt; [79] \"PROMIS_DEPRESSION\"               \n#&gt; [80] \"PROMIS_ANXIETY\"                  \n#&gt; [81] \"PROMIS_SLEEP_DISTURB_V1_0\"       \n#&gt; [82] \"PROMIS_PAIN_INTERFERENCE\"        \n#&gt; [83] \"GH_MENTAL_SCORE\"                 \n#&gt; [84] \"GH_PHYSICAL_SCORE\"               \n#&gt; [85] \"AGE_AT_CONTACT\"                  \n#&gt; [86] \"BMI\"                             \n#&gt; [87] \"CCI_TOTAL_SCORE\"                 \n#&gt; [88] \"PAIN_INTENSITY_AVERAGE.FOLLOW_UP\"\n#&gt; [89] \"PAT_SEX\"                         \n#&gt; [90] \"PAT_RACE\"                        \n#&gt; [91] \"CCI_BIN\"                         \n#&gt; [92] \"MEDICAID_BIN\"\n\nRecall that the $ operator can be used to access a single column. Alternatively, we can use double brackets [[]] to select a column. We demonstrate both ways to print the first five values in the column with the patient’s average pain intensity.\n\npain$PAIN_INTENSITY_AVERAGE[1:5]\n#&gt; [1] 7 5 4 7 8\npain[[\"PAIN_INTENSITY_AVERAGE\"]][1:5]\n#&gt; [1] 7 5 4 7 8\n\n\n3.2.1 Column Summaries\nTo explore the range and distribution of a column’s values, we can use some of the base R functions. For example, the summary() function is a useful way to summarize a numeric column’s values. We can see that the pain intensity values range from 0 to 10 with a median value of 7 and that there is 1 NA value.\n\nsummary(pain$PAIN_INTENSITY_AVERAGE)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;    0.00    5.00    7.00    6.49    8.00   10.00       1\n\nWe have already seen the max(), min(), mean(), and median() functions that could have computed some of these values for us separately. Since we do have an NA value, we add the na.rm=TRUE argument to these functions. Without this argument, the returned value for all of the functions is NA.\n\nmin(pain$PAIN_INTENSITY_AVERAGE, na.rm=TRUE)\n#&gt; [1] 0\nmax(pain$PAIN_INTENSITY_AVERAGE, na.rm=TRUE)\n#&gt; [1] 10\nmean(pain$PAIN_INTENSITY_AVERAGE, na.rm=TRUE)\n#&gt; [1] 6.49\nmedian(pain$PAIN_INTENSITY_AVERAGE, na.rm=TRUE)\n#&gt; [1] 7\n\nAdditionally, the following functions are helpful for summarizing quantitative columns.\n\nrange() - returns the minimum and maximum values for a numeric vector x\nquantile() - returns the sample quantiles for a numeric vector\nIQR() - returns the interquartile range for a numeric vector\n\nBy default, the quantile() function returns the sample quantiles.\n\nquantile(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE)\n#&gt;   0%  25%  50%  75% 100% \n#&gt;    0    5    7    8   10\n\nHowever, we can pass in a list of probabilities to use instead. For example, in the following code we find the 0.1 and 0.9 quantiles. Again, we add the na.rm=TRUE argument.\n\nquantile(pain$PAIN_INTENSITY_AVERAGE, probs = c(0.1, 0.9), na.rm=TRUE)\n#&gt; 10% 90% \n#&gt;   4   9\n\nWe can also plot a histogram of the sample distribution using the hist() function. We look more in depth at how to change aspects of this histogram in Chapter 4.\n\nhist(pain$PAIN_INTENSITY_AVERAGE)\n\n\n\n\n\n\n\n\n\n\n3.2.2 Practice Question\nSummarize the PROMIS_SLEEP_DISTURB_V1_0 column both numerically and visually. Your results should look like the results in Figure 3.1.\n\n\n\n\n\n\nFigure 3.1: Summarizing a Column.\n\n\n\n\n# Insert your solution here:\n\nWe can also use the summary() function for categorical variables. In this case, R finds the counts for each level.\n\nsummary(pain$PAT_SEX)\n#&gt;    Length     Class      Mode \n#&gt;     21659 character character\n\nFor categorical columns, it is also useful to use the table() function, which returns the counts for each possible value, instead of the summary() function. By default, table() ignores NA values. However, we can set useNA=\"always\" if we also want to display the number of NA values in the table output. Additionally, we can use the prop.table() function to convert the counts to proportions. Using these functions, we can see that the column PAT_SEX column, which corresponds to the reported patient sex, has a single missing value, and we can also see that around 60% of patients are female.\n\ntable(pain$PAT_SEX, useNA=\"always\")\n#&gt; \n#&gt; female   male   &lt;NA&gt; \n#&gt;  13102   8556      1\n\n\nprop.table(table(pain$PAT_SEX))\n#&gt; \n#&gt; female   male \n#&gt;  0.605  0.395\n\nNote that this column is not actually a factor column yet, which we can check using the is.factor() function. We can convert it to one using as.factor().\n\nis.factor(pain$PAT_SEX)\n#&gt; [1] FALSE\n\n\npain$PAT_SEX &lt;- as.factor(pain$PAT_SEX)\nis.factor(pain$PAT_SEX)\n#&gt; [1] TRUE\n\n\n\n3.2.3 Other Summary Functions\nSometimes we want to summarize information across multiple columns or rows. We can use the rowSums() and colSums() functions to sum over the rows or columns of a matrix or data frame. We first subset the data to the body pain map regions. In the first line of code, I select the column names pertaining to these columns. This allows me to select those columns in the second line of code and store this subset of the data as a new data frame called pain_body_map.\n\nbody_map_cols &lt;- names(pain)[2:75]\npain_body_map &lt;- pain[, body_map_cols]\nhead(pain_body_map)\n#&gt; # A tibble: 6 × 74\n#&gt;    X101  X102  X103  X104  X105  X106  X107  X108  X109  X110  X111\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0     0     0     0     0     0     0     0     0     0     0\n#&gt; 2     0     0     0     0     0     0     0     0     0     0     0\n#&gt; 3     0     0     0     0     0     0     0     0     0     0     0\n#&gt; 4     0     0     0     0     0     0     0     0     0     0     0\n#&gt; 5     0     0     0     0     0     0     0     0     0     0     0\n#&gt; 6     0     0     0     0     0     0     0     0     0     1     0\n#&gt; # ℹ 63 more variables: X112 &lt;dbl&gt;, X113 &lt;dbl&gt;, X114 &lt;dbl&gt;, X115 &lt;dbl&gt;,\n#&gt; #   X116 &lt;dbl&gt;, X117 &lt;dbl&gt;, X118 &lt;dbl&gt;, X119 &lt;dbl&gt;, X120 &lt;dbl&gt;,\n#&gt; #   X121 &lt;dbl&gt;, X122 &lt;dbl&gt;, X123 &lt;dbl&gt;, X124 &lt;dbl&gt;, X125 &lt;dbl&gt;,\n#&gt; #   X126 &lt;dbl&gt;, X127 &lt;dbl&gt;, X128 &lt;dbl&gt;, X129 &lt;dbl&gt;, X130 &lt;dbl&gt;,\n#&gt; #   X131 &lt;dbl&gt;, X132 &lt;dbl&gt;, X133 &lt;dbl&gt;, X134 &lt;dbl&gt;, X135 &lt;dbl&gt;,\n#&gt; #   X136 &lt;dbl&gt;, X201 &lt;dbl&gt;, X202 &lt;dbl&gt;, X203 &lt;dbl&gt;, X204 &lt;dbl&gt;,\n#&gt; #   X205 &lt;dbl&gt;, X206 &lt;dbl&gt;, X207 &lt;dbl&gt;, X208 &lt;dbl&gt;, X209 &lt;dbl&gt;, …\n\nI now compute the row sums and column sums on this subset of data. The row sum for each patient is the total number of body parts in which they experience pain, whereas the column sum for each pain region is the total number of patients who experience pain in that area. The following histogram shows that most people select a low number of total regions.\n\nhist(rowSums(pain_body_map))\n\n\n\n\n\n\n\n\nWe can also see that some body parts are more often selected than others. We create a vector called perc_patients by finding the number of patients who selected each region divided by the total number of patients. The histogram shows that some body regions are selected by over 50% of patients!\n\nperc_patients &lt;- colSums(pain_body_map, na.rm=TRUE) /\n  nrow(pain_body_map)\nhist(perc_patients)\n\n\n\n\n\n\n\n\nWe use the which.max() function to see that the 55th region X219 is selected the most number of times. This corresponds to lower back pain.\n\nwhich.max(perc_patients)\n#&gt; X219 \n#&gt;   55\n\nAnother pair of useful functions are pmin() and pmax(). These functions take at least two vectors and find the pairwise minimum or maximum across those vectors, as shown in the subsequent code.\n\nv1 = c(5, 9, 12)\nv2 = c(2, 18, 4)\npmax(v1, v2)  \n#&gt; [1]  5 18 12\n\nLooking back at the pain data, if we want to create a new column lower_back_pain that corresponds to whether someone selects either X218 or X219 we can use the pmax() function to find the maximum value between columns X218 and X219. We can see that almost 60% of patients select at least one of these regions.\n\nlower_back &lt;- pmax(pain_body_map$X218, pain_body_map$X219)\nprop.table(table(lower_back))\n#&gt; lower_back\n#&gt;     0     1 \n#&gt; 0.405 0.595\n\nWe might want to store the total number of pain regions and our indicator of whether or not a patient has lower back pain as new columns. We create new columns in the pain data using the $ operator in the previous code chunk. To be consistent with the column naming in the data, we use all upper case for our column names. The dim() function shows that our data has grown by two columns, as expected.\n\npain$NUM_REGIONS &lt;- rowSums(pain_body_map)\npain$LOWER_BACK &lt;- lower_back\ndim(pain)\n#&gt; [1] 21659    94\n\nAnother useful function that allows us to perform computations over the rows or columns of a matrix or data frame is the apply(X, MARGIN, FUN) function, which takes in three arguments. The first argument is a data frame or matrix X, the second argument MARGIN indicates whether to compute over the rows (1) or columns (2), and the last argument is the function FUN to apply across that margin. The subsequent code finds the maximum value for each row in the data frame pain_body_map. Taking the minimum value of the row maximum values shows that every patient selected at least one body map region.\n\nany_selected &lt;- apply(pain_body_map, 1, max)\nmin(any_selected, na.rm=TRUE)\n#&gt; [1] 1\n\nIn a second example, we find the sum of the body pain regions over the columns, which is equivalent to the previous example using colSums(). In this case, we added the na.rm=TRUE argument. The apply() function passes any additional arguments to the function FUN.\n\nperc_patients &lt;- apply(pain_body_map, 2, sum, na.rm=TRUE) /\n  nrow(pain_body_map)\nsummary(perc_patients)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   0.032   0.070   0.136   0.144   0.181   0.542\n\n\n\n3.2.4 Practice Question\nFind the sum of each of the PROMIS measures across all patients using apply() and then using colSums(). Verify that these two methods return the same result, which is given in Figure 3.2.\n\n\n\n\n\n\nFigure 3.2: Summing Across Columns.\n\n\n\n\n# Insert your solution here:\n\n\n\n3.2.5 Missing, Infinite, and NaN Values\nAs we have seen, this data contains some missing values, which are represented as NA in R. R treats these values as if they were unknown, which is why we have to add the na.rm=TRUE argument to functions like sum() and max(). In the following example, we can see that R figures out that 1 plus an unknown number is also unknown!\n\nNA+1\n#&gt; [1] NA\n\nWe can determine whether a value is missing using the function is.na(). This function returns TRUE if the value is NA and FALSE otherwise. We can then sum up these values for a single column since each TRUE value corresponds to a value of 1 and each FALSE corresponds to a value of 0. We observe that there is a single NA value for the column PATIENT_NUM, which is the patient ID number.\n\nsum(is.na(pain$PATIENT_NUM))\n#&gt; [1] 1\n\nIf we want to calculate the sum of NA values for each column instead of just a single column, we can use the apply() function. Since we want to apply this computation over the columns, the second argument has value 2. Recall that the last argument is the function we want to call for each column. In this case, we want to apply the combination of the sum() and is.na() function. To do so, we have to specify this function ourselves. This is called an anonymous function since it doesn’t have a name.\n\nnum_missing_col &lt;- apply(pain, 2, function(x) sum(is.na(x)))\nmin(num_missing_col)\n#&gt; [1] 1\n\nInterestingly, we can see that there is at least one missing value in each column. It might be the case that there is a row with all NA values. Let’s apply the same function by row. Taking the maximum, we can see that row 11749 has all NA values.\n\nnum_missing_row &lt;- apply(pain, 1, function(x) sum(is.na(x)))\nmax(num_missing_row)\n#&gt; [1] 94\nwhich.max(num_missing_row)\n#&gt; [1] 11749\n\nWe remove that row and then find the percentage of missing values by column. We can see that the column with the highest percentage of missing values is the pain intensity at follow-up. In fact, only 33% of patients have a recorded follow-up visit.\n\npain &lt;- pain[-11749, ]\nnum_missing_col &lt;- apply(pain, 2, \n                         function(x) sum(is.na(x))/nrow(pain))\nnum_missing_col\n#&gt;                      PATIENT_NUM                             X101 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X102                             X103 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X104                             X105 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X106                             X107 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X108                             X109 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X110                             X111 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X112                             X113 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X114                             X115 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X116                             X117 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X118                             X119 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X120                             X121 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X122                             X123 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X124                             X125 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X126                             X127 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X128                             X129 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X130                             X131 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X132                             X133 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X134                             X135 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X136                             X201 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X202                             X203 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X204                             X205 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X206                             X207 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X208                             X209 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X210                             X211 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X212                             X213 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X214                             X215 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X216                             X217 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X218                             X219 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X220                             X221 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X222                             X223 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X224                             X225 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X226                             X227 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X228                             X229 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X230                             X231 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X232                             X233 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X234                             X235 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X236                             X237 \n#&gt;                          0.00000                          0.00000 \n#&gt;                             X238           PAIN_INTENSITY_AVERAGE \n#&gt;                          0.00000                          0.00000 \n#&gt;         PROMIS_PHYSICAL_FUNCTION             PROMIS_PAIN_BEHAVIOR \n#&gt;                          0.00000                          0.29412 \n#&gt;                PROMIS_DEPRESSION                   PROMIS_ANXIETY \n#&gt;                          0.00402                          0.00402 \n#&gt;        PROMIS_SLEEP_DISTURB_V1_0         PROMIS_PAIN_INTERFERENCE \n#&gt;                          0.00402                          0.00697 \n#&gt;                  GH_MENTAL_SCORE                GH_PHYSICAL_SCORE \n#&gt;                          0.13602                          0.13602 \n#&gt;                   AGE_AT_CONTACT                              BMI \n#&gt;                          0.00000                          0.26004 \n#&gt;                  CCI_TOTAL_SCORE PAIN_INTENSITY_AVERAGE.FOLLOW_UP \n#&gt;                          0.00000                          0.67042 \n#&gt;                          PAT_SEX                         PAT_RACE \n#&gt;                          0.00000                          0.00651 \n#&gt;                          CCI_BIN                     MEDICAID_BIN \n#&gt;                          0.00000                          0.01385 \n#&gt;                      NUM_REGIONS                       LOWER_BACK \n#&gt;                          0.00000                          0.00000\n\nWe create two new columns: first, we create a column for the change in pain at follow-up, and second, we create a column which is the percent change in pain at follow-up.\n\npain$PAIN_CHANGE &lt;- pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP - \n  pain$PAIN_INTENSITY_AVERAGE\nhist(pain$PAIN_CHANGE)\n\n\n\n\n\n\n\n\n\npain$PERC_PAIN_CHANGE &lt;- pain$PAIN_CHANGE / \n  pain$PAIN_INTENSITY_AVERAGE\nsummary(pain$PERC_PAIN_CHANGE)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#&gt;      -1       0       0     Inf       0     Inf   14520\n\nIn the summary of the percent change, we can see that the maximum value is Inf. This is R’s representation of infinity. This occurred because some patients have an initial pain score of 0, which creates infinite values when we divide through by this value to find the percent change. We can test whether something is infinite using the is.infinite() or is.finite() functions. This shows that there were three patients with infinite values. The value -Inf is used to represent negative infinity.\n\nsum(is.infinite(pain$PERC_PAIN_CHANGE))\n#&gt; [1] 3\n\nAnother special value in R is NaN, which stands for “Not a Number”. For example, 0/0 results in a NaN value. We can test for NaN values using the is.nan() function.\n\n0/0\n#&gt; [1] NaN\n\nLooking back at the missing values, there are two useful functions for selecting the complete cases in a data frame. The na.omit() function returns the data frame with incomplete cases removed, whereas complete.cases() returns TRUE/FALSE values for each row indicating whether each row is complete, which we can then use to select the rows with TRUE values. In the following code, we see both approaches select the same number of rows.\n\npain_sub1 &lt;- na.omit(pain)\npain_sub2 &lt;- pain[complete.cases(pain), ]\ndim(pain_sub1)\n#&gt; [1] 2413   96\ndim(pain_sub2)\n#&gt; [1] 2413   96\n\n\n\n3.2.6 Dates in R\nThe columns in the pain data contain character and numeric values. One special type of character column that is not present is a column that corresponds to a date or date-time. By default, read.csv() reads these columns in as character columns whereas the read_csv() function from the readr package in the tidyverse family recognizes common date formats. If we have a character column, we can convert to a date object using as.Date() for date columns and as.POSIXct() for date-time columns. For columns with only a time but no date, you can add a date or use the hms package (Müller 2023), which is not demonstrated here. These functions automatically try to detect the format of the inputted string, but it is often helpful to provide the format format and time zone tz. To input our format we use the following key.\n\n\n\nSymbol\nDescription\n\n\n\n\n%Y\nFour digit year.\n\n\n%y\nTwo digit year.\n\n\n%m\nNumeric month.\n\n\n%b%\nAbbreviated name of month.\n\n\n%B\nFull name of month.\n\n\n%d\nNumeric day of the month.\n\n\n%H\nMilitary time hour (24 hour).\n\n\n%I\nImperial time hour (12 hour).\n\n\n%M\nMinute.\n\n\n%S\nSeconds.\n\n\n%p\nAM/PM\n\n\n\n\ndate_example &lt;- data.frame(x = c(\"2020-01-15\", \"2021-11-16\", \n                                 \"2019-08-01\"),\n                           y = c(\"2020-01-15 3:14 PM\", \n                                 \"2021-11-16 5:00 AM\",\n                                 \"2019-08-01 3:00 PM\"),\n                           z = c(\"04:10:00\", \"11:35:11\", \"18:00:45\"))\n\n# Convert date and date times using formats\ndate_example$x &lt;- as.Date(date_example$x, format = \"%Y-%m-%d\", \n                          tz = \"EST\")\ndate_example$y &lt;- as.POSIXct(date_example$y, \n                             format = \"%Y-%m-%d %I:%M %p\")\n\n# Add date to z and convert\ndate_example$z &lt;- paste(\"2024-06-24\", date_example$z)\ndate_example$z &lt;- as.POSIXct(date_example$z, \n                             format = \"%Y-%m-%d %H:%M:%S\")\ndate_example\n#&gt;            x                   y                   z\n#&gt; 1 2020-01-15 2020-01-15 15:14:00 2024-06-24 04:10:00\n#&gt; 2 2021-11-16 2021-11-16 05:00:00 2024-06-24 11:35:11\n#&gt; 3 2019-08-01 2019-08-01 15:00:00 2024-06-24 18:00:45\n\nBy recognizing these columns as dates, we can find the time between two dates using the difftime() function. This function takes in two times time1 and time2 and finds the difference time1 - time2 in the given units.\n\ndifftime(date_example$x[2], date_example$x[1], units = \"days\")\n#&gt; Time difference of 671 days\n\nAdditionally, we can use the seq() function to add or subtract time by specifying a unit for by.\n\nseq(date_example$x[1], by = \"month\", length = 3)\n#&gt; [1] \"2020-01-15\" \"2020-02-15\" \"2020-03-15\"\n\nFor those interested in doing more manipulations with dates, the lubridate package (Spinu, Grolemund, and Wickham 2023) in the tidyverse expands upon the base functionality of R for working with dates. This package uses its own date-time class and includes functions to easily extract information from and manipulate dates.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Working with Data Files in R</span>"
    ]
  },
  {
    "objectID": "book/working_data_files.html#using-logic-to-subset-summarize-and-transform",
    "href": "book/working_data_files.html#using-logic-to-subset-summarize-and-transform",
    "title": "3  Working with Data Files in R",
    "section": "3.3 Using Logic to Subset, Summarize, and Transform",
    "text": "3.3 Using Logic to Subset, Summarize, and Transform\nWe have already seen how to use TRUE/FALSE values to select rows in a data frame. The following logic operators in R allow us to expand on this capability to write more complex logic.\n\n&lt; less than\n&lt;= less than or equal to\n&gt; greater than\n&gt;= greater than or equal to\n== equal to\n!= not equal to\na %in% b a’s value is in a vector of values b\n\nThe first six operators are a direct comparison between two values.\n\n2 &lt; 2\n#&gt; [1] FALSE\n2 &lt;= 2\n#&gt; [1] TRUE\n3 &gt; 2\n#&gt; [1] TRUE\n3 &gt;= 2\n#&gt; [1] TRUE\n\"A\" == \"B\"\n#&gt; [1] FALSE\n\"A\" != \"B\"\n#&gt; [1] TRUE\n\nThe operators assume there is a natural ordering or comparison between values. For example, for strings the ordering is alphabetical and for logical operators we use their numeric interpretation (TRUE = 1, FALSE = 0).\n\n\"A\" &lt; \"B\"\n#&gt; [1] TRUE\nTRUE &lt; FALSE\n#&gt; [1] FALSE\n\nThe %in% operator is slightly different. This operator checks whether a value is in a set of possible values. For example, we can check whether values are in the set c(4,1,2).\n\n1 %in% c(4, 1, 2)\n#&gt; [1] TRUE\nc(0, 1, 5) %in% c(4, 1, 2)\n#&gt; [1] FALSE  TRUE FALSE\n\nAdditionally, we can use the following operators, which allow us to negate or combine logical operators.\n\n!x - the NOT operator ! reverses TRUE/FALSE values\nx | y - the OR operator | checks whether either x or y is equal to TRUE\nx & y - the AND operator & checks whether both x and y are equal to TRUE\nxor(x,y) - the xor function checks whether exactly one of x or y is equal to TRUE (called exclusive or)\nany(x) - the any function checks whether any value in x is TRUE (equivalent to using an OR operator | between all values)\nall(x) - the all function checks whether all values in x are TRUE (equivalent to using an AND operator & between all values)\n\nSome simple examples for each are given in the following code chunk.\n\n!(2 &lt; 3)\n#&gt; [1] FALSE\n(\"Alice\" &lt; \"Bob\") | (\"Alice\" &lt; \"Aaron\")\n#&gt; [1] TRUE\n(\"Alice\" &lt; \"Bob\") & (\"Alice\" &lt; \"Aaron\")\n#&gt; [1] FALSE\nxor(TRUE, FALSE)\n#&gt; [1] TRUE\nany(c(FALSE, TRUE, TRUE))\n#&gt; [1] TRUE\nall(c(FALSE, TRUE, TRUE))\n#&gt; [1] FALSE\n\nLet’s demonstrate these operators on the pain data. We first update the Medicaid column by making the character values more informative. The logic on the left hand side selects those that do or do not have Medicaid and then assigns those values to the new ones.\n\npain$MEDICAID_BIN[pain$MEDICAID_BIN == \"no\"] &lt;- \"No Medicaid\"\npain$MEDICAID_BIN[pain$MEDICAID_BIN == \"yes\"] &lt;- \"Medicaid\"\ntable(pain$MEDICAID_BIN)\n#&gt; \n#&gt;    Medicaid No Medicaid \n#&gt;        4601       16757\n\nAdditionally, we could subset the data to only those who have follow-up. The not operator ! reverses the TRUE/FALSE values returned from the is.na() function. Therefore, the new value is TRUE if the follow-up value is not NA.\n\npain_follow_up &lt;- pain[!is.na(pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP), ]\n\nEarlier, we created a column indicating whether or not a patient has lower back pain. We now use the any() function to check whether a patient has general back pain. If at least one of these values is equal to 1, then the function returns TRUE. If we had used the all() function instead, this would check whether all values are equal to 1, indicating that a patient has pain on their whole back.\n\npain$BACK &lt;- any(pain$X208==1, pain$X209==1, pain$X212==1, \n                 pain$X213==1, pain$X218==1, pain$X219==1)\n\n\n3.3.1 Practice Question\nSubset the pain data to those who have follow-up and have an initial average pain intensity of 5 or above. Name this subset of the data pain_subset. Print the head of this data. The first 6 patient IDs in this new dataset should be 13118, 21384, 1827, 11309, 11093, and 14667.\n\n# Insert your solution here:\n\nLastly, we look at the column for patient race PAT_RACE. The table() function shows that most patients are WHITE or BLACK. Given how few observations are in the other categories, we may want to combine some of these levels into one.\n\ntable(pain$PAT_RACE)\n#&gt; \n#&gt;          ALASKA NATIVE        AMERICAN INDIAN                  BLACK \n#&gt;                      2                     58                   3229 \n#&gt;                CHINESE               DECLINED               FILIPINO \n#&gt;                     21                    121                      6 \n#&gt;          GUAM/CHAMORRO               HAWAIIAN         INDIAN (ASIAN) \n#&gt;                      1                      1                     49 \n#&gt;               JAPANESE                 KOREAN          NOT SPECIFIED \n#&gt;                      9                     10                      4 \n#&gt;                  OTHER            OTHER ASIAN OTHER PACIFIC ISLANDER \n#&gt;                      1                     47                     12 \n#&gt;             VIETNAMESE                  WHITE \n#&gt;                      6                  17940\n\nAnother way we could have found all possible values for this column is to use the unique() function. This function takes in a data frame or vector x and returns x with all duplicate rows or values removed.\n\nunique(pain$PAT_RACE)\n#&gt;  [1] \"WHITE\"                  \"BLACK\"                 \n#&gt;  [3] \"DECLINED\"               \"AMERICAN INDIAN\"       \n#&gt;  [5] \"INDIAN (ASIAN)\"         \"ALASKA NATIVE\"         \n#&gt;  [7] NA                       \"FILIPINO\"              \n#&gt;  [9] \"JAPANESE\"               \"VIETNAMESE\"            \n#&gt; [11] \"KOREAN\"                 \"CHINESE\"               \n#&gt; [13] \"OTHER ASIAN\"            \"NOT SPECIFIED\"         \n#&gt; [15] \"HAWAIIAN\"               \"OTHER PACIFIC ISLANDER\"\n#&gt; [17] \"OTHER\"                  \"GUAM/CHAMORRO\"\n\nTo combine some of these levels, we can use the %in% operator. We first create an Asian, Asian American, or Pacific Islander race category and then create an American Indian or Alaska Native category.\n\naapi_values &lt;- c(\"CHINESE\", \"HAWAIIAN\", \"INDIAN (ASIAN)\", \"FILIPINO\", \n                 \"VIETNAMESE\", \"JAPANESE\", \"KOREAN\", \"GUAM/CHAMORRO\", \n                 \"OTHER ASIAN\", \"OTHER PACIFIC ISLANDER\")\npain$PAT_RACE[pain$PAT_RACE %in% aapi_values] &lt;- \"AAPI\"\npain$PAT_RACE[pain$PAT_RACE %in% \n                c(\"ALASKA NATIVE\", \"AMERICAN INDIAN\")] &lt;- \"AI/AN\"\ntable(pain$PAT_RACE)\n#&gt; \n#&gt;          AAPI         AI/AN         BLACK      DECLINED NOT SPECIFIED \n#&gt;           162            60          3229           121             4 \n#&gt;         OTHER         WHITE \n#&gt;             1         17940\n\n\n\n3.3.2 Other Selection Functions\nIn the previous code, we selected rows using TRUE/FALSE Boolean values. Instead, we could have also used the which() function. This function takes TRUE/FALSE values and returns the index values for all the TRUE values. We use this to treat those with race given as DECLINED as not specified.\n\npain$PAT_RACE[which(pain$PAT_RACE == \"DECLINED\")] &lt;- \"NOT SPECIFIED\"\n\nAnother selection function is the subset() function. This function takes in two arguments. The first is the vector, matrix, or data frame to select from and the second is a vector of TRUE/FALSE values to use for row selection. We use this to find the observation with race marked as OTHER. We then update this race to also be marked as not specified.\n\nsubset(pain, pain$PAT_RACE == \"OTHER\")\n#&gt; # A tibble: 1 × 97\n#&gt;   PATIENT_NUM  X101  X102  X103  X104  X105  X106  X107  X108  X109\n#&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        3588     1     1     1     0     1     1     1     0     0\n#&gt; # ℹ 87 more variables: X110 &lt;dbl&gt;, X111 &lt;dbl&gt;, X112 &lt;dbl&gt;, X113 &lt;dbl&gt;,\n#&gt; #   X114 &lt;dbl&gt;, X115 &lt;dbl&gt;, X116 &lt;dbl&gt;, X117 &lt;dbl&gt;, X118 &lt;dbl&gt;,\n#&gt; #   X119 &lt;dbl&gt;, X120 &lt;dbl&gt;, X121 &lt;dbl&gt;, X122 &lt;dbl&gt;, X123 &lt;dbl&gt;,\n#&gt; #   X124 &lt;dbl&gt;, X125 &lt;dbl&gt;, X126 &lt;dbl&gt;, X127 &lt;dbl&gt;, X128 &lt;dbl&gt;,\n#&gt; #   X129 &lt;dbl&gt;, X130 &lt;dbl&gt;, X131 &lt;dbl&gt;, X132 &lt;dbl&gt;, X133 &lt;dbl&gt;,\n#&gt; #   X134 &lt;dbl&gt;, X135 &lt;dbl&gt;, X136 &lt;dbl&gt;, X201 &lt;dbl&gt;, X202 &lt;dbl&gt;,\n#&gt; #   X203 &lt;dbl&gt;, X204 &lt;dbl&gt;, X205 &lt;dbl&gt;, X206 &lt;dbl&gt;, X207 &lt;dbl&gt;, …\n\n\npain$PAT_RACE[pain$PATIENT_NUM==3588] &lt;- \"NOT SPECIFIED\"\ntable(pain$PAT_RACE)\n#&gt; \n#&gt;          AAPI         AI/AN         BLACK NOT SPECIFIED         WHITE \n#&gt;           162            60          3229           126         17940",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Working with Data Files in R</span>"
    ]
  },
  {
    "objectID": "book/working_data_files.html#recap-video",
    "href": "book/working_data_files.html#recap-video",
    "title": "3  Working with Data Files in R",
    "section": "3.4 Recap Video",
    "text": "3.4 Recap Video\nIn this video, we reference the population mean and standard deviations for the instrument variables in the data. Use the help operator (?pain) to refresh your memory about these variables.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Working with Data Files in R</span>"
    ]
  },
  {
    "objectID": "book/working_data_files.html#exercises",
    "href": "book/working_data_files.html#exercises",
    "title": "3  Working with Data Files in R",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\nFor these exercises, we use the pain data from the HDSinRdata package.\n\nPrint summary statistics for the PROMIS_PHYSICAL_FUNTION and PROMIS_ANXIETY columns in this dataset. Read the data documentation for these two columns, which both have range 0 to 100, and then comment on the distributions of these columns.\nCreate frequency tables for the values of PAT_SEX and PAT_RACE and summarize what these tables tell you about the distributions of these demographic characteristics.\nCreate a new data frame called pain.new that doesn’t contain patients with NA values for both GH_MENTAL_SCORE and GH_PHYSICAL_SCORE, which are the PROMIS global mental and physical scores, respectively.\nCreate a vector of the proportion of patients who reported pain in each of the pain regions. Then, find the minimum, median, mean, maximum, standard deviation, and variance of this vector.\nCalculate the median and interquartile range of the distribution of the total number of painful leg regions selected for each patient. Then, write a few sentences explaining anything interesting you observe about this distribution in the context of this dataset.\nLook at the distribution of average pain intensity between patients with only one pain region selected vs. those with more than one region selected. What do you notice?\nCreate a histogram to plot the distribution of the PAIN_INTENSITY_AVERAGE.FOLLOW_UP column. Then, create a table summarizing how many patients had missing values in this column. Finally, choose two columns to compare the distribution between those with and without missing follow-up. What do you notice?\n\n\n\n\n\nAlter, Benedict J, Nathan P Anderson, Andrea G Gillman, Qing Yin, Jong-Hyeon Jeong, and Ajay D Wasan. 2021. “Hierarchical Clustering by Patient-Reported Pain Distribution Alone Identifies Distinct Chronic Pain Subgroups Differing by Pain Intensity, Quality, and Clinical Outcomes.” PLoS One 16 (8): e0254862.\n\n\nMüller, Kirill. 2023. Hms: Pretty Time of Day. https://CRAN.R-project.org/package=hms.\n\n\nSpinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2023. lubridate: Make Dealing with Dates a Little Easier. https://CRAN.R-project.org/package=lubridate.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Working with Data Files in R</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#univariate-distributions",
    "href": "book/exploratory_analysis.html#univariate-distributions",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.1 Univariate Distributions",
    "text": "4.1 Univariate Distributions\nIn this chapter, we use a sample of the National Health and Nutrition Examination Survey (Centers for Disease Control and Prevention (CDC) 1999-2018) containing lead, blood pressure, BMI, smoking status, alcohol use, and demographic variables from NHANES 1999-2018. Variable selection and feature engineering followed the analysis in Huang (2022). There are 31,625 observations in this sample. Use the help operator ?NHANESsample to read the column descriptions.\n\ndata(NHANESsample)\ndim(NHANESsample)\n#&gt; [1] 31265    21\nnames(NHANESsample)\n#&gt;  [1] \"ID\"            \"AGE\"           \"SEX\"           \"RACE\"         \n#&gt;  [5] \"EDUCATION\"     \"INCOME\"        \"SMOKE\"         \"YEAR\"         \n#&gt;  [9] \"LEAD\"          \"BMI_CAT\"       \"LEAD_QUANTILE\" \"HYP\"          \n#&gt; [13] \"ALC\"           \"DBP1\"          \"DBP2\"          \"DBP3\"         \n#&gt; [17] \"DBP4\"          \"SBP1\"          \"SBP2\"          \"SBP3\"         \n#&gt; [21] \"SBP4\"\n\nTo start our exploration, we look at whether there are any missing values. We use the complete.cases() function to observe that there are no complete cases. We also see that the subsequent blood pressure measurements and alcohol use have the highest percentage of missing values. For demonstration, we choose to only keep the first systolic and diastolic blood pressure measurements and do a complete case analysis using the na.omit() function to define our complete data frame nhanes_df.\n\nsum(complete.cases(NHANESsample))\n#&gt; [1] 0\napply(NHANESsample, 2, function(x) sum(is.na(x)))/nrow(NHANESsample)\n#&gt;            ID           AGE           SEX          RACE     EDUCATION \n#&gt;      0.000000      0.000000      0.000000      0.000000      0.000672 \n#&gt;        INCOME         SMOKE          YEAR          LEAD       BMI_CAT \n#&gt;      0.000000      0.000000      0.000000      0.000000      0.000000 \n#&gt; LEAD_QUANTILE           HYP           ALC          DBP1          DBP2 \n#&gt;      0.000000      0.000000      0.026867      0.060035      0.063905 \n#&gt;          DBP3          DBP4          SBP1          SBP2          SBP3 \n#&gt;      0.070974      0.891124      0.060035      0.063905      0.070942 \n#&gt;          SBP4 \n#&gt;      0.891124\n\n\nnhanes_df &lt;- na.omit(subset(NHANESsample, \n                            select = -c(SBP2, SBP3, SBP4, DBP2, DBP3, \n                                       DBP4)))\n\nIn the last chapter, we introduced the table() and summary() functions to quickly summarize categorical and quantitative vectors. We can observe that over half of the observations never smoked and that the most recent NHANES cycle in the data is 2017-2018.\n\ntable(nhanes_df$SMOKE)\n#&gt; \n#&gt; NeverSmoke  QuitSmoke StillSmoke \n#&gt;      13774       8019       6799\nsummary(nhanes_df$YEAR)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1999    2003    2007    2008    2011    2017\n\nWe decide to select the most recent observations from NHANES 2017-2018 for our analysis in this chapter. We use the subset() function to select these rows.\n\nnhanes_df &lt;- subset(nhanes_df, nhanes_df$YEAR == 2017)\n\nAs shown, smoking status has been coded into three categories: “NeverSmoke”, “QuitSmoke”, and “StillSmoke”. We want to create a new column to represent whether someone has ever smoked. To do so, we use the ifelse() function, which allows us to create a new vector using logic. The logic captured by this function is that we want to use one value if we meet some condition and we want to use a second value if the condition is not met. The first argument is a vector of TRUE/FALSE values representing the conditions, the next argument is the value or vector to use if we meet the condition(s), and the last argument is the value or vector to use otherwise. We use this function to create a new vector EVER_SMOKE that is equal to “Yes” for those who are either still smoking or quit smoking and equal to “No” otherwise.\n\nnhanes_df$EVER_SMOKE &lt;- ifelse(nhanes_df$SMOKE %in% c(\"QuitSmoke\", \n                                                      \"StillSmoke\"), \n                               \"Yes\", \"No\")\ntable(nhanes_df$EVER_SMOKE)\n#&gt; \n#&gt;   No  Yes \n#&gt; 1411 1173\n\nIf we did not want to store this new column, we could use the pipe operator |&gt; to send the output directly to the table() function. The pipe operator takes the result on the left hand side and passes it as the first argument to the function on the right hand side.\n\nifelse(nhanes_df$SMOKE %in% c(\"QuitSmoke\", \"StillSmoke\"), \n       \"Yes\", \"No\") |&gt;\n  table()\n#&gt; \n#&gt;   No  Yes \n#&gt; 1411 1173\n\nThe summary() and table() functions allow us to summarize the univariate sample distributions of columns. We may also want to plot these distributions. We saw in Chapter 3 that the hist() function creates a histogram plot. We use this function to plot a histogram of the log transformation of the lead column.\n\nhist(log(nhanes_df$LEAD))\n\n\n\n\n\n\n\n\nIf we want to polish this figure, we can use some of the other optional arguments to the hist() function. For example, we may want to update the text log(nhanes_df$lead) in the title and x-axis. In the following code, we update the color, labels, and number of bins for the plot. The function colors() returns all recognized colors in R. The argument breaks specifies the number of bins to use to create the histogram, col specifies the color, main specifies the title of the plot, and xlab specifies the x-axis label (using ylab would specify the y-axis label). Read the documentation ?hist for the full list of arguments available.\n\nhist(log(nhanes_df$LEAD), breaks = 30, col = \"blue\", \n     main = \"Histogram of Log Blood Lead Level\",\n     xlab = \"Log Blood Lead Level\")\n\n\n\n\n\n\n\n\nFor categorical columns, we may want to plot the counts in each category using a bar plot. The function barplot() asks us to specify the names and heights of the bars. To do so, we need to store the counts for each category. Again, we update the color and labels.\n\nsmoke_counts &lt;- table(nhanes_df$SMOKE)\nbarplot(height = smoke_counts, names = names(smoke_counts), \n        col = \"violetred\", xlab=\"Smoking Status\", ylab=\"Frequency\")\n\n\n\n\n\n\n\n\nWith a bar plot, we can even specify a different color for each bar. To do so, col must be a vector of specified colors with the same length as the number of categories.\n\nbarplot(height = smoke_counts, names = names(smoke_counts), \n        col = c(\"orange\", \"violetred\", \"blue\"),\n        xlab = \"Smoking Status\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\n4.1.1 Practice Question\nRecreate the barplot in Figure 4.1 showing the proportion of values in each LEAD_QUANTILE category.\n\n\n\n\n\n\nFigure 4.1: Lead Quantile Bar Plot.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#bivariate-distributions",
    "href": "book/exploratory_analysis.html#bivariate-distributions",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.2 Bivariate Distributions",
    "text": "4.2 Bivariate Distributions\nWe now turn our attention to relationships among multiple columns. When we have two categorical columns, we can use the table() function to find the counts across all combinations. For example, we look at the distribution of smoking status levels by sex. We observe that a higher percentage of female participants have never smoked.\n\ntable(nhanes_df$SMOKE, nhanes_df$SEX)\n#&gt;             \n#&gt;              Male Female\n#&gt;   NeverSmoke  596    815\n#&gt;   QuitSmoke   390    241\n#&gt;   StillSmoke  324    218\n\nTo look at the sample distribution of a continuous column stratified by a categorical column, we can call the summary() function for each subset of the data. In the subsequent code, we look at the distribution of blood lead level by sex and observe higher blood lead levels in male observations.\n\nsummary(nhanes_df$LEAD[nhanes_df$SEX == \"Female\"])\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.10    0.47    0.77    0.98    1.21    8.67\nsummary(nhanes_df$LEAD[nhanes_df$SEX == \"Male\"])\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    0.05    0.70    1.09    1.46    1.66   22.01\n\nWe can also observe this visually through a box plot. When given one categorical column and one continuous column, the plot() function creates a box plot. By default, the first argument is the x-axis and the second argument is the y-axis.\n\nplot(nhanes_df$SEX, log(nhanes_df$LEAD), ylab = \"Log Blood Lead Level\", \n     xlab = \"Sex\")\n\n\n\n\n\n\n\n\nAlternatively, we can use the boxplot() function, which can be passed a formula. A formula is a string representation of how to group the data, where the left-hand side is the continuous column and the right-hand side is one or more categorical columns to group by. In the following case, we group by multiple columns, SEX and EVER_SMOKE, so our formula is log(LEAD) ~ SEX + EVER_SMOKE. The second argument to the function specifies the data. We specify the column colors to show the link between the box plots shown.\n\nboxplot(log(LEAD) ~ SEX + EVER_SMOKE, data = nhanes_df, \n        col=c(\"orange\", \"blue\", \"orange\", \"blue\"),\n        xlab = \"Sex : Ever Smoked\", ylab = \"Log Blood Lead Level\")\n\n\n\n\n\n\n\n\nTo visualize the bivariate distributions between two continuous columns, we can use scatter plots. To create a scatter plot, we use the plot() function again. We use this function to show the relationship between systolic and diastolic blood pressure.\n\nplot(nhanes_df$SBP1, nhanes_df$DBP1, col = \"blue\", \n    xlab = \"Systolic Blood Pressure\",\n    ylab = \"Diastolic Blood Pressure\")\n\n\n\n\n\n\n\n\nThe two measures of blood pressure look highly correlated. We can calculate their Pearson and Spearman correlation using the cor() function. The default method is the Pearson correlation, but we can also calculate the Kendall or Spearman correlation by specifying the method.\n\ncor(nhanes_df$SBP1, nhanes_df$DBP1)\n#&gt; [1] 0.417\ncor(nhanes_df$SBP1, nhanes_df$DBP1, method = \"spearman\")\n#&gt; [1] 0.471\n\nWe may also want to add some extra information to our plot. This time, instead of specifying the color manually, we use the column hyp, an indicator for hypertension, to specify the color. We have to make sure this vector is a factor for R to color by group. Additionally, we add a blue vertical and horizontal line using the abline() function to mark cutoffs for hypertension. Even though this function is called after plot(), the lines are automatically added to the current plot. We can see that most of those with hypertension have systolic or diastolic blood pressure measurements above this threshold.\n\nplot(nhanes_df$SBP1, nhanes_df$DBP1, col = as.factor(nhanes_df$HYP), \n     xlab = \"Systolic Blood Pressure\",\n     ylab = \"Diastolic Blood Pressure\")\nabline(v = 130, col = \"blue\")\nabline(h = 80, col = \"blue\")\n\n\n\n\n\n\n\n\nThe previous plots are all displayed as a single figure. If we want to display multiple plots next to each other, we can specify the graphical parameters using the par() function by updating the argument mfrow = c(nrow, ncol) with the number of columns and rows we would like to use for our figures. We use this to display the distribution of log blood lead level between those with and without hypertension next to the previous plot.\n\npar(mfrow = c(1, 2))\n\n# box plot\nboxplot(log(LEAD) ~ HYP, data = nhanes_df, xlab = \"Hypertension\", \n        ylab = \"Log Blood Lead Level\")\n\n# scatter plot\nplot(nhanes_df$SBP1, nhanes_df$DBP1, col = as.factor(nhanes_df$HYP), \n     xlab = \"Systolic Blood Pressure\",\n     ylab = \"Diastolic Blood Pressure\")\nabline(v = 130, col = \"blue\")\nabline(h = 80, col = \"blue\")\n\n\n\n\n\n\n\n\nWe then reset to only display a single plot for future images using the par() function again.\n\npar(mfrow = c(1, 1))\n\n\n4.2.1 Practice Question\nRecreate the three boxplots in Figure 4.2 (one for each education level) of income by BMI category and arrange them next to each other using the par() function.\n\n\n\n\n\n\nFigure 4.2: Box Plot Example.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#autogenerated-plots",
    "href": "book/exploratory_analysis.html#autogenerated-plots",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.3 Autogenerated Plots",
    "text": "4.3 Autogenerated Plots\nIn the previous sections, we learned some new functions for visualizing the relationship between columns. The GGally package contains some useful functions for looking at multiple univariate and bivariate relationships at the same time, such as the ggpairs() function. ggpairs() takes the data as its first argument. By default, it plots the pairwise distributions for all columns, but we can also specify to only select a subset of columns using the columns argument. You can see in the following example that it plots bar plots and density plots for each univariate sample distribution. It then plots the bivariate distributions and calculates the Pearson correlation for all pairs of continuous columns. That’s a lot of information!\n\nggpairs(nhanes_df, columns = c(\"SEX\", \"AGE\", \"LEAD\", \"SBP1\", \"DBP1\"))\n\n\n\n\n\n\n\n\nAnother useful function in this package is the ggcorr() function: this function takes in a data frame with only numeric columns and displays the correlation between all pairs of columns, where the color of each grid cell indicates the strength of the correlation. The additional argument label=TRUE prints the actual correlation value on each grid cell. This is a useful way to identify pairs of strongly correlated columns. Note that we used the pipe operator again to find the correlation on the continuous columns without saving this subset of data.\n\nnhanes_df[, c(\"AGE\", \"LEAD\", \"SBP1\", \"DBP1\")] |&gt;\n  ggcorr(label = TRUE)",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#tables",
    "href": "book/exploratory_analysis.html#tables",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.4 Tables",
    "text": "4.4 Tables\nAnother useful way to display information about your data is through tables. For example, it is standard practice in articles to have the first table in the paper give information about the study sample, such as the mean and standard deviation for all continuous columns and the proportions for categorical columns. The gt package is designed to create polished tables that can include footnotes, titles, column labels, etc. The gtsummary package is an extension of this package that can create summary tables. We focus on the latter but come back to creating nice tables in Chapter 22.\nTo start, we create a gt object (a special type of table) of the first six rows of our data using the gt() function. You can see the difference in the formatting as opposed to printing the data.\n\ngt(head(nhanes_df[, c(\"ID\", \"AGE\", \"SEX\", \"RACE\")])) \n\n\n\n\n\n\n\nID\nAGE\nSEX\nRACE\n\n\n\n\n93711\n56\nMale\nOther Race\n\n\n93713\n67\nMale\nNon-Hispanic White\n\n\n93716\n61\nMale\nOther Race\n\n\n93717\n22\nMale\nNon-Hispanic White\n\n\n93721\n60\nFemale\nMexican American\n\n\n93722\n60\nFemale\nNon-Hispanic White\n\n\n\n\n\n\n\nWe now show you how to use the tbl_summary() function in the gtsummary package. The first argument to this function is again the data frame. By default, this function summarizes all the columns in the data. Instead, we use the include argument to specify a list of columns to include. We then pipe this output to the function as_gt(), which creates a gt table from the summary output. Note that the table computes the total number of observations and the proportions for categorical columns and the median and interquartile range for continuous columns.\n\ntbl_summary(nhanes_df, \n            include = c(\"SEX\", \"RACE\", \"AGE\", \"EDUCATION\", \"SMOKE\", \n                        \"BMI_CAT\", \"LEAD\", \"SBP1\", \"DBP1\", \"HYP\")) |&gt;\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 2,5841\n\n\n\n\nSEX\n\n\n\n\n    Male\n1,310 (51%)\n\n\n    Female\n1,274 (49%)\n\n\nRACE\n\n\n\n\n    Mexican American\n358 (14%)\n\n\n    Other Hispanic\n225 (8.7%)\n\n\n    Non-Hispanic White\n992 (38%)\n\n\n    Non-Hispanic Black\n568 (22%)\n\n\n    Other Race\n441 (17%)\n\n\nAGE\n48 (33, 62)\n\n\nEDUCATION\n\n\n\n\n    LessThanHS\n373 (14%)\n\n\n    HS\n593 (23%)\n\n\n    MoreThanHS\n1,618 (63%)\n\n\nSMOKE\n\n\n\n\n    NeverSmoke\n1,411 (55%)\n\n\n    QuitSmoke\n631 (24%)\n\n\n    StillSmoke\n542 (21%)\n\n\nBMI_CAT\n\n\n\n\n    BMI&lt;=25\n663 (26%)\n\n\n    25&lt;BMI&lt;30\n808 (31%)\n\n\n    BMI&gt;=30\n1,113 (43%)\n\n\nLEAD\n0.93 (0.56, 1.44)\n\n\nSBP1\n122 (112, 134)\n\n\nDBP1\n72 (66, 80)\n\n\nHYP\n1,451 (56%)\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\nWe can update our table by changing some of its arguments. This time, we specify that we want to stratify our table by hypertension status so that the table summarizes the data by this grouping. Additionally, we change how continuous columns are summarized by specifying that we want to report the mean and standard deviation instead of the median and interquartile range. We do this using the statistic argument. The documentation for the tbl_summary() function can help you format this argument depending on which statistics you would like to display.\n\ntbl_summary(nhanes_df, \n            include = c(\"SEX\", \"RACE\", \"AGE\", \"EDUCATION\", \"SMOKE\", \n                        \"BMI_CAT\", \"LEAD\", \"SBP1\", \"DBP1\", \"HYP\"),\n            by = \"HYP\",\n            statistic = list(all_continuous() ~ \"{mean} ({sd})\")) |&gt;\nas_gt() \n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0, N = 1,1331\n1, N = 1,4511\n\n\n\n\nSEX\n\n\n\n\n\n\n    Male\n472 (42%)\n838 (58%)\n\n\n    Female\n661 (58%)\n613 (42%)\n\n\nRACE\n\n\n\n\n\n\n    Mexican American\n186 (16%)\n172 (12%)\n\n\n    Other Hispanic\n104 (9.2%)\n121 (8.3%)\n\n\n    Non-Hispanic White\n429 (38%)\n563 (39%)\n\n\n    Non-Hispanic Black\n203 (18%)\n365 (25%)\n\n\n    Other Race\n211 (19%)\n230 (16%)\n\n\nAGE\n40 (15)\n55 (16)\n\n\nEDUCATION\n\n\n\n\n\n\n    LessThanHS\n151 (13%)\n222 (15%)\n\n\n    HS\n250 (22%)\n343 (24%)\n\n\n    MoreThanHS\n732 (65%)\n886 (61%)\n\n\nSMOKE\n\n\n\n\n\n\n    NeverSmoke\n678 (60%)\n733 (51%)\n\n\n    QuitSmoke\n220 (19%)\n411 (28%)\n\n\n    StillSmoke\n235 (21%)\n307 (21%)\n\n\nBMI_CAT\n\n\n\n\n\n\n    BMI&lt;=25\n392 (35%)\n271 (19%)\n\n\n    25&lt;BMI&lt;30\n351 (31%)\n457 (31%)\n\n\n    BMI&gt;=30\n390 (34%)\n723 (50%)\n\n\nLEAD\n1.03 (1.15)\n1.37 (1.25)\n\n\nSBP1\n112 (10)\n134 (18)\n\n\nDBP1\n67 (9)\n77 (14)\n\n\n\n1 n (%); Mean (SD)\n\n\n\n\n\n\n\n\nOutside of the gt and gtsummary packages, another common package used to create summary tables is the tableone package (Yoshida and Bartel 2022), which is not covered in this book.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#recap-video",
    "href": "book/exploratory_analysis.html#recap-video",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.5 Recap Video",
    "text": "4.5 Recap Video",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/exploratory_analysis.html#exercises",
    "href": "book/exploratory_analysis.html#exercises",
    "title": "4  Intro to Exploratory Data Analysis",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nFor these exercises, we continue using the nhanes_df data.\n\nUsing both numerical and graphical summaries, describe the distribution of the first diastolic blood pressure reading DBP1among study participants. Then, create a column called INCOME_CAT with two categories: “low” for those whose income is at most 2 and “not low” otherwise and examine the bivariate distribution of DBP1 and INCOME_CAT. Arrange the two plots next to each other. What do you notice?\nCreate a subset of the data containing only adults between the ages of 20 and 55, inclusive. Then, explore how blood pressure varies by age and gender among this age group. Is there a visible trend in blood pressure with increasing age among either sex?\nFor males between the ages of 50-59, compare blood pressure across race as reported in the race column. Then, create a summary table stratified by the race column and report the mean, standard deviation, minimum, and maximum values for all continuous columns.\nRecreate the plots in Figure 4.3 and Figure 4.4. Based on these plots, what trend do you expect to see in blood lead levels over time? Check your answer to the previous question by plotting these two columns against each other.\n\n\n\n\n\n\n\nFigure 4.3: Education Levels Over Time.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Blood Lead Level by Education Level.\n\n\n\n\n\n\n\nCenters for Disease Control and Prevention (CDC). 1999-2018. “National Health and Nutrition Examination Survey Data (NHANES).” U.S. Department of Health; Human Services. http://www.cdc.gov/nchs/nhanes.htm.\n\n\nHuang, Ziyao. 2022. “Association Between Blood Lead Level with High Blood Pressure in US (NHANES 1999–2018).” Frontiers in Public Health 10: 836357.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2023. gt: Easily Create Presentation-Ready Display Tables, Url = https://CRAN.R-project.org/package=gt.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery, Karissa Whiting, and Emily C. Zabor. 2023. gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://CRAN.R-project.org/package=gtsummary.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create ’Table 1’ to Describe Baseline Characteristics with or Without Propensity Score Weights. https://CRAN.R-project.org/package=tableone.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#tibbles-and-data-frames",
    "href": "book/data_transformations_summaries.html#tibbles-and-data-frames",
    "title": "5  Data Transformations and Summaries",
    "section": "5.1 Tibbles and Data Frames",
    "text": "5.1 Tibbles and Data Frames\nTake a look at the class of NHANESsample. As we might expect, the data is stored as a data frame.\n\nclass(NHANESsample)\n#&gt; [1] \"data.frame\"\n\nHowever, tidyverse packages also work with another data structure called a tibble. A tibble has all the properties of data frames that we have learned so far, but they are a more modern version of a data frame. To convert our data to this data structure we use the as_tibble() function. In practice, there are only very slight differences between the two data structures, and you generally do not need to convert data frames to tibbles. In the following code chunks, we convert our data from a data frame to a tibble and print the head of the data before converting it back to a data frame and repeating. You can see the two structures have a slightly different print statement but are otherwise very similar.\n\nnhanes_df &lt;- as_tibble(NHANESsample)\nprint(head(nhanes_df))\n#&gt; # A tibble: 6 × 21\n#&gt;      ID   AGE SEX    RACE     EDUCATION INCOME SMOKE  YEAR  LEAD BMI_CAT\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n#&gt; 1     2    77 Male   Non-His… MoreThan…   5    Neve…  1999   5   BMI&lt;=25\n#&gt; 2     5    49 Male   Non-His… MoreThan…   5    Quit…  1999   1.6 25&lt;BMI…\n#&gt; 3    12    37 Male   Non-His… MoreThan…   4.93 Neve…  1999   2.4 BMI&gt;=30\n#&gt; 4    13    70 Male   Mexican… LessThan…   1.07 Quit…  1999   1.6 25&lt;BMI…\n#&gt; 5    14    81 Male   Non-His… LessThan…   2.67 Stil…  1999   5.5 25&lt;BMI…\n#&gt; 6    15    38 Female Non-His… MoreThan…   4.52 Stil…  1999   1.5 25&lt;BMI…\n#&gt; # ℹ 11 more variables: LEAD_QUANTILE &lt;fct&gt;, HYP &lt;dbl&gt;, ALC &lt;chr&gt;,\n#&gt; #   DBP1 &lt;dbl&gt;, DBP2 &lt;dbl&gt;, DBP3 &lt;dbl&gt;, DBP4 &lt;dbl&gt;, SBP1 &lt;dbl&gt;,\n#&gt; #   SBP2 &lt;dbl&gt;, SBP3 &lt;dbl&gt;, SBP4 &lt;dbl&gt;\n\n\nnhanes_df &lt;- as.data.frame(nhanes_df)\nprint(head(nhanes_df))\n#&gt;   ID AGE    SEX               RACE  EDUCATION INCOME      SMOKE YEAR\n#&gt; 1  2  77   Male Non-Hispanic White MoreThanHS   5.00 NeverSmoke 1999\n#&gt; 2  5  49   Male Non-Hispanic White MoreThanHS   5.00  QuitSmoke 1999\n#&gt; 3 12  37   Male Non-Hispanic White MoreThanHS   4.93 NeverSmoke 1999\n#&gt; 4 13  70   Male   Mexican American LessThanHS   1.07  QuitSmoke 1999\n#&gt; 5 14  81   Male Non-Hispanic White LessThanHS   2.67 StillSmoke 1999\n#&gt; 6 15  38 Female Non-Hispanic White MoreThanHS   4.52 StillSmoke 1999\n#&gt;   LEAD   BMI_CAT LEAD_QUANTILE HYP ALC DBP1 DBP2 DBP3 DBP4 SBP1 SBP2\n#&gt; 1  5.0   BMI&lt;=25            Q4   0 Yes   58   56   56   NA  106   98\n#&gt; 2  1.6 25&lt;BMI&lt;30            Q3   1 Yes   82   84   82   NA  122  122\n#&gt; 3  2.4   BMI&gt;=30            Q4   1 Yes  108   98  100   NA  182  172\n#&gt; 4  1.6 25&lt;BMI&lt;30            Q3   1 Yes   78   62   70   NA  140  130\n#&gt; 5  5.5 25&lt;BMI&lt;30            Q4   1 Yes   56   NA   58   64  142   NA\n#&gt; 6  1.5 25&lt;BMI&lt;30            Q3   0 Yes   68   68   70   NA  106  112\n#&gt;   SBP3 SBP4\n#&gt; 1   98   NA\n#&gt; 2  122   NA\n#&gt; 3  176   NA\n#&gt; 4  130   NA\n#&gt; 5  134  138\n#&gt; 6  106   NA\n\nWe mention tibbles here since some functions in the tidyverse convert data frames to tibbles in their output. In particular, when we later summarize over groups we can expect a tibble to be returned. It is useful to be aware that our data may change data structure with such functions and to know that we can always convert back if needed.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#subsetting-data",
    "href": "book/data_transformations_summaries.html#subsetting-data",
    "title": "5  Data Transformations and Summaries",
    "section": "5.2 Subsetting Data",
    "text": "5.2 Subsetting Data\nIn earlier chapters, we have seen how to select and filter data using row and column indices as well as using the subset() function. The dplyr package has its own functions that are useful for subsetting data. The select() function allows us to select a subset of columns: this function takes in the data frame (or tibble) and the names or indices of the columns we want to select. For example, if we only wanted to select the variables for race and blood lead level, we could specify these two columns. To display the result of this selection, we use the pipe operator %&gt;% from the magittr library of the tidyverse. Similar to the pipe operator |&gt; in base R, the pipe operator %&gt;% takes the result on the left hand side and passes it as the first argument to the function on the right hand side. The following output shows that there are only two columns in the filtered data.\n\nselect(nhanes_df, c(RACE, LEAD)) %&gt;% head()\n#&gt;                 RACE LEAD\n#&gt; 1 Non-Hispanic White  5.0\n#&gt; 2 Non-Hispanic White  1.6\n#&gt; 3 Non-Hispanic White  2.4\n#&gt; 4   Mexican American  1.6\n#&gt; 5 Non-Hispanic White  5.5\n#&gt; 6 Non-Hispanic White  1.5\n\nThe select() function can also be used to remove columns by adding a negative sign in front of the vector of column names in its arguments. For example, we keep all columns except ID and LEAD_QUANTILE. Note that in this case we have saved the selected data back to our data frame nhanes_df. Additionally, this time we used a pipe operator to pipe the data to the select function itself.\n\nnhanes_df &lt;- nhanes_df %&gt;% select(-c(ID, LEAD_QUANTILE))\nnames(nhanes_df)\n#&gt;  [1] \"AGE\"       \"SEX\"       \"RACE\"      \"EDUCATION\" \"INCOME\"   \n#&gt;  [6] \"SMOKE\"     \"YEAR\"      \"LEAD\"      \"BMI_CAT\"   \"HYP\"      \n#&gt; [11] \"ALC\"       \"DBP1\"      \"DBP2\"      \"DBP3\"      \"DBP4\"     \n#&gt; [16] \"SBP1\"      \"SBP2\"      \"SBP3\"      \"SBP4\"\n\nWhile select() allows us to choose a subset of columns, the filter() function allows us to choose a subset of rows. The filter() function takes a data frame as the first argument and a vector of booleans as the second argument. This vector of booleans can be generated using conditional statements as we used in Chapter 4. We choose to filter the data to only observations after 2008.\n\nnhanes_df_recent &lt;- nhanes_df %&gt;% filter(YEAR &gt;= 2008)\n\nWe can combine conditions by using multiple filter() calls, by creating a more complicated conditional statement using the & (and), | (or), and %in% (in) operators, or by separating the conditions with commas within filter. In the following code, we demonstrate these three ways to filter the data to males between 2008 and 2012. Note that the between() function allows us to capture the logic YEAR &gt;= 2008 & YEAR &lt;= 2012.\n\n# Example 1: multiple filter calls\nnhanes_df_males1 &lt;- nhanes_df %&gt;%\n  filter(YEAR &lt;= 2012) %&gt;%\n  filter(YEAR &gt;= 2008) %&gt;%\n  filter(SEX == \"Male\")\n\n# Example 2: combine with & operator\nnhanes_df_males2 &lt;- nhanes_df %&gt;%\n  filter((YEAR &lt;= 2012) & (YEAR &gt;= 2008) & (SEX == \"Male\"))\n\n# Example 3: combine into one filter call with commas\nnhanes_df_males3 &lt;- nhanes_df %&gt;%\n  filter(between(YEAR, 2008, 2012), SEX == \"Male\")\n\nThe use of parentheses in the previous code is especially important in order to capture our desired logic. In all these examples, we broke our code up into multiple lines, which makes it easier to read. A good rule of thumb is to not go past 80 characters in a line, and R Studio conveniently has a vertical gray line at this limit. To create a new line, you can hit enter either after an operator (e.g. %&gt;%, +, |) or within a set of unfinished brackets or parentheses. Either of these breaks lets R know that your code is not finished yet.\nLastly, we can subset the data using the slice() function to select a slice of rows by their index. The function takes in the data set and a vector of indices. In the following example, we find the first and last rows of the data.\n\nslice(nhanes_df, c(1, nrow(nhanes_df)))\n#&gt;   AGE  SEX               RACE  EDUCATION INCOME      SMOKE YEAR LEAD\n#&gt; 1  77 Male Non-Hispanic White MoreThanHS   5.00 NeverSmoke 1999  5.0\n#&gt; 2  38 Male Non-Hispanic White MoreThanHS   1.56 StillSmoke 2017  0.9\n#&gt;   BMI_CAT HYP ALC DBP1 DBP2 DBP3 DBP4 SBP1 SBP2 SBP3 SBP4\n#&gt; 1 BMI&lt;=25   0 Yes   58   56   56   NA  106   98   98   NA\n#&gt; 2 BMI&gt;=30   1 Yes   98   92   98   NA  150  146  148   NA\n\nA few other useful slice functions are slice_sample(), slice_max(), and slice_min(). The first takes in an argument n which specifies the number of random rows to sample from the data. For example, we could randomly sample 100 rows from our data. The latter two allow us to specify a column through the argument order_by and return the n rows with either the highest or lowest values in that column. For example, we can find the three male observations from 2007 with the highest and lowest blood lead levels and select a subset of columns to display.\n\n# three male observations with highest blood lead level in 2007\nnhanes_df %&gt;%\n  filter(YEAR == 2007, SEX == \"Male\") %&gt;%\n  select(c(RACE, EDUCATION, SMOKE, LEAD, SBP1, DBP1)) %&gt;%\n  slice_max(order_by = LEAD, n = 3)\n#&gt;                 RACE  EDUCATION      SMOKE LEAD SBP1 DBP1\n#&gt; 1 Non-Hispanic Black LessThanHS NeverSmoke 33.1  106   66\n#&gt; 2     Other Hispanic LessThanHS StillSmoke 26.8  106   72\n#&gt; 3     Other Hispanic LessThanHS StillSmoke 25.7  112   60\n\n# three male observations with lowest blood lead level in 2007\nnhanes_df %&gt;%\n  filter(YEAR == 2007, SEX == \"Male\") %&gt;%\n  select(c(RACE, EDUCATION, SMOKE, LEAD, SBP1, DBP1)) %&gt;%\n  slice_min(order_by = LEAD, n = 3)\n#&gt;                 RACE  EDUCATION      SMOKE  LEAD SBP1 DBP1\n#&gt; 1 Non-Hispanic White LessThanHS NeverSmoke 0.177  114   80\n#&gt; 2     Other Hispanic LessThanHS  QuitSmoke 0.280  122   62\n#&gt; 3   Mexican American MoreThanHS  QuitSmoke 0.320  112   66\n\n\n5.2.1 Practice Question\nFilter the data to only those with an education level of more than HS who report alcohol use. Then, select only the diastolic blood pressure variables and display the 4th and 10th rows. Your result should match the result in Figure 5.1.\n\n\n\n\n\n\nFigure 5.1: Filtering and Selecting Data.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#updating-rows-and-columns",
    "href": "book/data_transformations_summaries.html#updating-rows-and-columns",
    "title": "5  Data Transformations and Summaries",
    "section": "5.3 Updating Rows and Columns",
    "text": "5.3 Updating Rows and Columns\nThe next few functions we look at allow us to update the rows and columns in our data. For example, the rename() function allows us to change the names of columns. In the following code, we change the name of INCOME to PIR since this variable is the poverty income ratio and also update the name of SMOKE to be SMOKE_STATUS. When specifying these names, the new name is on the left of the = and the old name is on the right.\n\nnhanes_df &lt;- nhanes_df %&gt;% rename(PIR = INCOME, SMOKE_STATUS = SMOKE)\nnames(nhanes_df)\n#&gt;  [1] \"AGE\"          \"SEX\"          \"RACE\"         \"EDUCATION\"   \n#&gt;  [5] \"PIR\"          \"SMOKE_STATUS\" \"YEAR\"         \"LEAD\"        \n#&gt;  [9] \"BMI_CAT\"      \"HYP\"          \"ALC\"          \"DBP1\"        \n#&gt; [13] \"DBP2\"         \"DBP3\"         \"DBP4\"         \"SBP1\"        \n#&gt; [17] \"SBP2\"         \"SBP3\"         \"SBP4\"\n\nIn the last chapter, we created a new variable called EVER_SMOKE based on the smoking status variable using the ifelse() function. Recall that this function allows us to specify a condition and then two alternative values based on whether we meet or do not meet this condition. We see that there are about 15,000 subjects in our data who never smoked.\n\nifelse(nhanes_df$SMOKE_STATUS == \"NeverSmoke\", \"No\", \"Yes\") %&gt;% \n  table()\n#&gt; .\n#&gt;    No   Yes \n#&gt; 15087 16178\n\nAnother useful function from the tidyverse is the case_when() function, which is an extension of the ifelse() function but allows to specify more than two cases. We demonstrate this function to show how we could relabel the levels of the SMOKE_STATUS column. For each condition, we use the right side of the ~ to specify the value to be assigned when that condition is TRUE.\n\ncase_when(nhanes_df$SMOKE_STATUS == \"NeverSmoke\" ~ \"Never Smoked\",\n          nhanes_df$SMOKE_STATUS == \"QuitSmoke\" ~ \"Quit Smoking\",\n          nhanes_df$SMOKE_STATUS == \n            \"StillSmoke\" ~ \"Current Smoker\") %&gt;% \n  table()\n#&gt; .\n#&gt; Current Smoker   Never Smoked   Quit Smoking \n#&gt;           7317          15087           8861\n\nIn the previous example, we did not store the columns we created. To do so, we could use the $ operator or the cbind() function. The tidyverse also includes an alternative function to add columns called mutate(). This function takes in a data frame and a set of columns with associated names to add to the data or update. In the subsequent example, we create the column EVER_SMOKE and update the column SMOKE_STATUS. Within the mutate() function, we do not have to use the $ operator to reference the column SMOKE_STATUS. Instead, we can specify just the column name and the function interprets it as that column.\n\nnhanes_df &lt;- nhanes_df %&gt;% \n  mutate(EVER_SMOKE = ifelse(SMOKE_STATUS == \"NeverSmoke\", \n                             \"No\", \"Yes\"), \n         SMOKE_STATUS = \n           case_when(SMOKE_STATUS == \"NeverSmoke\" ~ \"Never Smoked\",\n                     SMOKE_STATUS == \"QuitSmoke\" ~ \"Quit Smoking\",\n                     SMOKE_STATUS == \"StillSmoke\" ~ \"Current Smoker\")) \n\nThe last function we demonstrate in this section is the arrange() function, which takes in a data frame and a vector of columns used to sort the data (data is sorted by the first column with ties being sorted by the second column, etc.). By default, the arrange() function sorts the data in increasing order, but we can use the desc() function to instead sort in descending order. For example, the following code filters the data to male smokers before sorting by decreasing systolic and diastolic blood pressure in descending order. That is, the value of DBP1 is used to sort rows that have the same systolic blood pressure values.\n\nnhanes_df %&gt;% \n  select(c(YEAR, SEX, SMOKE_STATUS, SBP1, DBP1, LEAD)) %&gt;%\n  filter(SEX == \"Male\", SMOKE_STATUS == \"Current Smoker\") %&gt;%\n  arrange(desc(SBP1), desc(DBP1)) %&gt;%\n  head(8)\n#&gt;   YEAR  SEX   SMOKE_STATUS SBP1 DBP1 LEAD\n#&gt; 1 2011 Male Current Smoker  230  120 5.84\n#&gt; 2 2015 Male Current Smoker  230   98 1.56\n#&gt; 3 2009 Male Current Smoker  220   80 4.84\n#&gt; 4 2001 Male Current Smoker  218  118 3.70\n#&gt; 5 2017 Male Current Smoker  212  122 2.20\n#&gt; 6 2003 Male Current Smoker  212   54 4.00\n#&gt; 7 2011 Male Current Smoker  210   92 5.37\n#&gt; 8 2007 Male Current Smoker  210   80 2.18\n\nIf instead we had only sorted by SBP1 then the rows with the same value for systolic blood pressure would appear in their original order. You can see the difference in the following output.\n\nnhanes_df %&gt;% \n  select(c(YEAR, SEX, SMOKE_STATUS, SBP1, DBP1, LEAD)) %&gt;%\n  filter(SEX == \"Male\", SMOKE_STATUS == \"Current Smoker\") %&gt;%\n  arrange(desc(SBP1)) %&gt;%\n  head(8)\n#&gt;   YEAR  SEX   SMOKE_STATUS SBP1 DBP1 LEAD\n#&gt; 1 2011 Male Current Smoker  230  120 5.84\n#&gt; 2 2015 Male Current Smoker  230   98 1.56\n#&gt; 3 2009 Male Current Smoker  220   80 4.84\n#&gt; 4 2001 Male Current Smoker  218  118 3.70\n#&gt; 5 2003 Male Current Smoker  212   54 4.00\n#&gt; 6 2017 Male Current Smoker  212  122 2.20\n#&gt; 7 2007 Male Current Smoker  210   80 2.18\n#&gt; 8 2011 Male Current Smoker  210   92 5.37\n\n\n5.3.1 Practice Question\nCreate a new column called DBP_CHANGE that is equal to the difference between a patient’s first and fourth diastolic blood pressure readings. Then, sort the data frame by this new column in increasing order and print the first four rows. The first four DBP_CHANGE values in the head of the resulting data frame should be -66, -64, -64, and -62.\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#summarizing-and-grouping",
    "href": "book/data_transformations_summaries.html#summarizing-and-grouping",
    "title": "5  Data Transformations and Summaries",
    "section": "5.4 Summarizing and Grouping",
    "text": "5.4 Summarizing and Grouping\nIf we want to understand how many observations there are for each given race category, we could use the table() function as we described in earlier chapters. Another similar function is the count() function. This function takes in a data frame and one or more columns and counts the number of rows for each combination of unique values in these columns. If no columns are specified, it counts the total number of rows in the data frame. In the following code, we find the total number of rows (31,265) and the number of observations by race and year. We can see that the number in each group fluctuates quite a bit!\n\ncount(nhanes_df)\n#&gt;       n\n#&gt; 1 31265\ncount(nhanes_df, RACE, YEAR)\n#&gt;                  RACE YEAR    n\n#&gt; 1    Mexican American 1999  713\n#&gt; 2    Mexican American 2001  674\n#&gt; 3    Mexican American 2003  627\n#&gt; 4    Mexican American 2005  634\n#&gt; 5    Mexican American 2007  639\n#&gt; 6    Mexican American 2009  672\n#&gt; 7    Mexican American 2011  322\n#&gt; 8    Mexican American 2013  234\n#&gt; 9    Mexican American 2015  287\n#&gt; 10   Mexican American 2017  475\n#&gt; 11     Other Hispanic 1999  181\n#&gt; 12     Other Hispanic 2001  129\n#&gt; 13     Other Hispanic 2003   80\n#&gt; 14     Other Hispanic 2005   96\n#&gt; 15     Other Hispanic 2007  395\n#&gt; 16     Other Hispanic 2009  367\n#&gt; 17     Other Hispanic 2011  337\n#&gt; 18     Other Hispanic 2013  167\n#&gt; 19     Other Hispanic 2015  214\n#&gt; 20     Other Hispanic 2017  313\n#&gt; 21 Non-Hispanic White 1999 1401\n#&gt; 22 Non-Hispanic White 2001 1882\n#&gt; 23 Non-Hispanic White 2003 1785\n#&gt; 24 Non-Hispanic White 2005 1818\n#&gt; 25 Non-Hispanic White 2007 1940\n#&gt; 26 Non-Hispanic White 2009 2169\n#&gt; 27 Non-Hispanic White 2011 1463\n#&gt; 28 Non-Hispanic White 2013  917\n#&gt; 29 Non-Hispanic White 2015  685\n#&gt; 30 Non-Hispanic White 2017 1413\n#&gt; 31 Non-Hispanic Black 1999  463\n#&gt; 32 Non-Hispanic Black 2001  542\n#&gt; 33 Non-Hispanic Black 2003  576\n#&gt; 34 Non-Hispanic Black 2005  679\n#&gt; 35 Non-Hispanic Black 2007  728\n#&gt; 36 Non-Hispanic Black 2009  661\n#&gt; 37 Non-Hispanic Black 2011  876\n#&gt; 38 Non-Hispanic Black 2013  357\n#&gt; 39 Non-Hispanic Black 2015  351\n#&gt; 40 Non-Hispanic Black 2017  808\n#&gt; 41         Other Race 1999   76\n#&gt; 42         Other Race 2001   88\n#&gt; 43         Other Race 2003  109\n#&gt; 44         Other Race 2005  122\n#&gt; 45         Other Race 2007  123\n#&gt; 46         Other Race 2009  175\n#&gt; 47         Other Race 2011  475\n#&gt; 48         Other Race 2013  223\n#&gt; 49         Other Race 2015  209\n#&gt; 50         Other Race 2017  595\n\nFinding the counts like we did previously is a form of a summary statistic for our data. The summarize() function in the tidyverse is used to compute summary statistics of the data and allows us to compute multiple statistics: this function takes in a data frame and one or more summary functions based on the given column names. In the subsequent example, we find the total number of observations as well as the mean and median systolic blood pressure for Non-Hispanic Blacks. Note that the n() function is the function within summarize() that finds the number of observations. In the mean() and median() functions we set na.rm=TRUE to remove NAs before computing these values (otherwise we could get NA as our output).\n\nnhanes_df %&gt;%\n  filter(RACE == \"Non-Hispanic Black\") %&gt;%\n  summarize(TOT = n(), MEAN_SBP = mean(SBP1, na.rm=TRUE), \n            MEAN_DBP = mean(DBP1, na.rm=TRUE))\n#&gt;    TOT MEAN_SBP MEAN_DBP\n#&gt; 1 6041      129     72.6\n\nIf we wanted to repeat this for the other race groups, we would have to change the arguments to the filter() function each time. To avoid having to repeat our code and/or do this multiple times, we can use the group_by() function, which takes a data frame and one or more columns with which to group the data. In the following code, we group using the RACE variable. When we look at printed output it looks almost the same as it did before except we can see that its class is now a grouped data frame, which is printed at the top. In fact, a grouped data frame (or grouped tibble) acts like a set of data frames: one for each group. If we use the slice() function with index 1, it returns the first row for each group.\n\nnhanes_df %&gt;% \n  group_by(RACE) %&gt;%\n  slice(1)\n#&gt; # A tibble: 5 × 20\n#&gt; # Groups:   RACE [5]\n#&gt;     AGE SEX    RACE     EDUCATION   PIR SMOKE_STATUS  YEAR  LEAD BMI_CAT\n#&gt;   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n#&gt; 1    70 Male   Mexican… LessThan…  1.07 Quit Smoking  1999   1.6 25&lt;BMI…\n#&gt; 2    61 Female Other H… MoreThan…  3.33 Current Smo…  1999   2.2 BMI&lt;=25\n#&gt; 3    77 Male   Non-His… MoreThan…  5    Never Smoked  1999   5   BMI&lt;=25\n#&gt; 4    38 Female Non-His… HS         0.92 Current Smo…  1999   1.8 25&lt;BMI…\n#&gt; 5    63 Female Other R… MoreThan…  5    Never Smoked  1999   1.2 BMI&lt;=25\n#&gt; # ℹ 11 more variables: HYP &lt;dbl&gt;, ALC &lt;chr&gt;, DBP1 &lt;dbl&gt;, DBP2 &lt;dbl&gt;,\n#&gt; #   DBP3 &lt;dbl&gt;, DBP4 &lt;dbl&gt;, SBP1 &lt;dbl&gt;, SBP2 &lt;dbl&gt;, SBP3 &lt;dbl&gt;,\n#&gt; #   SBP4 &lt;dbl&gt;, EVER_SMOKE &lt;chr&gt;\n\nGrouping data is very helpful in combination with the summarize() function. Like with the slice() function, summarize() calculates the summary values for each group. We can now find the total number of observations as well as the mean systolic and diastolic blood pressure values for each racial group. Note that the returned summarized data is in a tibble.\n\nnhanes_df %&gt;% \n  group_by(RACE) %&gt;%\n  summarize(TOT = n(), MEAN_SBP = mean(SBP1, na.rm=TRUE), \n            MEAN_DBP = mean(DBP1, na.rm=TRUE))\n#&gt; # A tibble: 5 × 4\n#&gt;   RACE                 TOT MEAN_SBP MEAN_DBP\n#&gt;   &lt;fct&gt;              &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Mexican American    5277     124.     70.4\n#&gt; 2 Other Hispanic      2279     123.     70.1\n#&gt; 3 Non-Hispanic White 15473     125.     70.4\n#&gt; 4 Non-Hispanic Black  6041     129.     72.6\n#&gt; 5 Other Race          2195     122.     72.6\n\nAfter summarizing, the data is no longer grouped by race. If we ever want to remove the group structure from our data, we can use the ungroup() function, which restores the data to a single data frame. After ungrouping by race, we can see that we get a single observation returned by the slice() function.\n\nnhanes_df %&gt;% \n  select(SEX, RACE, SBP1, DBP1) %&gt;%\n  group_by(RACE) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(SBP1)) %&gt;%\n  slice(1)\n#&gt; # A tibble: 1 × 4\n#&gt;   SEX    RACE                SBP1  DBP1\n#&gt;   &lt;fct&gt;  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female Non-Hispanic White   270   124\n\n\n5.4.1 Practice Question\nCreate a data frame summarizing the percent of patients with hypertension by smoking status. The result should look like Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: Grouping and Summarizing Data.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#recap-video",
    "href": "book/data_transformations_summaries.html#recap-video",
    "title": "5  Data Transformations and Summaries",
    "section": "5.5 Recap Video",
    "text": "5.5 Recap Video",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/data_transformations_summaries.html#exercises",
    "href": "book/data_transformations_summaries.html#exercises",
    "title": "5  Data Transformations and Summaries",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nThe following exercises use the covidcases dataset from the HDSinRdata package. Before completing the exercises, be sure to read the documentation for this data (?covidcases).\n\ndata(covidcases)\n\n\nSuppose we are interested in the distribution of weekly cases by state. First, create a new column in covidcases called region specifying whether each state is in the Northeast, Midwest, South, or West (you can either do this by hand using this list of which states are in which region or you can use state.region from the datasets package in R). Then, create a data frame summarizing the average and standard deviation of the weekly cases for the Northeast.\nNow, create a data frame with the average and standard deviation summarized for each region rather than for just one selected region as in Question 1. Sort this data frame from highest to lowest average weekly cases. What other information would you need in order to more accurately compare these regions in terms of their average cases?\nFind the ten counties in the Midwest with the lowest weekly deaths in week 15 of this data ignoring ties (use slice_min() to find the argument needed for this). What do you notice about the minimum values? See the data documentation for why we observe these values.\nFilter the data to include weeks 9 and 20 (around the start of the pandemic), get the total cases per county during that time frame, and then find the county in each state that had the highest number of total cases.\n\n\n\n\n\nCenters for Disease Control and Prevention (CDC). 1999-2018. “National Health and Nutrition Examination Survey Data (NHANES).” U.S. Department of Health; Human Services. http://www.cdc.gov/nchs/nhanes.htm.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Transformations and Summaries</span>"
    ]
  },
  {
    "objectID": "book/cs_preprocessing.html",
    "href": "book/cs_preprocessing.html",
    "title": "6  Case Study: Pre-Processing Data",
    "section": "",
    "text": "In this chapter, we put some of our R skills together in a case study. This case study focuses on data cleaning and pre-processing. We use the tb_diagnosis_raw data from the HDSinRdata package. This data contains information on 1,634 patients in rural South Africa who presented at a health clinic with tuberculosis-related symptoms and were tested for tuberculosis (TB) using Xpert MTB/RIF. Our goal is to clean this data to reflect the pre-processing described in Baik et al. (2020). This paper uses this data to derive a simple risk score model for screening patients for treatment while awaiting Xpert results. We use the tidyverse packages as well as the summary tables from gtsummary.\n\nlibrary(HDSinRdata)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtsummary)\n\nTo begin, read in the data and review the description of the original columns. Some things to note in the data documentation are the ways unknown, missing, or refused values are coded as well as how some of the columns are related to each other.\n\n# Read in data\ndata(\"tb_diagnosis_raw\")\n\n# Inspect variable descriptions\n# ?tb_diagnosis_raw\n\nTo start, we select variables needed for our analysis. In particular, we drop columns related to the participation in the survey and about seeking care. Since some of these variables contain long or vague names, we also rename most of the variables.\n\n# Select variables and rename\ntb_df &lt;- tb_diagnosis_raw %&gt;% \n  select(c(xpert_status_fac, age_group, sex, hiv_status_fac,\n           other_conditions_fac___1, other_conditions_fac___3,\n           other_conditions_fac___88, other_conditions_fac___99,\n           symp_fac___1, symp_fac___2, symp_fac___3, symp_fac___4, \n           symp_fac___99, length_symp_unit_fac, length_symp_days_fac,\n           length_symp_wk_fac, length_symp_mnt_fac, length_symp_yr_fac,\n           smk_fac, dx_tb_past_fac, educ_fac)) %&gt;%\n    rename(tb = xpert_status_fac, hiv_pos = hiv_status_fac,\n           cough = symp_fac___1, fever = symp_fac___2, \n           weight_loss = symp_fac___3, night_sweats = symp_fac___4, \n           symptoms_missing = symp_fac___99,\n           ever_smoke = smk_fac, \n           past_tb = dx_tb_past_fac, education = educ_fac)\n\nWe then use a summary table to understand the initial distributions of the variables observed. This also highlights where we have missing or unknown data.\n\ntbl_summary(tb_df) %&gt;%\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1,6341\n\n\n\n\ntb\n\n\n\n\n    1\n765 (47%)\n\n\n    2\n869 (53%)\n\n\nage_group\n\n\n\n\n    [15,25)\n240 (15%)\n\n\n    [25,35)\n333 (20%)\n\n\n    [35,45)\n385 (24%)\n\n\n    [45,55)\n343 (21%)\n\n\n    [55,99)\n333 (20%)\n\n\nsex\n\n\n\n\n    1\n830 (51%)\n\n\n    2\n804 (49%)\n\n\nhiv_pos\n\n\n\n\n    1\n632 (39%)\n\n\n    2\n815 (50%)\n\n\n    77\n139 (8.5%)\n\n\n    88\n48 (2.9%)\n\n\nother_conditions_fac___1\n895 (55%)\n\n\nother_conditions_fac___3\n52 (3.2%)\n\n\nother_conditions_fac___88\n11 (0.7%)\n\n\nother_conditions_fac___99\n30 (1.8%)\n\n\ncough\n1,279 (78%)\n\n\nfever\n479 (29%)\n\n\nweight_loss\n534 (33%)\n\n\nnight_sweats\n579 (35%)\n\n\nsymptoms_missing\n22 (1.3%)\n\n\nlength_symp_unit_fac\n\n\n\n\n    1\n207 (14%)\n\n\n    2\n603 (39%)\n\n\n    3\n538 (35%)\n\n\n    4\n83 (5.4%)\n\n\n    77\n98 (6.4%)\n\n\n    Unknown\n105\n\n\nlength_symp_days_fac\n3 (3, 4)\n\n\n    Unknown\n1,427\n\n\nlength_symp_wk_fac\n\n\n\n\n    1\n183 (30%)\n\n\n    2\n237 (39%)\n\n\n    3\n147 (24%)\n\n\n    4\n15 (2.5%)\n\n\n    5\n5 (0.8%)\n\n\n    6\n13 (2.2%)\n\n\n    7\n3 (0.5%)\n\n\n    Unknown\n1,031\n\n\nlength_symp_mnt_fac\n2 (1, 3)\n\n\n    Unknown\n1,096\n\n\nlength_symp_yr_fac\n2 (1, 4)\n\n\n    Unknown\n1,551\n\n\never_smoke\n\n\n\n\n    1\n294 (18%)\n\n\n    2\n252 (15%)\n\n\n    3\n1,072 (66%)\n\n\n    99\n16 (1.0%)\n\n\npast_tb\n\n\n\n\n    1\n255 (16%)\n\n\n    2\n1,354 (83%)\n\n\n    77\n25 (1.5%)\n\n\neducation\n10 (7, 12)\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\nOne observation from the table is that the coding of variables is inconsistent, with some using 0/1 and others using 1/2. We want to standardize how these variables are represented. To start, we update our tb column. Additionally, we create a column male from the previous column sex to make the reference level clear. We can then drop the sex column.\n\n# Re-code binary variables to 0/1 instead of 1/2\ntb_df$tb &lt;- case_when(tb_df$tb == 1 ~ 1, \n                      tb_df$tb == 2 ~ 0)\n\ntb_df$male &lt;- case_when(tb_df$sex == 1 ~ 1, \n                        tb_df$sex == 2 ~ 0)\ntb_df &lt;- tb_df %&gt;% select(-c(sex))\n\nDiabetes is another variable that should be coded this way. In the raw data, several columns correspond to this question about other medical conditions. Therefore, we need to use the columns other_conditions_fac___88 and other_conditions_fac___99 to check whether the participant did not answer the question when interpreting the 0/1 value for diabetes.\n\n# Re-code diabetes to check if missing\ntb_df$diabetes &lt;- case_when(tb_df$other_conditions_fac___3 == 1 ~ 1,\n                            tb_df$other_conditions_fac___1 == 1 ~ 0,\n                            tb_df$other_conditions_fac___88 == 1 ~ NA,\n                            tb_df$other_conditions_fac___99 == 1 ~ NA,\n                            TRUE ~ 0)\ntb_df &lt;- tb_df %&gt;% select(-c(other_conditions_fac___1,\n                             other_conditions_fac___3,\n                             other_conditions_fac___88,\n                             other_conditions_fac___99))\ntable(tb_df$diabetes)\n#&gt; \n#&gt;    0    1 \n#&gt; 1541   52\n\nNext, we similarly code our variables about HIV status, smoking, and whether the patient has ever been diagnosed with tuberculosis before. For these variables, if the patient answered that they did not know if their HIV status or had tested positive for TB, we code these as 0 to be consistent with the paper.\n\n# Re-code variables with missing or refused values\ntb_df$hiv_pos &lt;- case_when((tb_df$hiv_pos == 1) ~ 1, \n                           tb_df$hiv_pos %in% c(2,77) ~ 0,\n                           tb_df$hiv_pos == 88 ~ NA)\n\ntb_df$ever_smoke &lt;- case_when(tb_df$ever_smoke %in% c(1,2) ~ 1,\n                              tb_df$ever_smoke == 3 ~ 0,\n                              tb_df$ever_smoke == 99 ~ NA)\n\ntb_df$past_tb &lt;- case_when(tb_df$past_tb == 1 ~ 1, \n                           tb_df$past_tb %in% c(2,77) ~ 0)\n\nThe next variable we clean is education. First, we need to code NA values correctly. We can then observe the distribution of years of education.\n\n# Code NA values and look at education distribution\ntb_df$education[tb_df$education == 99] &lt;- NA\nhist(tb_df$education, xlab = \"Years of Education\", \n     main = \"Histogram of Education Years\")\n\n\n\n\n\n\n\n\nFor our purposes, we want to represent education as whether a person has a high school education or less.\n\n# Categorize education to HS and above\ntb_df$hs_less &lt;- case_when(tb_df$education &lt;= 12 ~ 1,\n                           tb_df$education &gt; 12 ~ 0,\n                           TRUE ~ NA)\ntb_df &lt;- tb_df %&gt;% select(-c(education))\n\nThere are several variables in the data related to how long a person has experienced symptoms. In the following code, we can see that the unit of the symptoms, recorded in length_symp_unit_fac, determines which other column is entered. For example, if length_symp_unit_fac == 1, then the only column without an NA value is length_symp_days_fc.\n\ntb_df %&gt;%\n  group_by(length_symp_unit_fac) %&gt;%\n  summarize(missing_days = sum(is.na(length_symp_days_fac))/n(),\n            missing_wks = sum(is.na(length_symp_wk_fac))/n(),\n            missing_mnt = sum(is.na(length_symp_mnt_fac))/n(),\n            missing_yr = sum(is.na(length_symp_yr_fac))/n())\n#&gt; # A tibble: 6 × 5\n#&gt;   length_symp_unit_fac missing_days missing_wks missing_mnt missing_yr\n#&gt;                  &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1                    1            0           1           1          1\n#&gt; 2                    2            1           0           1          1\n#&gt; 3                    3            1           1           0          1\n#&gt; 4                    4            1           1           1          0\n#&gt; 5                   77            1           1           1          1\n#&gt; 6                   NA            1           1           1          1\n\nAdditionally, these measurements are positive integer values.\n\nmin(tb_df$length_symp_days_fac, na.rm = TRUE)\n#&gt; [1] 1\nis.integer(tb_df$length_symp_days_fac)\n#&gt; [1] TRUE\n\nThis allows us to create a new variable that represents whether or not someone has had symptoms for more than two weeks. In our case_when() function call, we first check whether the duration is missing before checking for the cases when symptoms would be less than two weeks.\n\n# Categorize number of weeks experiencing symptoms\ntb_df &lt;- tb_df %&gt;%\n  mutate(case_when((length_symp_unit_fac == 77 |\n                      is.na(length_symp_unit_fac)) ~ NA,\n                   (length_symp_unit_fac == 1 & \n                      length_symp_days_fac &lt;= 14) ~ 0,\n                   (length_symp_unit_fac == 2 &\n                      length_symp_wk_fac &lt;= 2) ~ 0,\n                   TRUE ~ 1))\n\ntb_df &lt;- tb_df %&gt;% \n  select(-c(length_symp_wk_fac, length_symp_days_fac, \n            length_symp_mnt_fac, length_symp_yr_fac, \n            length_symp_unit_fac))\n\nLast, we update our symptom variables to have a summary column num_symptoms that represents the total number of classic TB symptoms rather than keeping track of individual symptoms. We also exclude anyone who does not have any TB symptoms.\n\n# Count total number of symptoms\ntb_df$num_symptoms &lt;- tb_df$fever + tb_df$weight_loss + tb_df$cough + \n  tb_df$night_sweats\ntb_df$num_symptoms[tb_df$symptoms_missing == 1] &lt;- NA\ntb_df &lt;- tb_df %&gt;% select(-c(night_sweats, weight_loss, cough, fever,\n                             symptoms_missing))\n\n# Exclude observations with no TB symptoms \ntb_df &lt;- tb_df %&gt;%\n  filter(num_symptoms != 0)\n\ntable(tb_df$num_symptoms)\n#&gt; \n#&gt;   1   2   3   4 \n#&gt; 600 344 265 196\n\nLast, we convert all variables to factors.\n\n# Convert all variables to factors \ntb_df[] &lt;- lapply(tb_df, function(x){return(as.factor(x))})\n\nOur final data is summarized in the following table. The add_overall() function includes the overall summary statistics in addition to our stratified summaries. Our summary table looks similar to the one in the paper. However, it looks like we have a few more observations included. Additionally, our education variable shows a lower percentage of observations with post-high school education and positive HIV status.\n\ntbl_summary(tb_df, by = \"tb\") %&gt;%\n  add_overall() %&gt;%\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 1,4051\n0, N = 7041\n1, N = 7011\n\n\n\n\nage_group\n\n\n\n\n\n\n\n\n    [15,25)\n205 (15%)\n121 (17%)\n84 (12%)\n\n\n    [25,35)\n286 (20%)\n120 (17%)\n166 (24%)\n\n\n    [35,45)\n338 (24%)\n136 (19%)\n202 (29%)\n\n\n    [45,55)\n305 (22%)\n158 (22%)\n147 (21%)\n\n\n    [55,99)\n271 (19%)\n169 (24%)\n102 (15%)\n\n\nhiv_pos\n\n\n\n\n\n\n\n\n    0\n808 (59%)\n503 (73%)\n305 (45%)\n\n\n    1\n556 (41%)\n186 (27%)\n370 (55%)\n\n\n    Unknown\n41\n15\n26\n\n\never_smoke\n\n\n\n\n\n\n\n\n    0\n899 (64%)\n483 (69%)\n416 (60%)\n\n\n    1\n496 (36%)\n213 (31%)\n283 (40%)\n\n\n    Unknown\n10\n8\n2\n\n\npast_tb\n\n\n\n\n\n\n\n\n    0\n1,186 (84%)\n613 (87%)\n573 (82%)\n\n\n    1\n219 (16%)\n91 (13%)\n128 (18%)\n\n\nmale\n\n\n\n\n\n\n\n\n    0\n669 (48%)\n394 (56%)\n275 (39%)\n\n\n    1\n736 (52%)\n310 (44%)\n426 (61%)\n\n\ndiabetes\n\n\n\n\n\n\n\n\n    0\n1,326 (97%)\n658 (97%)\n668 (96%)\n\n\n    1\n47 (3.4%)\n22 (3.2%)\n25 (3.6%)\n\n\n    Unknown\n32\n24\n8\n\n\nhs_less\n\n\n\n\n\n\n\n\n    0\n119 (8.5%)\n73 (10%)\n46 (6.6%)\n\n\n    1\n1,276 (91%)\n625 (90%)\n651 (93%)\n\n\n    Unknown\n10\n6\n4\n\n\ncase_when(...)\n\n\n\n\n\n\n\n\n    0\n592 (44%)\n386 (57%)\n206 (31%)\n\n\n    1\n760 (56%)\n294 (43%)\n466 (69%)\n\n\n    Unknown\n53\n24\n29\n\n\nnum_symptoms\n\n\n\n\n\n\n\n\n    1\n600 (43%)\n426 (61%)\n174 (25%)\n\n\n    2\n344 (24%)\n181 (26%)\n163 (23%)\n\n\n    3\n265 (19%)\n67 (9.5%)\n198 (28%)\n\n\n    4\n196 (14%)\n30 (4.3%)\n166 (24%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n\n\nBaik, Yeonsoo, Hannah M Rickman, Colleen F Hanrahan, Lesego Mmolawa, Peter J Kitonsa, Tsundzukana Sewelana, Annet Nalutaaya, et al. 2020. “A Clinical Score for Identifying Active Tuberculosis While Awaiting Microbiological Results: Development and Validation of a Multivariable Prediction Model in Sub-Saharan Africa.” PLoS Medicine 17 (11): e1003420.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Case Study: Pre-Processing Data</span>"
    ]
  },
  {
    "objectID": "book/merging_data.html#tidy-data",
    "href": "book/merging_data.html#tidy-data",
    "title": "7  Merging and Reshaping Data",
    "section": "7.1 Tidy Data",
    "text": "7.1 Tidy Data\nThe tidyverse is designed around interacting with tidy data with the premise that using data in a tidy format can streamline our analysis. Data is considered tidy if:\n\nEach variable is associated with a single column.\nEach observation is associated with a single row.\nEach value has its own cell.\n\nTake a look at the sample data which stores information about the maternal mortality rate for five countries over time (Roser and Ritchie 2013). This data is not tidy because the variable for maternity mortality rate is associated with multiple columns. Every row should correspond to one class observation.\n\nmat_mort1 &lt;- data.frame(country = c(\"Turkey\", \"United States\", \n                                    \"Sweden\", \"Japan\"),\n                       y2002 = c(64, 9.9, 4.17, 7.8),\n                       y2007 = c(21.9, 12.7, 1.86, 3.6),\n                       y2012 = c(15.2, 16, 5.4, 4.8))\nhead(mat_mort1)\n#&gt;         country y2002 y2007 y2012\n#&gt; 1        Turkey 64.00 21.90  15.2\n#&gt; 2 United States  9.90 12.70  16.0\n#&gt; 3        Sweden  4.17  1.86   5.4\n#&gt; 4         Japan  7.80  3.60   4.8\n\nHowever, we can make this data tidy by creating separate columns for country, year, and maternity mortality rate as we demonstrate in the following code. Now every observation is associated with an individual row.\n\nmat_mort2 &lt;- data.frame(\n    country = rep(c(\"Turkey\", \"United States\", \"Sweden\", \"Japan\"), 3),\n    year = c(rep(2002, 4), rep(2007, 4), rep(2012, 4)),\n    mat_mort_rate = c(64.0, 9.9, 4.17, 7.8, 21.9, 12.7, 1.86, 3.6, \n                      15.2, 16, 5.4, 4.8))\nhead(mat_mort2)\n#&gt;         country year mat_mort_rate\n#&gt; 1        Turkey 2002         64.00\n#&gt; 2 United States 2002          9.90\n#&gt; 3        Sweden 2002          4.17\n#&gt; 4         Japan 2002          7.80\n#&gt; 5        Turkey 2007         21.90\n#&gt; 6 United States 2007         12.70",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "book/merging_data.html#reshaping-data",
    "href": "book/merging_data.html#reshaping-data",
    "title": "7  Merging and Reshaping Data",
    "section": "7.2 Reshaping Data",
    "text": "7.2 Reshaping Data\nThe mobility and COVID-19 case data are both already in tidy form: each observation corresponds to a single row, and every column is a single variable. We might consider whether the lockdown dates should be reformatted to be tidy. Another way to represent this data would be to have each observation be the start or end of a stay at home order.\nTo reshape our data, we use the pivot_longer() function to change the data from what is called wide form to what is called long form. This kind of pivot involves taking a subset of columns that we want to gather into a single column while increasing the number of rows in the data set. Before pivoting, we have to think about which columns we are transforming. The image in Figure 7.1 shows a picture of some data on whether students have completed a physical, hearing, or eye exam. The data is presented in wide form on the left and long form on the right. To transform wide data to long data, we have identified a subset of columns cols that we want to transform (these cols are phys, hear, and eye in the left table). The long form contains a new column names_to that contains the exam type and values_to that contains a binary variable indicating whether or not each exam was completed.\n\n\n\n\n\n\nFigure 7.1: Pivoting Longer.\n\n\n\nIn our case, we want to take the lockdown start and end columns and create two new columns: one column will indicate whether or not a date represents the start or end of a lockdown, and the other will contain the date itself. These are called the key and value columns, respectively. The key column gets its values from the names of the columns we are transforming (or the keys) whereas the value column gets its values from the entries in those columns (or the values).\nThe pivot_longer() function takes in a data table, the columns cols that we are pivoting to longer form, the column name names_to that will store the data from the previous column names, and the column name values_to for the column that will store the information from the columns gathered. In our case, we name the first column Lockdown_Event since it will contain whether each date is the start or end of a lockdown, and we name the second column Date. Take a look at the result.\n\nlockdown_long &lt;- lockdowndates %&gt;%\n  pivot_longer(cols = c(\"Lockdown_Start\", \"Lockdown_End\"), \n               names_to = \"Lockdown_Event\", values_to = \"Date\") %&gt;%\n  mutate(Date = as.Date(Date, formula =\"%Y-%M-%D\"), \n         Lockdown_Event = ifelse(Lockdown_Event==\"Lockdown_Start\", \n                                 \"Start\", \"End\")) %&gt;%\n  na.omit()\nhead(lockdown_long)\n#&gt; # A tibble: 6 × 3\n#&gt;   State   Lockdown_Event Date      \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;date&gt;    \n#&gt; 1 Alabama Start          2020-04-04\n#&gt; 2 Alabama End            2020-04-30\n#&gt; 3 Alaska  Start          2020-03-28\n#&gt; 4 Alaska  End            2020-04-24\n#&gt; 5 Arizona Start          2020-03-31\n#&gt; 6 Arizona End            2020-05-16\n\nIn R, we can also transform our data in the opposite direction (from long form to wide form instead of from wide form to long form) using the function pivot_wider(). This function again first takes in a data table but now we specify the arguments names_from and values_from. The former indicates the column that R should get the new column names from, and the latter indicates where the row values should be taken from. For example, in order to pivot our lockdown data back to wide form in the following code, we specify that names_from is the lockdown event and values_from is the date itself. Now we are back to the same form as before!\n\nlockdown_wide &lt;- pivot_wider(lockdown_long, \n                             names_from = Lockdown_Event, \n                             values_from = Date)\nhead(lockdown_wide)\n#&gt; # A tibble: 6 × 3\n#&gt;   State       Start      End       \n#&gt;   &lt;chr&gt;       &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Alabama     2020-04-04 2020-04-30\n#&gt; 2 Alaska      2020-03-28 2020-04-24\n#&gt; 3 Arizona     2020-03-31 2020-05-16\n#&gt; 4 California  2020-03-19 2021-01-25\n#&gt; 5 Colorado    2020-03-26 2020-04-27\n#&gt; 6 Connecticut 2020-03-23 2020-05-20\n\nHere’s another example: suppose that I want to create a data frame where the columns correspond to the number of cases for each state in New England and the rows correspond to the numbered months. First, I need to filter my data to New England and then summarize my data to find the number of cases per month. I use the month() function to be able to group by month and state. Additionally, you can see that I add an ungroup() at the end. When we summarize on data grouped by more than one variable, the summarized output is still grouped. In this case, the warning message states that the data is still grouped by state.\n\nne_cases &lt;- covidcases %&gt;% \n  filter(state %in% c(\"Maine\", \"Vermont\", \"New Hampshire\", \n                      \"Connecticut\", \"Rhode Island\", \n                      \"Massachusetts\")) %&gt;%\n  mutate(month = month(date)) %&gt;%\n  group_by(state, month) %&gt;%\n  summarize(total_cases = sum(weekly_cases)) %&gt;%\n  ungroup()\nhead(ne_cases)\n#&gt; # A tibble: 6 × 3\n#&gt;   state       month total_cases\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;       &lt;int&gt;\n#&gt; 1 Connecticut     3        6872\n#&gt; 2 Connecticut     4       18575\n#&gt; 3 Connecticut     5       11936\n#&gt; 4 Connecticut     6        2619\n#&gt; 5 Connecticut     7        2290\n#&gt; 6 Connecticut     8        3710\n\nNow, I need to convert this data to wide format with a column for each state, so my names_from argument is state. Further, I want each row to have the case values for each state, so my values_from argument is total_cases. The format of this data may not be tidy, but it allows me to quickly compare cases across states.\n\npivot_wider(ne_cases, names_from = state, values_from = total_cases)\n#&gt; # A tibble: 7 × 7\n#&gt;   month Connecticut Maine Massachusetts `New Hampshire` `Rhode Island`\n#&gt;   &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;         &lt;int&gt;           &lt;int&gt;          &lt;int&gt;\n#&gt; 1     3        6872   434         13118             651            827\n#&gt; 2     4       18575   725         47499            1649           6843\n#&gt; 3     5       11936  1575         28798            2179          14653\n#&gt; 4     6        2619  1185          5545             751           1426\n#&gt; 5     7        2290  1046          7621             827          65562\n#&gt; # ℹ 2 more rows\n#&gt; # ℹ 1 more variable: Vermont &lt;int&gt;\n\n\n7.2.1 Practice Question\nCreate a similar data frame as we did in the previous example but this time using the mobility dataset. In other words, create a data frame where the columns correspond to the average mobility for each state in New England and the rows correspond to the numbered months. You should get a result that looks like in Figure 7.2.\n\n\n\n\n\n\nFigure 7.2: Pivoting Mobility Data.\n\n\n\n\n# Insert your solution here:\n\nThe pivots seen so far were relatively simple in that there was only one set of values we were pivoting on (e.g. the lockdown date, covid cases). The tidyr package provides examples of more complex pivots that you might want to apply to your data (Wickham, Vaughan, and Girlich 2023).\n\n\n7.2.2 Pivot Video\nIn this video, we demonstrate a pivot longer when there is information in the column names, a pivot wider with multiple value columns, and a pivot longer with many variables in the columns.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "book/merging_data.html#merging-data-with-joins",
    "href": "book/merging_data.html#merging-data-with-joins",
    "title": "7  Merging and Reshaping Data",
    "section": "7.3 Merging Data with Joins",
    "text": "7.3 Merging Data with Joins\nIn the last section, we saw how to manipulate our current data into new formats. Now, we see how we can combine multiple data sources. Merging two data frames is called joining, and the functions we use to perform this joining depends on how we want to match values between the data frames. For example, we can join information about age and statin use from table1 and table2 matching by name.\n\ntable1 &lt;- data.frame(age = c(14, 26, 32), \n                     name = c(\"Alice\", \"Bob\", \"Alice\"))\ntable2 &lt;- data.frame(name = c(\"Carol\", \"Bob\"), \n                     statins = c(TRUE, FALSE))\nfull_join(table1, table2, by = \"name\")\n#&gt;   age  name statins\n#&gt; 1  14 Alice      NA\n#&gt; 2  26   Bob   FALSE\n#&gt; 3  32 Alice      NA\n#&gt; 4  NA Carol    TRUE\n\nThe following list gives an overview of the different possible joins. For each join type, we specify two tables, table1 and table2, and the by argument, which specifies the columns used to match rows between tables.\nTypes of Joins:\n\nleft_join(table1, table2, by): Joins each row of table1 with all matches in table2.\n\nright_join(table1, table2, by): Joins each row of table2 with all matches in table1 (the opposite of a left join)\n\ninner_join(table1, table2, by): Looks for all matches between rows in table1 and table2. Rows that do not find a match are dropped.\n\nfull_join(table1, table2, by): Keeps all rows from both tables and joins those that match. Rows that do not find a match have NA values filled in.\n\nsemi_join(table1, table2, by): Keeps all rows in table1 that have a match in table2 but does not join to any information from table2.\n\nanti_join(table1, table2, by): Keeps all rows in table1 that do not have a match in table2 but does not join to any information from table2. The opposite of a semi join.\n\n\n7.3.1 Types of Joins Video\n\nWe first demonstrate a left join using the left_join() function. This function takes in two data tables (table1 and table2) and the columns to match rows by. In a left join, for every row of table1, we look for all matching rows in table2 and add any columns not used to do the matching. Thus, every row in table1 corresponds to at least one entry in the resulting table but possibly more if there are multiple matches. In the subsequent code chunk, we use a left join to add the lockdown information to our covidcases data. In this case, the first table is covidcases and we match by state. Since the state column has a slightly different name in the two data frames (“state” in covidcases and “State” in lockdowndates), we specify that state is equivalent to State in the by argument.\n\ncovidcases_full &lt;- left_join(covidcases, lockdowndates, \n                             by = c(\"state\" = \"State\"))\nhead(covidcases_full)\n#&gt; # A tibble: 6 × 8\n#&gt;   state   county   week weekly_cases weekly_deaths date      \n#&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;        &lt;int&gt;         &lt;int&gt; &lt;date&gt;    \n#&gt; 1 Alabama Autauga    12            3             0 2020-03-15\n#&gt; 2 Alabama Autauga    13            3             0 2020-03-22\n#&gt; 3 Alabama Autauga    14            2             1 2020-03-29\n#&gt; 4 Alabama Autauga    15           11             1 2020-04-05\n#&gt; 5 Alabama Autauga    16            5             1 2020-04-12\n#&gt; 6 Alabama Autauga    17            8             2 2020-04-19\n#&gt; # ℹ 2 more variables: Lockdown_Start &lt;date&gt;, Lockdown_End &lt;date&gt;\n\nThese two new columns allow us to determine whether the start of each recorded week was during a lockdown. We use the between() function to create a new column lockdown before dropping the two date columns. We can check that this column worked as expected by choosing a single county to look at.\n\ncovidcases_full &lt;- covidcases_full %&gt;%\n  mutate(lockdown = between(date, Lockdown_Start, Lockdown_End)) %&gt;%\n  select(-c(Lockdown_Start, Lockdown_End)) \ncovidcases_full %&gt;%\n  filter(state == \"Alabama\", county == \"Jefferson\", \n         date &lt;= as.Date(\"2020-05-10\"))\n#&gt; # A tibble: 10 × 7\n#&gt;   state   county     week weekly_cases weekly_deaths date       lockdown\n#&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;        &lt;int&gt;         &lt;int&gt; &lt;date&gt;     &lt;lgl&gt;   \n#&gt; 1 Alabama Jefferson    11           19             0 2020-03-08 FALSE   \n#&gt; 2 Alabama Jefferson    12           66             0 2020-03-15 FALSE   \n#&gt; 3 Alabama Jefferson    13          153             0 2020-03-22 FALSE   \n#&gt; 4 Alabama Jefferson    14          156             8 2020-03-29 FALSE   \n#&gt; 5 Alabama Jefferson    15          128             2 2020-04-05 TRUE    \n#&gt; # ℹ 5 more rows\n\nWe now want to add in the mobility data. In the previous join, we wanted to keep any observation in covidcases regardless if it was in the lockdowndates data frame. Therefore, we used a left join. In this case, we only want to keep observations that have mobility data for that state on each date. This indicates that we want to use an inner join. The function inner_join() takes in two data tables (table1 and table2) and the columns to match rows by. The function only keeps rows in table1 that match to a row in table2. Again, those columns in table2 not used to match with table1 are added to the resulting outcome. In this case, we match by both state and date.\n\ncovidcases_full &lt;- inner_join(covidcases_full, mobility, \n                              by = c(\"state\", \"date\")) %&gt;%\n  select(-c(samples, m50_index))\nhead(covidcases_full)\n#&gt; # A tibble: 6 × 8\n#&gt;   state   county   week weekly_cases weekly_deaths date       lockdown\n#&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;        &lt;int&gt;         &lt;int&gt; &lt;date&gt;     &lt;lgl&gt;   \n#&gt; 1 Alabama Autauga    12            3             0 2020-03-15 FALSE   \n#&gt; 2 Alabama Autauga    13            3             0 2020-03-22 FALSE   \n#&gt; 3 Alabama Autauga    14            2             1 2020-03-29 FALSE   \n#&gt; 4 Alabama Autauga    15           11             1 2020-04-05 TRUE    \n#&gt; 5 Alabama Autauga    16            5             1 2020-04-12 TRUE    \n#&gt; 6 Alabama Autauga    17            8             2 2020-04-19 TRUE    \n#&gt; # ℹ 1 more variable: m50 &lt;dbl&gt;\n\n\n\n7.3.2 Practice Question\nLook at the two data frames, df_A and df_B, defined in the following code. What kind of join would produce the data frame in Figure 7.3? Perform this join yourself.\n\n\n\n\n\n\nFigure 7.3: Joining Data.\n\n\n\n\ndf_A &lt;- data.frame(patient_id = c(12, 9, 12, 8, 14, 8), \n                  visit_num = c(1, 1, 2, 1, 1, 2), \n                  temp = c(97.5, 96, 98, 99, 102, 98.6), \n                  systolic_bp = c(120, 138, 113, 182, 132, 146))\ndf_A\n#&gt;   patient_id visit_num  temp systolic_bp\n#&gt; 1         12         1  97.5         120\n#&gt; 2          9         1  96.0         138\n#&gt; 3         12         2  98.0         113\n#&gt; 4          8         1  99.0         182\n#&gt; 5         14         1 102.0         132\n#&gt; 6          8         2  98.6         146\ndf_B &lt;- data.frame(patient_id = c(12, 12, 12, 8, 8, 8, 14, 14), \n                   visit_num = c(1, 2, 3, 1, 2, 3, 1, 2),\n                   digit_span = c(3, 5, 7, 7, 9, 5, 8, 7))\ndf_B\n#&gt;   patient_id visit_num digit_span\n#&gt; 1         12         1          3\n#&gt; 2         12         2          5\n#&gt; 3         12         3          7\n#&gt; 4          8         1          7\n#&gt; 5          8         2          9\n#&gt; 6          8         3          5\n#&gt; 7         14         1          8\n#&gt; 8         14         2          7\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "book/merging_data.html#exercises",
    "href": "book/merging_data.html#exercises",
    "title": "7  Merging and Reshaping Data",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nTake a look at the provided code - what is wrong with it? Hint: think about what causes the warning message.\n\nvisit_info &lt;- data.frame(\n  name.f = c(\"Phillip\", \"Phillip\", \"Phillip\", \"Jessica\", \n             \"Jessica\"),\n  name.l = c(\"Johnson\", \"Johnson\", \"Richards\", \"Smith\", \n             \"Abrams\"),\n  measure = c(\"height\", \"age\", \"age\", \"age\", \"height\"),\n  measurement = c(45, 186, 50, 37, 156)\n)\n\ncontact_info &lt;- data.frame(\nfirst_name = c(\"Phillip\", \"Phillip\", \"Jessica\", \"Margaret\"),\nlast_name = c(\"Richards\", \"Johnson\", \"Smith\", \"Reynolds\"),\nemail = c(\"pr@aol.com\", \"phillipj@gmail.com\", \n          \"jesssmith@brown.edu\", \"marg@hotmail.com\")\n)\n\nleft_join(visit_info, contact_info, \n          by = c(\"name.f\" = \"first_name\"))\n#&gt; Warning in left_join(visit_info, contact_info, by = c(name.f = \"first_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n#&gt; ℹ Row 1 of `x` matches multiple rows in `y`.\n#&gt; ℹ Row 1 of `y` matches multiple rows in `x`.\n#&gt; ℹ If a many-to-many relationship is expected, set `relationship =\n#&gt;   \"many-to-many\"` to silence this warning.\n#&gt;    name.f   name.l measure measurement last_name               email\n#&gt; 1 Phillip  Johnson  height          45  Richards          pr@aol.com\n#&gt; 2 Phillip  Johnson  height          45   Johnson  phillipj@gmail.com\n#&gt; 3 Phillip  Johnson     age         186  Richards          pr@aol.com\n#&gt; 4 Phillip  Johnson     age         186   Johnson  phillipj@gmail.com\n#&gt; 5 Phillip Richards     age          50  Richards          pr@aol.com\n#&gt; 6 Phillip Richards     age          50   Johnson  phillipj@gmail.com\n#&gt; 7 Jessica    Smith     age          37     Smith jesssmith@brown.edu\n#&gt; 8 Jessica   Abrams  height         156     Smith jesssmith@brown.edu\n\nFirst, use the covidcases data to create a new data frame called sub_cases containing the total number of cases by month for the states of California, Michigan, Connecticut, Rhode Island, Ohio, New York, and Massachusetts. Then, manipulate the mobility data to calculate the average m50 mobility measure for each month. Finally, merge these two data sets using an appropriate joining function.\nConvert the sub_cases data frame from the previous exercise to wide format so that each row displays the cases in each state for a single month. Then, add on the average m50 overall for each month as an additional column using a join function.\n\n\n\n\n\nGuidotti, Emanuele. 2022. “A Worldwide Epidemiological Database for COVID-19 at Fine-Grained Spatial Resolution.” Scientific Data 9 (1): 112. https://doi.org/10.1038/s41597-022-01245-1.\n\n\nGuidotti, Emanuele, and David Ardia. 2020. “COVID-19 Data Hub.” Journal of Open Source Software 5 (51): 2376. https://doi.org/10.21105/joss.02376.\n\n\nRaifman, Julia, Kristen Nocka, David Jones, Jacob Bor, Sarah Lipson, Jonathan Jay, Megan Cole, et al. 2022. “COVID-19 US State Policy Database.” Inter-university Consortium for Political; Social Research. https://doi.org/10.3886/E119446V143.\n\n\nRoser, Max, and Hannah Ritchie. 2013. “Maternal Mortality.” https://ourworldindata.org/maternal-mortality.\n\n\nSpinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2023. lubridate: Make Dealing with Dates a Little Easier. https://CRAN.R-project.org/package=lubridate.\n\n\nWarren, Michael S, and Samuel W Skillman. 2020. “Mobility Changes in Response to COVID-19.” arXiv Preprint arXiv:2003.14228.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Merging and Reshaping Data</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#intro-to-ggplot",
    "href": "book/visualization_ggplot.html#intro-to-ggplot",
    "title": "8  Visualization with ggplot2",
    "section": "8.1 Intro to ggplot",
    "text": "8.1 Intro to ggplot\nWe’ll begin by demonstrating how to create a scatter plot in ggplot2 to introduce the three key elements of a ggplot2 object. Specifically, we create a scatter plot of a patient’s depression vs. anxiety score. To start a graph, we can use the ggplot() function to create a ggplot object as shown in the following code. Note that this brings up a gray box - this is the base that we build up from.\n\nggplot()\n\n\n\n\n\n\n\n\nNext, we can begin adding layers to our ggplot object. One type of layer is a geom, which creates a geometric object. In the next code chunk, we use the geom_point() function to add a scatter plot layer. For this function, we first need to specify which data we want to use, and then we need to tell R how to use that data to create the scatter plot using the aes() function, which creates an aesthetic. For a scatter plot, we need to at least specify the x-axis and y-axis in the aesthetic. Both the data and the aesthetic can either be specified in our initial ggplot() function, which passes this information to all future layers, or in the geom_point() function itself. In the following code, we specify the aesthetic in the geom function but also include two alternative ways to code the same image in the subsequent code chunk. The resulting plot shows a fairly linear relationship between anxiety and depression.\n\nggplot(pain_df) + geom_point(aes(x=PROMIS_ANXIETY, \n                                 y = PROMIS_DEPRESSION))\n\n\n\n\n\n\n\n\n\n# Alternative 1:\nggplot(pain_df, aes(x = PROMIS_ANXIETY, y = PROMIS_DEPRESSION)) + \n  geom_point()\n# Alternative 2:\nggplot() + \n  geom_point(data = pain_df, aes(x = PROMIS_ANXIETY, \n                                 y = PROMIS_DEPRESSION))\n\nIf we want to improve our plot, we may want to add different labels and a title. To do so, we use the labs() function to add a layer in which we can specify all labels. Additionally, I have passed more information to the geometry layer by changing the color, size, and shape of the points. These things are specified outside of the aes() function since they do not come from the data - every point has the same color, size, and shape in this example.\n\nggplot(pain_df)+\n  geom_point(aes(x = PROMIS_ANXIETY, y = PROMIS_DEPRESSION), \n             color = \"blue\", size = 2, shape = 5) + \n  labs(x = \"PROMIS Anxiety Score\", y = \"PROMIS Depression Score\", \n       title = \"Depression vs Anxiety Scores\")\n\n\n\n\n\n\n\n\nLet’s create another example. This time, I create a histogram for initial recorded pain level. To find the corresponding geom for the type of plot we’d like to make, we can use the data visualization cheat sheet from Posit. The first page lists all the geom options available along with what aesthetics we can set for each option. For example, here we are interested in plotting the distribution of one continuous variable, and under the geom_histogram() function we can see that we can specify x (the variable whose distribution we want to plot) as well as binwidth, y, alpha, color, fill, linetype, size, and weight. By default, the y value in a histogram is the count for each bin.\nIn the following code, you can see that we updated the color (color), fill (fill), and opacity (alpha) of our histogram bars and updated the number of bins to be 11 (to account for the possible values 0-10). Additionally, we used the theme_minimal() function to change the background colors used. You can find the available themes on the second page of the cheat sheet. Try changing the theme of the following plot to theme_bw().\n\nggplot(pain_df)+\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE), color = \"violetred\", \n                 fill = \"lightblue\", alpha = 0.5, bins = 11) +\n  labs(x = \"Patient Reported Pain Intensity\", y = \"Count\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n8.1.1 Practice Question\nRecreate Figure 8.1.\n\n\n\n\n\n\nFigure 8.1: Line Plot.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#adjusting-the-axes-and-aesthetics",
    "href": "book/visualization_ggplot.html#adjusting-the-axes-and-aesthetics",
    "title": "8  Visualization with ggplot2",
    "section": "8.2 Adjusting the Axes and Aesthetics",
    "text": "8.2 Adjusting the Axes and Aesthetics\nWe can further control how each aesthetic element is displayed using scale functions. For example, suppose that I want to update the previous plot. In particular, I first want to update the x-axis to display all of the values 0 to 10 instead of 0, 2.5, 5, etc.. To update the x-axis, I need to find the corresponding scale function for x with continuous values. This function is scale_x_continuous(), which allows me to specify limits (limits), breaks (breaks), and labels (labels). The scale functions can be found on the second sheet of the cheat sheet. In this case, I just want to update the breaks to be all integer values from 0 to 10.\n\nggplot(pain_df)+\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE), color = \"violetred\", \n                 fill = \"lightblue\", alpha = 0.5, bins = 11) +\n  labs(x = \"Patient Reported Pain Intensity\", y = \"Count\")+\n  scale_x_continuous(breaks = 0:10)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, let’s take a more complex example. The following plot shows each patient’s reported sleep disturbance vs. physical function and colors each point by their reported pain intensity. Since some points might overlap in values, we added position=\"jitter\" to the geom_point() function to jitter the points, which corresponds to adding some random noise to each point’s position. As presented, this plot is difficult to interpret. For example, the color of pain intensity makes it hard to see how pain changes, and the legend title needs to be simpler.\n\nggplot(pain_df)+\n  geom_point(aes(x = PROMIS_PHYSICAL_FUNCTION, \n                 y = PROMIS_SLEEP_DISTURB_V1_0, \n                 color = PAIN_INTENSITY_AVERAGE), position=\"jitter\")\n\n\n\n\n\n\n\n\nSuppose that we wanted to visualize the pain intensity and sleep disturbance for patients with below-average physical function. Note that both sleep disturbance and physical function are reported as T-Scores, meaning that the raw scores have been converted to a standardized score with mean 50 and standard deviation 10 within the population. We can use the scale functions to update our axes and labels to reflect this information. As before, we need to use the scale_x_continuous() function to update the x-axis for a continuous variable. In this case, we update the limits (to restrict to below-average physical function), breaks, and labels. We similarly update the y-axis.\nLastly, suppose we want to update the color aesthetic. As before, this aesthetic corresponds to a continuous variable. The cheat sheet provides several possible scale functions depending on how we want to specify the color gradient. We choose the scale_color_gradient() function since this allows us to specify the low and high end colors. We can also specify the breaks for the legend values similar to how we specified the breaks for the x and y axes. The argument name also allows us to rename this legend. The palette then converts this to a continuous color gradient. Note that in contrast to the scale_color_gradient() function that we chose to use for this example, the functions scale_color_gradient2() and scale_color_gradientn() allow you to specify more color points in the gradient rather than just the two extreme colors.\nWe can observe that decreased physical function is associated with higher sleep disturbance and that those with worse physical function and worse sleep disturbance tend to have higher reported pain. Note that this time we receive a warning message, which is because our axis limits have cut off some points. To avoid this message, we could use the function coord_cartesian() to specify our limits which clips the values rather than removing points outside the limits.\n\nggplot(pain_df)+\n  geom_point(aes(x = PROMIS_PHYSICAL_FUNCTION, \n                 y = PROMIS_SLEEP_DISTURB_V1_0, \n                 color = PAIN_INTENSITY_AVERAGE), \n             position = \"jitter\", alpha = 0.5) +\n  scale_x_continuous(limits = c(15,50), breaks = c(20, 30, 40, 50), \n                     labels = c(\"-3 SD\", \"-2 SD\", \"-1 SD\", \n                                \"Pop Mean\")) + \n  scale_y_continuous(breaks = c(40, 50, 60, 70, 80), \n                     labels = c(\"-1 SD\", \"Pop Mean\", \"+1 SD\", \"+2 SD\", \n                                \"+3 SD\")) +\n  scale_color_gradient(breaks = seq(0,10,2), low = \"green\", \n                       high = \"red\", \"Reported Pain\") +\n  labs(x = \"PROMIS Physical Function T-Score\", \n       y = \"PROMIS Sleep Disturbance T-Score\") + \n  theme_minimal()\n#&gt; Warning: Removed 121 rows containing missing values or values outside the scale\n#&gt; range (`geom_point()`).\n\n\n\n\n\n\n\n\nWe now demonstrate these scale functions for discrete variables. In the subsequent example, we first create a new race variable that has only three categories since other groups have limited observations. We then create a box plot for pain intensity by race. There are two discrete aesthetics here: color and the y-axis. This plot shows a higher median pain for black patients compared to other races.\n\npain_df$PAT_RACE_CAT &lt;- ifelse(pain_df$PAT_RACE %in% c(\"BLACK\", \n                                                       \"WHITE\"), \n                               pain_df$PAT_RACE, \"OTHER\")\npain_df$PAT_RACE_CAT &lt;- as.factor(pain_df$PAT_RACE_CAT)\n\n\nggplot(pain_df)+\n  geom_boxplot(aes(y = PAT_RACE_CAT, x = PAIN_INTENSITY_AVERAGE, \n                   fill = PAT_RACE_CAT), alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe function scale_y_discrete() is the scale function that corresponds to a discrete y-axis. In this case, we want to update the order and labels of this y-axis. To update the order, we can either refactor the variable using factor() prior to plotting or update the limits argument of the scale function. The function scale_fill_brewer() is a scale function to control the color palette of a discrete variable used for the fill aesthetic. We use this function to specify the color palette (palette) and to specify that we do not want a legend (guide). Since we do not have a legend, we do not update the values and labels in this function.\n\nggplot(pain_df)+\n  geom_boxplot(aes(y = PAT_RACE_CAT, x = PAIN_INTENSITY_AVERAGE, \n                   fill = PAT_RACE_CAT), alpha = 0.5) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_y_discrete(limits = c(\"OTHER\", \"WHITE\", \"BLACK\"), \n                   labels = c(\"Other\", \"White\", \"Black\")) +\n  scale_fill_brewer(palette = \"Dark2\", guide = \"none\") +\n  labs(x = \"Reported Pain Intensity\", y = \"Reported Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe RColorBrewer package (Neuwirth 2022) contains several default palettes to choose from, shown in the following output. You can also create your own palette using the brewer.pal() function from this package. To visualize a palette you can use the available online tool.\n\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\n\n\n\nRColorBrewer Palettes.\n\n\nHere is one more example of how you can use the scale functions - take a look at the next plot example. We used two geom_histogram() calls, or layers, to plot a histogram of pain at baseline and at follow-up. This allows us to visualize that pain at follow-up tends to be lower than at baseline.\nWe also specify the fill to be by “Baseline” and “Follow-up” within the aesthetic, even though this isn’t a column in the data: this is a sort of manual way to color the bars. We use the scale_fill_manual() function to then specify the colors we want to use for these two categories using the values argument. We received three warnings when creating this plot! This is because we have many NA values for follow-up and because we did not specify the bin size for either histogram. C’est la vie.\n\nggplot(pain_df)+\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE, fill = \"Baseline\")) +\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE.FOLLOW_UP, \n                     fill = \"Follow-Up\")) +\n  scale_x_continuous(breaks = c(0:10)) + \n  scale_fill_manual(values = c(\"violetred\", \"pink\"), \n                    name = \"Measurement\") +\n  labs(x = \"Reported Pain Intensity\", y = \"Count\") +\n  theme_minimal()\n#&gt; Warning: Removed 3604 rows containing non-finite outside the scale range\n#&gt; (`stat_bin()`).",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#adding-groups",
    "href": "book/visualization_ggplot.html#adding-groups",
    "title": "8  Visualization with ggplot2",
    "section": "8.3 Adding Groups",
    "text": "8.3 Adding Groups\nIn the previous example, we created two histograms using two calls to the geom_histogram() function. However, there is another way to create multiple layers like this when you want to separate the geom layer based on a variable. For example, suppose we want to visualize the distribution of physical function by whether someone has follow-up information. In the following code, we create the variable HAS_FOLLOW_UP before using it in our aesthetic for geom_density() as both the color and group. In fact, we do not have to add the group argument because as soon as we specify to ggplot that we want to color the density plots by this variable, it creates the grouping. Finally, we update the legend for this grouping using the scale_color_discrete() function, as the discrete variable HAS_FOLLOW_UP determines the color.\n\npain_df$HAS_FOLLOW_UP &lt;- \n  !is.na(pain_df$PAIN_INTENSITY_AVERAGE.FOLLOW_UP)\nggplot(pain_df) +\n  geom_density(aes(x = PROMIS_PHYSICAL_FUNCTION, \n                   group = HAS_FOLLOW_UP, \n                   color= HAS_FOLLOW_UP)) +\n  scale_x_continuous(breaks = c(0:10)) + \n  scale_color_discrete(name = \"Follow-Up\", labels = c(\"No\", \"Yes\")) +\n  labs(x = \"PROMIS Physical Function T-Score\", \n       y = \"Estimated Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s try another example. Suppose that we want to find the distribution of initial overall pain by those that do and do not have follow up. In this case, we want to plot the proportion of each pain score for each group rather than comparing counts. We first need to find these proportions, which we do by grouping and summarizing over our data.\n\npain_df_grp &lt;- pain_df %&gt;%\n  group_by(HAS_FOLLOW_UP, PAIN_INTENSITY_AVERAGE) %&gt;%\n  summarize(tot = n()) %&gt;%\n  mutate(prop = tot/sum(tot)) %&gt;%\n  ungroup()\nhead(pain_df_grp)\n#&gt; # A tibble: 6 × 4\n#&gt;   HAS_FOLLOW_UP PAIN_INTENSITY_AVERAGE   tot    prop\n#&gt;   &lt;lgl&gt;                          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 FALSE                              0     8 0.00222\n#&gt; 2 FALSE                              1    16 0.00444\n#&gt; 3 FALSE                              2    62 0.0172 \n#&gt; 4 FALSE                              3   132 0.0366 \n#&gt; 5 FALSE                              4   273 0.0757 \n#&gt; 6 FALSE                              5   508 0.141\n\nWe can now use the geom_col() function to create a bar plot of these proportions. By default, this function stacks the bars on top of each other when there is grouping. Try adding position=\"dodge\" to the geom_col() function to place the bars side by side instead of on top of each other.\n\nggplot(pain_df_grp)+\n  geom_col(aes(x = PAIN_INTENSITY_AVERAGE, y = prop, \n               fill = HAS_FOLLOW_UP)) +\n  scale_x_continuous(breaks = c(0:10)) + \n  scale_fill_discrete(name = \"Seen at Follow Up\", \n                      labels = c(\"No\", \"Yes\")) +\n  labs(x = \"Reported Pain Intensity\", y = \"Proportion\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n8.3.1 Practice Question\nRecreate Figure 8.2.\n\n\n\n\n\n\nFigure 8.2: BMI Distribution.\n\n\n\n\n# Insert your solution here:\n\nAnother way to visualize data by group is to add a facet wrap to your ggplot object. Facets divide a plot into subplots based on one or more discrete variable values. We can either arrange these plots as a grid where the rows and/or columns correspond to the variables we are grouping by using facet_grid() and specifying the column and row variables using the col and row arguments respectively. Or we can wrap the plots into a rectangular format using facet_wrap() and specifying the columns using the facet argument. In the following code, we take one of our previous plots and add a facet grid where the columns of the grid are given by racial group. If we had set row=vars(PAT_RACE_CAT), then this would stack the plots vertically. Note that we have to specify the variables inside the vars() function.\n\nggplot(pain_df)+\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE, fill = \"Baseline\")) +\n  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE.FOLLOW_UP, \n                     fill = \"Follow-Up\")) +\n  scale_x_continuous(breaks = c(0:10)) + \n  scale_fill_manual(values = c(\"violetred\", \"pink\"), \n                    name = \"Measurement\") +\n  labs(x= \"Reported Pain Intensity\", y = \"Count\") +\n  facet_grid(row = vars(PAT_RACE_CAT))+\n  theme_minimal()\n#&gt; Warning: Removed 3604 rows containing non-finite outside the scale range\n#&gt; (`stat_bin()`).",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#extra-options",
    "href": "book/visualization_ggplot.html#extra-options",
    "title": "8  Visualization with ggplot2",
    "section": "8.4 Extra Options",
    "text": "8.4 Extra Options\nFor our final plot, we will demonstrate additional features not yet covered in this chapter. To create this plot, we first find the number of participants who selected each body region as well as the average pain intensity for those patients. We also classify each body part region into larger groups.\n\npain_body_map &lt;- data.frame(part = names(pain_df)[2:75])\npain_body_map$num_patients &lt;- colSums(pain_df[, 2:75])\npain_body_map$perc_patients &lt;- pain_body_map$num_patients / \n                               nrow(pain_df)\npain_body_map$avg_pain &lt;- colSums(pain_df[, 2:75] * \n                                pain_df$PAIN_INTENSITY_AVERAGE) /\n                                pain_body_map$num_patients\npain_body_map &lt;- pain_body_map %&gt;% \n    mutate(region = case_when(\n    part %in% c(\"X208\", \"X209\", \"X218\", \"X219\", \"X212\",\n                \"X213\") ~ \"Back\",\n    part %in% c(\"X105\", \"X106\", \"X205\", \"X206\") ~ \"Neck\",\n    part %in% c(\"X107\", \"X110\", \"X207\", \"X210\") ~ \"Shoulders\",\n    part %in% c(\"X108\", \"X109\", \"X112\", \"X113\") ~ \"Chest/Abs\",\n    part %in% c(\"X126\", \"X127\", \"X228\", \"X229\",\n                \"X131\", \"X132\", \"X233\", \"X234\") ~ \"Legs\",\n    part %in% c(\"X111\", \"X114\", \"X211\", \"X214\", \"X115\", \"X116\",\n                \"X117\", \"X118\", \"X217\", \"X220\") ~ \"Arms\",\n    part %in% c(\"X119\", \"X124\", \"X221\", \"X226\", \"X125\", \"X128\",\n                \"X227\", \"X230\") ~ \"Wrists/Hands\",\n    part %in% c(\"X215\", \"X216\") ~ \"Elbows\",\n    part %in% c(\"X135\", \"X136\", \"X237\", \"X238\", \"X133\", \"X134\",\n                \"X235\", \"X236\") ~ \"Feet/Ankles\",\n    part %in% c(\"X129\", \"X130\", \"X231\", \"X232\") ~ \"Knees\",\n    part %in% c(\"X101\", \"X102\", \"X103\", \"X104\", \"X201\", \"X203\",\n                \"X202\", \"X204\") ~ \"Head\",\n    part %in% c(\"X120\", \"X121\", \"X122\", \"X123\", \"X222\", \"X223\",\n                \"X224\", \"X225\") ~ \"Hips\"))\n    \nhead(pain_body_map)\n#&gt;   part num_patients perc_patients avg_pain region\n#&gt; 1 X101          323        0.0646     6.69   Head\n#&gt; 2 X102          322        0.0644     6.82   Head\n#&gt; 3 X103          165        0.0330     6.86   Head\n#&gt; 4 X104          165        0.0330     6.95   Head\n#&gt; 5 X105          493        0.0986     6.90   Neck\n#&gt; 6 X106          507        0.1014     6.92   Neck\n\nWithin the theme we’ve chosen, we are able to update any of the theme options (see ?theme). In the following code, we use the theme() function to update the legend position to the bottom and the grid lines to light pink. Additionally, we add a horizontal line using the geom_hline() function (geom_vline() and geom_abline() can add vertical or diagonal lines respectively) and add a text annotation using the annotate() function. The resulting plot shows the average pain value for each body part as well as the proportion of patients who categorized it as being painful.\n\nggplot(pain_body_map) +\n  geom_label(aes(x = perc_patients, y = avg_pain, label = part, \n                 color = region)) + \n  geom_hline(yintercept = mean(pain_body_map$avg_pain)) +\n  annotate(geom = \"text\", label = \"Average Pain Value\", \n           x = 0.35, y = 7.0) + \n  labs(x = \"Proportion Patients Selected Region\", \n       y = \"Average Pain of Patients\") +\n  theme_minimal()+\n  theme(legend.position=\"bottom\", \n        panel.grid.major = element_line(colour = \"lightpink\"))\n\n\n\n\n\n\n\n\nSo far we have not saved any of our figures as objects. In the next example, I create two plots and save them as objects named p1 and p2. If we want to save these plots, we can use the ggsave() function, which saves the last plot generated under the file name provided. Additionally, I can use the patchwork package to incorporate multiple plots together. A + between plots adds them together into a single figure and then the plot_layout() function allows us to specify the grid used to arrange our figures. We have added an extra element using the guide_area() function to add a placeholder to put the legends and then used the guide = \"collect\" argument in the plot_layout() function to specify that all guides should be put together.\n\np1 &lt;- ggplot(pain_body_map) +\n  geom_label(aes(x = perc_patients, y = avg_pain, label = part, \n                 color = region)) + \n  geom_hline(yintercept = mean(pain_body_map$avg_pain)) +\n  annotate(geom = \"text\", label = \"Average Pain Value\", \n           x = 0.35, y = 7.0) + \n  labs(x = \"Proportion Patients Selected Region\", \n       y = \"Average Pain of Patients\") +\n  theme_minimal()+\n  theme(legend.position = \"bottom\", \n        panel.grid.major = element_line(colour = \"lightpink\"))\n\np2 &lt;- ggplot(pain_body_map) +\n  geom_histogram(aes(x = perc_patients), color = \"violetred\", \n                 fill = \"lightpink\") + \n  labs(x = \"Proportion of Patients Selected Region\", y = \"Count\") +\n  theme_minimal()+\n  theme(panel.grid.major = element_line(colour = \"lightpink\"))\n\n\np1 + p2 + guide_area() + plot_layout(ncol=1, guides = \"collect\")\n\n\n\n\n\n\n\n\n\nggsave(\"images/visualization_ggplot/myplot.png\")",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#recap-video",
    "href": "book/visualization_ggplot.html#recap-video",
    "title": "8  Visualization with ggplot2",
    "section": "8.5 Recap Video",
    "text": "8.5 Recap Video",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/visualization_ggplot.html#exercises",
    "href": "book/visualization_ggplot.html#exercises",
    "title": "8  Visualization with ggplot2",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\nFor this chapter’s exercises, use the covidcases data set that we first introduced in Chapter 5 to recreate some plots. These are complex plots, so try to build them up one step at a time and just try to get as close as possible to the given examples.\n\nReplicate the plot in Figure 8.3, which shows the weekly Covid-19 cases by state in 2020 - the black vertical line signifies the week of May 28th, 2020, which is when US cases passed the 100,000 mark (AJMC Staff 2021), and NA values are displayed as white squares). Hint: set negative weekly case counts to be NA and color the squares with a log 10 transformation using trans argument for the scale_color_gradientn() function.\n\n\n\n\n\n\n\nFigure 8.3: Covid-19 Cases Over Time by State.\n\n\n\n\nReplicate the plot in Figure 8.4, which is a stacked area chart for the total deaths from Covid-19 in the states with the top ten total death counts overall.\n\n\n\n\n\n\n\nFigure 8.4: Covid-19 Cases Over Time by State.\n\n\n\n\n\n\n\nAJMC Staff. 2021. “A Timeline of COVID-19 Developments in 2020.” https://www.ajmc.com/view/a-timeline-of-covid19-developments-in-2020.\n\n\nAlter, Benedict J, Nathan P Anderson, Andrea G Gillman, Qing Yin, Jong-Hyeon Jeong, and Ajay D Wasan. 2021. “Hierarchical Clustering by Patient-Reported Pain Distribution Alone Identifies Distinct Chronic Pain Subgroups Differing by Pain Intensity, Quality, and Clinical Outcomes.” PLoS One 16 (8): e0254862.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "book/cs_eda.html#preprocessing",
    "href": "book/cs_eda.html#preprocessing",
    "title": "9  Case Study: Exploratory Data Analysis",
    "section": "9.1 Preprocessing",
    "text": "9.1 Preprocessing\nWe start by cleaning and merging our data. The covidcases data contains weekly confirmed COVID-19 cases and deaths at the state and county level in 2020. As the data description notes, some of these values may be negative due to data discrepancies in the cumulative counts data. The mobility data contains daily mobility statistics by state.\n\n# Read in data\ndata(covidcases)\ndata(mobility)\n\nFirst, we look at the columns in our data. We convert the date columns in the mobility data to be recognized as a date using the as.Date() function. The covidcases data has the week number of 2020. We create a similar column for the mobility data.\n\n# Convert to date format and find week\nmobility$date &lt;- as.Date(mobility$date, formula = \"%Y-%M-%D\")\nmobility$week &lt;- week(mobility$date)\n\nThis allows us to summarize the mobility for a state across each week.\n\n# Find average mobility for week\nmobility_week &lt;- mobility %&gt;%\n  group_by(state, week) %&gt;%\n  summarize(m50 = mean(m50, na.rm=TRUE), .groups = \"drop\") \nhead(mobility_week)\n#&gt; # A tibble: 6 × 3\n#&gt;   state    week   m50\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Alabama     9 13.2 \n#&gt; 2 Alabama    10 14.6 \n#&gt; 3 Alabama    11 13.4 \n#&gt; 4 Alabama    12  8.98\n#&gt; 5 Alabama    13  7.81\n#&gt; 6 Alabama    14  6.73\n\nFor both of our datasets, we want to check whether each state was observed across all dates and how the state’s name is represented. For the mobility data, our data is at the state level so we can use the table() function.\n\n# Find number of dates recorded for each state\ntable(mobility_week$state)\n#&gt; \n#&gt;          Alabama           Alaska          Arizona         Arkansas \n#&gt;               27               27               27               27 \n#&gt;       California         Colorado      Connecticut         Delaware \n#&gt;               27               27               27               27 \n#&gt;          Florida          Georgia           Hawaii            Idaho \n#&gt;               27               27               27               27 \n#&gt;         Illinois          Indiana             Iowa           Kansas \n#&gt;               27               27               27               27 \n#&gt;         Kentucky        Louisiana            Maine         Maryland \n#&gt;               27               27               27               27 \n#&gt;    Massachusetts         Michigan        Minnesota      Mississippi \n#&gt;               27               27               27               27 \n#&gt;         Missouri          Montana         Nebraska           Nevada \n#&gt;               27               27               27               27 \n#&gt;    New Hampshire       New Jersey       New Mexico         New York \n#&gt;               27               27               27               27 \n#&gt;   North Carolina     North Dakota             Ohio         Oklahoma \n#&gt;               27               27               27               27 \n#&gt;           Oregon     Pennsylvania     Rhode Island   South Carolina \n#&gt;               27               27               27               27 \n#&gt;     South Dakota        Tennessee            Texas             Utah \n#&gt;               27               27               27               27 \n#&gt;          Vermont         Virginia       Washington Washington, D.C. \n#&gt;               27               27               27               27 \n#&gt;    West Virginia        Wisconsin          Wyoming \n#&gt;               27               27               27\n\nFor the covidcases data, our data is at the county level. We need to summarize the data instead. In this case, some states were observed for fewer weeks than others.\n\n# Find state names and number of weeks recorded for each state\nunique(covidcases$state)\n#&gt;  [1] \"Alabama\"              \"Alaska\"              \n#&gt;  [3] \"Arizona\"              \"Arkansas\"            \n#&gt;  [5] \"California\"           \"Colorado\"            \n#&gt;  [7] \"Connecticut\"          \"Delaware\"            \n#&gt;  [9] \"District of Columbia\" \"Florida\"             \n#&gt; [11] \"Georgia\"              \"Hawaii\"              \n#&gt; [13] \"Idaho\"                \"Illinois\"            \n#&gt; [15] \"Indiana\"              \"Iowa\"                \n#&gt; [17] \"Kansas\"               \"Kentucky\"            \n#&gt; [19] \"Louisiana\"            \"Maine\"               \n#&gt; [21] \"Maryland\"             \"Massachusetts\"       \n#&gt; [23] \"Michigan\"             \"Minnesota\"           \n#&gt; [25] \"Mississippi\"          \"Missouri\"            \n#&gt; [27] \"Montana\"              \"Nebraska\"            \n#&gt; [29] \"Nevada\"               \"New Hampshire\"       \n#&gt; [31] \"New Jersey\"           \"New Mexico\"          \n#&gt; [33] \"New York\"             \"North Carolina\"      \n#&gt; [35] \"North Dakota\"         \"Ohio\"                \n#&gt; [37] \"Oklahoma\"             \"Oregon\"              \n#&gt; [39] \"Pennsylvania\"         \"Rhode Island\"        \n#&gt; [41] \"South Carolina\"       \"South Dakota\"        \n#&gt; [43] \"Tennessee\"            \"Texas\"               \n#&gt; [45] \"Utah\"                 \"Vermont\"             \n#&gt; [47] \"Virginia\"             \"Washington\"          \n#&gt; [49] \"West Virginia\"        \"Wisconsin\"           \n#&gt; [51] \"Wyoming\"\nnum_wks &lt;- covidcases %&gt;%\n  group_by(state) %&gt;%\n  summarize(num_weeks = n_distinct(week), .groups = \"drop\")\nsummary(num_wks)\n#&gt;     state             num_weeks   \n#&gt;  Length:51          Min.   :23.0  \n#&gt;  Class :character   1st Qu.:25.5  \n#&gt;  Mode  :character   Median :26.0  \n#&gt;                     Mean   :26.0  \n#&gt;                     3rd Qu.:27.0  \n#&gt;                     Max.   :27.0\n\nNote that D.C. is written differently for each data source. We update this name in the mobility data.\n\nmobility_week$state[mobility_week$state == \"Washington, D.C.\"] &lt;- \n  \"District of Columbia\"\n\nAfter checking the formatting of the state and week columns, we can now merge our data together. In this case, we want to add the mobility data to the case data and use a left_join().\n\n# Join cases and mobility data\ncovid &lt;- left_join(covidcases, mobility_week, by = c(\"state\", \"week\"))\n\nNext, we want to get some simple information about the continuous variables in our data. We observe two key points. First, we can see the negative values the data description warned us about, and second, there is no missing data.\n\nsummary(covid[, c(\"weekly_cases\", \"weekly_deaths\", \"m50\")])\n#&gt;   weekly_cases   weekly_deaths       m50      \n#&gt;  Min.   : -190   Min.   :-511   Min.   : 0.0  \n#&gt;  1st Qu.:    3   1st Qu.:   0   1st Qu.: 5.0  \n#&gt;  Median :   10   Median :   1   Median : 7.7  \n#&gt;  Mean   :   80   Mean   :   5   Mean   : 7.7  \n#&gt;  3rd Qu.:   36   3rd Qu.:   3   3rd Qu.: 9.9  \n#&gt;  Max.   :30584   Max.   :4416   Max.   :49.4\n\nThese negative numbers are clear data discrepancies. When showing the distribution of cases in our exploratory analysis we may choose to either code these as 0 or NA. We decide to recode these negative values as NA.\n\n# Set negative counts to NA\ncovid$weekly_cases &lt;- replace(covid$weekly_cases, \n                                   which(covid$weekly_cases &lt; 0),\n                                   NA)\n\ncovid$weekly_deaths &lt;- replace(covid$weekly_deaths, \n                                   which(covid$weekly_deaths &lt; 0),\n                                   NA)\n\nAs the last step in our pre-processing, we add in the state abbreviation and region for each state using the state.name and state.region vectors available in R. We code D.C. to be in the same region as Maryland and Virginia.\n\n# Add region and abbreviation and remove county\nregion_key &lt;- data.frame(state = c(state.name, \n                                   \"District of Columbia\"), \n                         state_abb = c(state.abb, \"DC\"),\n                         region = c(as.character(state.region), \n                                    \"South\"))\n\ncovid &lt;- covid %&gt;%\n  left_join(region_key, by = c(\"state\")) \n\nhead(covid)\n#&gt; # A tibble: 6 × 8\n#&gt;   state   county  week weekly_cases weekly_deaths   m50 state_abb region\n#&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;        &lt;int&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; \n#&gt; 1 Alabama Autau…    12            3             0  8.98 AL        South \n#&gt; 2 Alabama Autau…    13            3             0  7.81 AL        South \n#&gt; 3 Alabama Autau…    14            2             1  6.73 AL        South \n#&gt; 4 Alabama Autau…    15           11             1  6.50 AL        South \n#&gt; 5 Alabama Autau…    16            5             1  7.53 AL        South \n#&gt; 6 Alabama Autau…    17            8             2  8.29 AL        South",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Study: Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/cs_eda.html#mobility-and-cases-over-time",
    "href": "book/cs_eda.html#mobility-and-cases-over-time",
    "title": "9  Case Study: Exploratory Data Analysis",
    "section": "9.2 Mobility and Cases Over Time",
    "text": "9.2 Mobility and Cases Over Time\nNow that our data are merged and cleaned, we start exploring mobility and cases by region. The following summary table shows that these measures did differ by region overall.\n\ncovid %&gt;%\n  select(c(\"region\", \"m50\", \"weekly_cases\", \"weekly_deaths\")) %&gt;%\n  tbl_summary(by = \"region\", missing = \"no\") %&gt;%\n  as_gt()\n\n\n\n\n\n\n\nCharacteristic\nNorth Central, N = 22,6531\nNortheast, N = 5,1651\nSouth, N = 32,2761\nWest, N = 9,4361\n\n\n\n\nm50\n7.4 (5.4, 8.8)\n4.2 (1.6, 5.2)\n9.7 (7.1, 11.3)\n4.3 (2.9, 7.0)\n\n\nweekly_cases\n6 (2, 22)\n17 (4, 91)\n13 (4, 44)\n8 (2, 38)\n\n\nweekly_deaths\n1 (0, 2)\n2 (1, 10)\n1 (0, 3)\n0 (0, 2)\n\n\n\n1 Median (IQR)\n\n\n\n\n\n\n\n\nWe then plot mobility over time both for the whole country and by region. Across the country we see a similar pattern in how mobility fluctuated but that certain regions had overall higher mobility than others.\n\n# Average mobility in the US over time - overall \npmob1 &lt;- covid %&gt;%\n  select(c(region, state, week, m50)) %&gt;%\n  distinct() %&gt;%\n  group_by(week) %&gt;%\n  summarize(avg_m50 = mean(m50, na.rm=TRUE), .groups=\"drop\") %&gt;%\n  ggplot() + \n  geom_line(aes(x = week, y = avg_m50)) +\n  labs(x = \"Week in 2020\", y = \"Average Mobility\", \n       title = \"Average Mobility in the US\") + \n  theme_bw()\n\n# Average mobility in the US over time - by region\npmob2 &lt;- covid %&gt;%\n  select(c(region, state, week, m50)) %&gt;%\n  distinct() %&gt;%\n  group_by(region, week) %&gt;%\n  summarize(avg_m50 = mean(m50, na.rm=TRUE), .groups=\"drop\") %&gt;%\n  ggplot() + \n  geom_line(aes(x = week, y = avg_m50, color = region)) +\n  labs(x = \"Week in 2020\", y = \"Average Mobility\", \n       title = \"Average Mobility by Region in the US\",\n       color = \"Region\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\") \n\npmob1+pmob2\n\n\n\n\n\n\n\n\nWe then look at cases and deaths by region. A limitation of these data are that we do not have population counts which would allow us to standardize these numbers. However, we do use a secondary y-axis using the sec_axis() function within scale_y_continuous() to allow us to plot deaths and cases together. In this case, the secondary axis is scaled by 1/10th of the primary axis.\n\n# Change in number cases over time, per region\ncovid %&gt;%\n  filter(!is.na(region)) %&gt;% \n  group_by(region, week) %&gt;%\n  summarize(weekly_cases = sum(weekly_cases, na.rm = TRUE),\n            weekly_deaths = sum(weekly_deaths, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\nggplot() +\n  geom_line(aes(x = week, y = weekly_cases, color = region, \n                linetype = \"Cases\")) + \n  geom_line(aes(x = week, y = weekly_deaths*10, color = region, \n                linetype = \"Deaths\")) +\n  scale_y_continuous(name = \"Average Weekly Cases\",\n                     sec.axis = sec_axis(~./10, \n                          name = \"Average Weekly Deaths\"))+\n  scale_linetype(name = \"Measure\") +\n  labs(x = \"Week in 2020\", color = \"Region\", \n       title = \"Weekly Cases Over Time by Region\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nTo look at how mobility and cases are related, we look at a scatter plot of mobility and cases in California.\n\ncovid_ca &lt;- covid %&gt;% filter(state == \"California\")\nggplot(covid_ca)+\n  geom_point(aes(x = weekly_cases, y = m50), na.rm = TRUE) +\n  labs(x = \"Weekly Cases\", y = \"Average Mobility\")\n\n\n\n\n\n\n\n\nThis motivates us to look at the correlation between these two columns by state. We plot this using the plot_usmap() function from the usmap package. Interestingly, we observe different relationships throughout the country, but none of the correlations are particularly strong.\n\n# Calculate and plot correlation between cases and mobility, y state\ncovid_cor &lt;- covid %&gt;%\n  group_by(state) %&gt;%\n  summarize(correlation = cor(weekly_cases, m50,\n                              use = \"complete.obs\"))\n\nplot_usmap(regions = \"states\", data = covid_cor, \n           values = \"correlation\") +\n  scale_fill_gradient2(low = \"darkblue\", high = \"darkred\", \n                       mid=\"white\", name = \"Correlation\") + \n  labs(title = \"Correlation Between Cases and Mobility\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nLast, we look at how the total cases and deaths are related to each other. This shows that the Northeast suffered more deaths per case overall, which may be related to the lower mobility and negative correlation between mobility and cases observed earlier.\n\n# Relationship between cases and deaths summarized\ncovid %&gt;%\n  group_by(region, state_abb) %&gt;%\n  summarize(total_cases = sum(weekly_cases, na.rm = TRUE),\n            total_deaths = sum(weekly_deaths, na.rm = TRUE),\n            .groups = \"drop\") %&gt;%\nggplot() + \n  geom_label(aes(x = total_cases, y = total_deaths, color = region,\n                 label = state_abb), size = 1.5) + \n  labs(x = \"Total Cases\", y = \"Total Deaths\", color = \"Region\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nDi Lorenzo, Paolo. 2024. Usmap: US Maps Including Alaska and Hawaii. https://CRAN.R-project.org/package=usmap.",
    "crumbs": [
      "Exploratory Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Study: Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "book/distributions.html#probability-distributions-in-r",
    "href": "book/distributions.html#probability-distributions-in-r",
    "title": "10  Probability Distributions in R",
    "section": "10.1 Probability Distributions in R",
    "text": "10.1 Probability Distributions in R\nAll of the common discrete (e.g. Bernoulli, binomial) and continuous (e.g. normal, uniform, exponential, Poisson) probability distributions have corresponding functions in R. For each of these distributions, there are four available functions:\n\nr[dist](): generates random samples from the given distribution (e.g. rnorm(), runif())\n\nd[dist](): density function for the distribution (e.g. dnorm(), dunif())\n\np[dist](): cumulative distribution function for the distribution (e.g. pnorm(), punif())\n\nq[dist](): quantile function for the distribution (e.g. qnorm(), qunif())\n\nLet’s see how these work in practice, using the normal and binomial distributions as examples.\n\n10.1.1 Random Samples\nThe following code generates a sample of 100 random numbers following a normal distribution with mean 5 and standard deviation 1. As you can see, the function takes in n (the number of observations), mean (the mean with default value 0), and sd (the standard deviation with default value 1). A histogram plot (using the built-in hist() function) shows that the generated values look roughly normally distributed.\n\nx &lt;- rnorm(n = 100, mean = 5, sd = 1)\nhist(x)\n\n\n\n\n\n\n\n\nWe can also input a vector instead of a single value for the mean or sd arguments if we want each sample to come from its own normal distribution. As an example, we generate 100 random numbers with the default standard deviation of 1 where half of the samples have a mean of 0 and the other half have a mean of 5.\n\nx &lt;- rnorm(n = 100, mean = rep(c(0,5), 50))\nhist(x)\n\n\n\n\n\n\n\n\nFor the binomial distribution, the difference is that we need to specify a probability p and number of trials size (rather than mean and sd in the normal case) to specify the distribution. In the following code, we generate 100 random numbers following a binomial distribution with 10 trials and a probability 0.5.\n\nx &lt;- rbinom(n = 100, p = 0.5, size = 10)\nhist(x)\n\n\n\n\n\n\n\n\nWe can also specify a different size or probability of success for each sample. We repeat our sample but this time let the probability of success be 0.25 for half of the sample and 0.75 for the other half.\n\nx &lt;- rbinom(n = 100, p = rep(c(0.25, 0.75), 50), size = 10)\nhist(x)\n\n\n\n\n\n\n\n\n\n\n10.1.2 Density Function\nNext, we look at the density function. Recall that the probability density function for a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by the following formula.\n\\[ f_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2} \\left (\\frac{x-\\mu}{\\sigma} \\right)^2 \\right) \\]\nUsing the following code, we can compare some of the values from the dnorm() function to this equation and see that they are in fact equal. We could also specify the mean and standard deviation in this function but choose to use the default values (mean = 0 and sd = 1).\n\ndnorm(0) == 1/sqrt(2*pi)\n#&gt; [1] TRUE\ndnorm(1) == exp(-1/2)/sqrt(2*pi)\n#&gt; [1] TRUE\ndnorm(2) == exp(-1/2*2^2)/sqrt(2*pi)\n#&gt; [1] TRUE\n\nIf we wanted to find the density function for several values, we can input a vector to this density function. In the following code, we find the values of the density function for a normal distribution with mean 1 and standard deviation 2 for values c(-1, 0, 1, 2, 3).\n\ndnorm(c(-1, 0, 1, 2, 3), mean = 1, sd = 2)\n#&gt; [1] 0.121 0.176 0.199 0.176 0.121\n\nFor the binomial distribution, dbinom() returns the probability of a certain number of successes and corresponds to the probability density function.\n\\[ P(X = x) = \\binom{size}{x} p^x (1-p)^{size-x}. \\]\nFor example, we can find the probability of getting exactly 3 heads from 10 coin flips, each with a probability of 0.5 for heads.\n\ndbinom(3, size = 10, p = 0.5)\n#&gt; [1] 0.117\n\nWhile dnorm() allows us to specify any continuous values for \\(x\\), dbinom() gives us a warning if x contains non-integer values since the support of a binomial variable only includes integers.\n\ndbinom(2.4, size = 10, p = 0.5)\n#&gt; Warning in dbinom(2.4, size = 10, p = 0.5): non-integer x = 2.400000\n#&gt; [1] 0\n\nWe can also specify a vector for a distribution’s parameters to find the distribution function for different distributions. For example, I find the the probability density function for \\(X=4\\) for the distribution with \\(p=0.25\\) and \\(p=0.5\\).\n\ndbinom(4, size = 10, p = c(0.25, 0.5))\n#&gt; [1] 0.146 0.205\n\n\n\n10.1.3 Cumulative Distribution\nNext, we take a look at the cumulative distribution function. For the normal distribution, the cumulative distribution is given by pnorm(), which takes in a value x, a mean, and a sd and returns the probability that a random variable following a \\(N(mean, sd)\\) distribution is less than x. For example, for x equal to the mean, this returns a 50% probability because the normal distribution is symmetric with mean equal to the median. In the following code, we verify this for two different values of the mean.\n\npnorm(0)\n#&gt; [1] 0.5\npnorm(5, mean = 5, sd = 1)\n#&gt; [1] 0.5\n\nSince the binomial distribution is discrete, it can only take on integer values from 0 to size. This means that, for example, the pbinom() function returns the same value for 3, 3.5, 3.6, all the way up to, but not including, 4 - this is because \\(P(X \\leq 3) = P(X \\leq 3.2) = P(X \\leq 3.5) = P(X \\leq 3.6)\\) and so on. Note that here we passed in a vector of values x.\n\npbinom(c(3, 3.5, 3.6, 4), size = 10, p = 0.5)\n#&gt; [1] 0.172 0.172 0.172 0.377\n\nWe can also vary the parameters for the distribution by passing a vector for size and/or p to the cumulative distribution function. In the subsequent code chunk, we find the probability that \\(X \\leq 3\\) and the probability that \\(X \\leq 4\\) with 12 trials and a probability 0.25 and with 10 trials and a probability 0.5.\n\npbinom(c(3, 3, 4, 4), size = c(12, 10, 12, 10), \n       p=c(0.25, 0.5, 0.25, 0.5))\n#&gt; [1] 0.649 0.172 0.842 0.377\n\n\n\n10.1.4 Quantile Distribution\nLastly, we have the quantile distribution function, which is the inverse of the cumulative distribution function. This function takes in a probability x, a mean, and a sd and returns the value for which the cumulative distribution function is equal to x. Thus, when x is equal to 0.5, the qnorm() function returns the median of the distribution, which is equal to the mean for the normal distribution.\n\nqnorm(0.5)\n#&gt; [1] 0\nqnorm(0.5, mean = 5, sd = 1)\n#&gt; [1] 5\n\nFor the discrete binomial distribution, the qbinom() function returns the largest integer value for which the probability of being less than or equal to that value is at most the inputted value x.\n\nqbinom(c(0.2, 0.3), size = 10, p = 0.5)\n#&gt; [1] 2 3\n\n\n\n10.1.5 Reference List for Probability Distributions\nIn the previous examples, we only used the normal and binomial distributions. The following list contains the other probability distributions available in R. For each distribution, we have given the arguments for the r[dist]() function. The other three functions have a similar format. Unless otherwise stated, the parameter n is the number of observations.\n\nBeta: rbeta(n, shape1, shape2, ncp = 0) with shape parameters shape1 and shape2 (and optional non-centrality parameter ncp).\nBinomial: rbinom(n, size, prob) with probability of success prob and number of trials size\nCauchy: rcauchy(n, location = 0, scale = 1) with location parameter location and scale parameter scale.\nChi-Square: rchisq(n, df, ncp = 0) with df degrees of freedom and optional non-centrality parameter ncp.\nExponential: rexp(n, rate = 1) with rate rate (i.e., mean = 1/rate).\nF: rf(n, df1, df2, ncp) with df1 and df2 degrees of freedom (and optional non-centrality parameter ncp).\nGamma: rgamma(n, shape, rate = 1, scale = 1/rate) with parameters shape and scale (or alternatively specified by rate).\nGeometric: rgeom(n, prob) with probability parameter prob.\nHypergeometric: rhyper(nn, m, n, k) with m white balls, n black balls, and k balls chosen.\nLogistic: rlogis(n, location = 0, scale = 1) with parameters location and scale.\nLog Normal: rlnorm(n, meanlog = 0, sdlog = 1) with mean meanlog and standard deviation sdlog on the log scale.\nNegative Binomial: rnbinom(n, size, prob, mu) with parameters size and prob.\nNormal: rnorm(n, mean = 0, sd = 1) with mean equal to mean and standard deviation equal to sd.\nPoisson: rpois(n, lambda) with parameter lambda.\nStudent t: rt(n, df, ncp) with df degrees of freedom (and optional non-centrality parameter ncp).\nUniform: runif(n, min = 0, max = 1) with minimum value min and maximum value max.\nWeibull: rweibull(n, shape, scale = 1) with parameters shape and scale.\nWilcoxon Rank Sum: rwilcox(nn, m, n) with nn number of observations and sample sizes m and n.\nWilcoxon Signed Rank: rsignrank(nn, n) with nn number of observations and sample size n.\n\n\n\n10.1.6 Practice Question\nSet the random seed to be 123, and then generate 5 random numbers following a uniform distribution with min 1 and max 5. Then, find the 0.15 quantile for this same distribution (it should be equal to 1.6).\n\n# Insert your solution here:",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability Distributions in R</span>"
    ]
  },
  {
    "objectID": "book/distributions.html#empirical-distributions-and-sampling-data",
    "href": "book/distributions.html#empirical-distributions-and-sampling-data",
    "title": "10  Probability Distributions in R",
    "section": "10.2 Empirical Distributions and Sampling Data",
    "text": "10.2 Empirical Distributions and Sampling Data\nAt the start of this chapter, we used the sample() function. This function can also be used to sample from an empirical distribution. The sample(x, size, replace=FALSE, prob=NULL) function takes in the values we want to sample from x, the number of observations we want to sample size, and whether we want to sample with replacement replace. If we don’t want to sample such that each value has an equal probability of being chosen, we can also set a probability vector prob, which must have the same length as x. In the following code, we sample 500 rows without replacement from the NHANESsample data. To do so, we select 500 values from the indices 1 to the number of rows in the data. We then select rows of the data using these indices.\n\nnhanes_sample_ids &lt;- sample(1:nrow(NHANESsample), 500, replace = FALSE)\nnhanes_sample &lt;- NHANESsample[nhanes_sample_ids, ]\ndim(nhanes_sample)\n#&gt; [1] 500  21\n\nWe now demonstrate sampling with replacement. By doing so, we create a new data set that is sampled from the empirical distribution of the data and that is called a bootstrap sample.\n\nnhanes_sample_ids &lt;- sample(1:nrow(NHANESsample), nrow(NHANESsample), \n                            replace = TRUE)\nnhanes_sample &lt;- NHANESsample[nhanes_sample_ids, ]\ndim(nhanes_sample)\n#&gt; [1] 31265    21\n\nAnother way to sample from a data frame is to use the slice_sample() function from the tidyverse. In this function, we can either specify the number of observations to sample n or the proportion of observations to sample prop. Additionally, we can sample with or without replacement by setting the value of the argument replace (with default value FALSE). We use this function to randomly sample 20% of observations without replacement.\n\nnhanes_sample &lt;- NHANESsample %&gt;%\n  slice_sample(prop = 0.2, replace = FALSE)\ndim(nhanes_sample)\n#&gt; [1] 6253   21\n\n\n10.2.1 Practice Question\nSet the random seed to 5 and then sample 50 observations with replacement from the set of integers from 1 to 100. Take the mean of those observations - it should be 56.7.\n\n# Insert your solution here:\n\nBeyond sampling, we can also find the empirical cumulative distribution. That is, we can use a given vector to infer a distribution. In the following case, we draw a random sample from a normal distribution vec and then find its empirical cumulative distribution using the ecdf() function. This function actually returns a function, which can then be used to find the sample cumulative distribution for different values similar to the p[dist]() functions. In our example, we find the sample probability that \\(X \\leq 0\\).\n\nvec &lt;- rnorm(100) \necdf_vec &lt;- ecdf(vec)\necdf_vec(0)\n#&gt; [1] 0.63\n\nWe now plot this empirical distribution against the actual cdf using the pnorm() function. Note that in order to do so, we create a sequence of possible x values to pass to both pnorm() and ecdf_vec().\n\ndf &lt;- data.frame(x = seq(-3, 3, 0.05))\ndf$ecdf &lt;- ecdf_vec(df$x)\ndf$distn = pnorm(df$x)\n\nggplot(df) + \n  geom_line(aes(x = x, y = ecdf), color = \"black\") + \n  geom_line(aes(x = x, y= distn), color = \"red\") \n\n\n\n\n\n\n\n\nIn practice, the empirical cumulative distribution might involve data from a given data set that you want to use to represent the population’s distribution. As an example, in the following code we find the empirical distribution of blood lead level from the NHANESsample data frame. A blood lead level of 5 µg/dL or above is considered elevated. We can see 96.4% of observations have a blood lead level below this threshold.\n\necdf_lead &lt;- ecdf(nhanes_sample$LEAD)\necdf_lead(5)\n#&gt; [1] 0.961",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability Distributions in R</span>"
    ]
  },
  {
    "objectID": "book/distributions.html#recap-video",
    "href": "book/distributions.html#recap-video",
    "title": "10  Probability Distributions in R",
    "section": "10.3 Recap Video",
    "text": "10.3 Recap Video",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability Distributions in R</span>"
    ]
  },
  {
    "objectID": "book/distributions.html#exercises",
    "href": "book/distributions.html#exercises",
    "title": "10  Probability Distributions in R",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 2.2 inches. Using this distribution, answer the following questions.\n\nWhat is the probability that a randomly chosen female is 5 feet or shorter?\nWhat is the probability that a randomly chosen female is 6 feet or taller?\nGenerate 500 random observations following this distribution and find the sample 0.15 quantile. Then, compare this to the 0.15 quantile using the qdist() function.\n\nCompute the probability that the height of a randomly chosen female is within 1 SD from the average height.\nCreate a vector of 100 patient IDs, and then use the sample() function to assign half of them to a treatment group and the other half to a control group. Then, suppose those in the control group have a reduction in viral load distributed as \\(X \\sim 100*exp(mean = V)\\), where \\(V\\) follows a uniform distribution between 1 and 2, whereas those who are in the treatment group have a reduction in viral load distributed as \\(X \\sim 100*exp(mean = 3)\\). Plot distributions of reduction in viral load for both groups.",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability Distributions in R</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#univariate-distributions-and-one-sample-tests",
    "href": "book/hypothesis_tests.html#univariate-distributions-and-one-sample-tests",
    "title": "11  Hypothesis Testing",
    "section": "11.1 Univariate Distributions and One-Sample Tests",
    "text": "11.1 Univariate Distributions and One-Sample Tests\nLet’s begin by looking at a single outcome of interest - the number of induced terminations of pregnancy (referred to as ITOPs or abortions below) in 2021 per 1000 females ages 15-49 in each county. We use the number of females ages 15-49 as a proxy to scale the number of abortions by the population size, though this is not truly reflective of the number of people who can give birth in each county.\n\ncounty_rates_2021 &lt;- tex_itop$total_rate[tex_itop$year == 2021]\nhist(county_rates_2021, breaks = 35)\n\n\n\n\n\n\n\n\nWe can see in the figure that this is a heavy-tailed distribution. In the following code, we find the 10 counties with the highest rates and see that there are some counties that very few total abortions but that have some of the highest abortion rates. This indicates a small population. On the other hand, we also observe Harris county, which contains the city of Houston and has both a high total abortion count and a high abortion rate.\n\ntex_itop %&gt;% \n  filter(year == 2021) %&gt;% \n  slice_max(n = 10, total_rate) %&gt;%\n  dplyr::select(c(county, total_itop, total_rate))\n#&gt; # A tibble: 10 × 3\n#&gt;   county  total_itop total_rate\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Loving           1      111. \n#&gt; 2 Terrell          5       50  \n#&gt; 3 Concho           4       13.9\n#&gt; 4 Harris       14122       13.5\n#&gt; 5 Irion            3       12.9\n#&gt; # ℹ 5 more rows\n\nSome of the counties are so small that we may want to consider dropping them from our analysis. In particular, among these small counties the rates in Loving County and Terrel County are high enough that we might consider them to be outliers. For this one-sample analysis, however, we do not remove them. If we wanted to estimate the mean abortion rate among counties \\(\\mu\\) we can do so by simply using the mean() function. For reference, the Centers for Disease Control estimated the national abortion rate in 2020 to be 11.2 abortions per 1,000 women aged 15–44 years (Kortsmit 2023).\n\nmean(county_rates_2021, na.rm = TRUE)\n#&gt; [1] 5.17\n\nWithin R we can also calculate a confidence interval for this mean. Recall that a \\((1-\\alpha)\\)% confidence interval for the mean is given by the equation \\(\\hat{\\mu} \\pm z_{1-\\alpha/2} \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\), where \\(\\hat{\\mu}\\) is our sample mean, \\(\\hat{\\sigma}^2\\) is the sample variance, and \\(n\\) is the number of observations.\nIn the subsequent code chunk, we use this formula to calculate a 95% confidence interval for the mean abortion rate among counties:\n\nest_mean &lt;- mean(county_rates_2021, na.rm = TRUE)\nest_sd &lt;- sd(county_rates_2021)\nz_alpha &lt;- dnorm(1 - 0.05 / 2)\nn &lt;- length(county_rates_2021)\nc(est_mean - z_alpha * est_sd / sqrt(n), \n  est_mean + z_alpha * est_sd / sqrt(n))\n#&gt; [1] 5.04 5.29\n\nIf we want to display this nicely, we can use the round() function, which allows us to specify a number of digits to be displayed, and the paste() function, which creates a single character string from multiple inputs.\n\nlower &lt;- round(est_mean - z_alpha*est_sd/sqrt(n), 3)\nupper &lt;- round(est_mean + z_alpha*est_sd/sqrt(n), 3)\npaste(\"Confidence Interval: (\", lower, \",\", upper, \")\")\n#&gt; [1] \"Confidence Interval: ( 5.044 , 5.289 )\"\n\nSuppose that we wanted to run a hypothesis test to compare the mean to a pre-determined value. In particular, the Texas Heartbeat Act was introduced in 2021 and drastically reduced the number of eligible abortions. We could test whether there were significantly fewer abortions in 2021 compared to 2020 using a one-sided t-test. Our null hypothesis is that \\(\\mu \\geq 6.23\\), the mean abortion rate in 2020. To run this hypothesis test, we use the t.test() function. For a one-sample t-test, we need to specify our sample x, the alternative hypothesis alternative (default is a two-sided test), the true value of the mean mu (default 0), and a confidence level conf.level (default 0.95). In the following code, we run this t-test, and we can see from the result that we reject the null hypothesis at the 0.05 level and observe a statistically significant decline in the abortion rate in 2021.\n\nt.test(county_rates_2021, alternative = \"less\", mu = 6.23, \n       conf.level=0.95)\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  county_rates_2021\n#&gt; t = -2, df = 253, p-value = 0.02\n#&gt; alternative hypothesis: true mean is less than 6.23\n#&gt; 95 percent confidence interval:\n#&gt;  -Inf 5.98\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;      5.17\n\nThe output for this test is printed. If we want to reference these values, we need to save the result. The object t_test_res is a list that contains information about the statistic, p-value, confidence interval, etc. The list of outputs are similar to other test objects, so it is useful to look at what is contained in each by reading the test documentation (?t.test). We find the p-value from t_test_res.\n\nt_test_res &lt;- t.test(county_rates_2021, alternative = \"less\", \n                     mu = 6.23, conf.level = 0.95)\nnames(t_test_res)\n#&gt;  [1] \"statistic\"   \"parameter\"   \"p.value\"     \"conf.int\"   \n#&gt;  [5] \"estimate\"    \"null.value\"  \"stderr\"      \"alternative\"\n#&gt;  [9] \"method\"      \"data.name\"\n\n\nt_test_res$p.value\n#&gt; [1] 0.0161\n\n\n11.1.1 Practice Question\nTest whether there were significantly more abortions in 2019 compared to 2020 using a one-sided t-test. Your test statistic should be -6.4736.\n\n# Insert your solution here:\n\nOne thing to consider is that the t.test() function assumes that the sample x comes from a normal distribution. The one-sample Wilcoxon signed rank test is a non-parametric alternative to the one-sample t-test that can be used to compare the median value of a sample to a theoretical value without assuming that the data are normally distributed. This test can be performed using the wilcox.test() function and takes in the same arguments as the t.test() function. In the following output, we can see that we again reject the null hypothesis at the 0.05 level and conclude that the median abortion rate in 2021 was significantly lower than 5.14, which was the median rate in 2020.\n\nwilcox_res &lt;- wilcox.test(county_rates_2021, alternative = \"less\", \n                          mu = 5.14, conf.level = 0.95)\nwilcox_res\n#&gt; \n#&gt;  Wilcoxon signed rank test with continuity correction\n#&gt; \n#&gt; data:  county_rates_2021\n#&gt; V = 12807, p-value = 0.002\n#&gt; alternative hypothesis: true location is less than 5.14\nwilcox_res$p.value\n#&gt; [1] 0.00193",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#correlation-and-covariance",
    "href": "book/hypothesis_tests.html#correlation-and-covariance",
    "title": "11  Hypothesis Testing",
    "section": "11.2 Correlation and Covariance",
    "text": "11.2 Correlation and Covariance\nWe now look at two-sample tests. To start, we look at the 2020 and 2021 rates by county. We pivot our data into a wider format in order to create 2020 and 2021 rate columns, and, this time, we filter out the Loving and Terrel counties to remove outliers. We then create a scatter plot of 2021 vs. 2020 rates and observe a linear correlation between the two.\n\ncounty_rates &lt;- tex_itop %&gt;%\n  dplyr::select(c(county, total_rate, year)) %&gt;%\n  filter(!(county %in% c(\"Terrell\", \"Loving\")), \n         year %in% c(2020, 2021)) %&gt;%\n  pivot_wider(names_from = year, values_from = total_rate) %&gt;%\n  na.omit() %&gt;%\n  rename(\"y2020\" = \"2020\", \"y2021\" = \"2021\")\nhead(county_rates)\n#&gt; # A tibble: 6 × 3\n#&gt;   county    y2020 y2021\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Anderson   6.84 5.07 \n#&gt; 2 Andrews    1.85 0.792\n#&gt; 3 Angelina   5.81 6.00 \n#&gt; 4 Aransas    3.44 7.18 \n#&gt; 5 Archer     1.47 0.733\n#&gt; 6 Armstrong  0    0\n\n\nggplot(county_rates) + \n geom_point(aes(x = y2020, y = y2021)) +\n labs(x = \"2020 ITOP Rates\", y =\"2021 ITOP Rates\")\n\n\n\n\n\n\n\n\nWe have seen before how to calculate the correlation between two columns using the cor() function. We can also calculate the covariance using the cov() function. As suspected, there is a positive correlation. The estimated covariance is around 5.2.\n\ncor(county_rates$y2020, county_rates$y2021)\n#&gt; [1] 0.5\ncov(county_rates$y2020, county_rates$y2021)\n#&gt; [1] 5.2\n\nBesides calculating the value of the correlation, we can also test whether this correlation is significantly different from zero. The function cor.test() tests for association between paired samples, using either Pearson’s product moment correlation coefficient, Kendall’s \\(\\tau\\), or Spearman’s \\(\\rho.\\) Similar to the t.test() and wilcox.test() functions, we can also specify the alternative and conf.level arguments. In the following code, we test whether there is a non-zero correlation between the 2020 and 2021 county rates using Pearson’s product-moment correlation. We can see from the resulting p-value that we can reject the null hypothesis that the correlation is zero and conclude that it is instead significantly different than zero. This time we also print the computed confidence interval for our estimate.\n\ncor_test_res &lt;- cor.test(county_rates$y2020, \n                         county_rates$y2021, \n                         method = \"pearson\")\ncor_test_res\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  county_rates$y2020 and county_rates$y2021\n#&gt; t = 9, df = 250, p-value &lt;2e-16\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.401 0.587\n#&gt; sample estimates:\n#&gt; cor \n#&gt; 0.5\n\n\ncor_test_res$conf.int\n#&gt; [1] 0.401 0.587\n#&gt; attr(,\"conf.level\")\n#&gt; [1] 0.95",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#two-sample-tests-for-continuous-variables",
    "href": "book/hypothesis_tests.html#two-sample-tests-for-continuous-variables",
    "title": "11  Hypothesis Testing",
    "section": "11.3 Two-Sample Tests for Continuous Variables",
    "text": "11.3 Two-Sample Tests for Continuous Variables\nIf we wanted to directly compare the difference between 2020 and 2021 rates, we could use a two-sample test. In this case, because our samples are paired by county, we can use a two-sample paired t-test. Specifically, we use a two-sided test to test the null hypothesis that the rates are equal by specifying two different vectors x and y. Note that we used the default values of mu = 0 and alternative = \"two.sided\". Additionally, we used the default value var.equal = FALSE, which implies that the samples may have different variances. From the results, we reject the null hypothesis that the two county rates are equal at the 0.05 level. We also print a 95% confidence interval of the difference in means.\n\nt_test_two_res &lt;- t.test(x = county_rates$y2020, \n                         y = county_rates$y2021)\nt_test_two_res\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  county_rates$y2020 and county_rates$y2021\n#&gt; t = 2, df = 497, p-value = 0.01\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.145 1.278\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;      5.28      4.57\nt_test_two_res$conf.int\n#&gt; [1] 0.145 1.278\n#&gt; attr(,\"conf.level\")\n#&gt; [1] 0.95\n\nIn the tex_itop dataset, each county has also been categorized by whether it was urban or rural. Suppose we want to compare the change in abortion rates from 2020 to 2021 between rural and urban counties. First, we create a variable describing the rate change between these years using the following code. We choose to use the change in rate rather than percent change to avoid infinite or undefined values.\n\ncounty_rates_type &lt;- tex_itop %&gt;%\n  dplyr::select(c(county, urban, county_type, total_rate, year)) %&gt;%\n  filter(total_rate &lt; 15, year %in% c(2020, 2021)) %&gt;%\n  pivot_wider(names_from = year, values_from = total_rate) %&gt;%\n  na.omit() %&gt;%\n  rename(\"y2020\" = \"2020\", \"y2021\" = \"2021\") %&gt;%\n  mutate(rate_change = (y2021 - y2020)) \n\nWe again use a two-sample two-sided t-test, but this time the data are not paired. In the following code, we show an alternative way to specify a t-test test using a formula lhs ~ rhs, where lhs is a numeric column and rhs is a factor column with two levels. We must also specify the data in this case. From the R output in this case, we would fail to reject the null hypothesis at the 0.05 level and conclude that the rate changes for urban and rural counties are not significantly different. We also print the estimates used in the t-test using estimate, which shows the estimated mean in both groups.\n\nt_test_unpaired &lt;- t.test(rate_change ~ urban, \n                          data = county_rates_type)\nt_test_unpaired\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  rate_change by urban\n#&gt; t = 0.1, df = 205, p-value = 0.9\n#&gt; alternative hypothesis: true difference in means between group Rural and group Urban is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.495  0.563\n#&gt; sample estimates:\n#&gt; mean in group Rural mean in group Urban \n#&gt;              -0.469              -0.503\nt_test_unpaired$estimate\n#&gt; mean in group Rural mean in group Urban \n#&gt;              -0.469              -0.503\n\nNote that this yields the same results as if we had specified the data using two vectors x and y.\n\nx &lt;- county_rates_type$rate_change[county_rates_type$urban == 'Urban']\ny &lt;- county_rates_type$rate_change[county_rates_type$urban == 'Rural']\nt.test(x = x, y = y, paired = FALSE) \n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x and y\n#&gt; t = -0.1, df = 205, p-value = 0.9\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.563  0.495\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;    -0.503    -0.469\n\nBesides a t-test, we can also use a two-sample Wilcoxon non-parametric test using the wilcox.test() function, which has the same arguments as the function t.test(). Both the t.test() and wilcox.test() can only compare two groups. When we want to compare two or more independent samples, we can use a Kruskal-Wallis rank sum test using the kruskal.test() function or a one-way analysis of variance (ANOVA) using the aov() function.\nThis time we use the column county_type, which is an indicator for whether the county is urban, suburban, or rural according to the RUCC (rural-urban continuum codes) from the U.S. Department of Agriculture. For the kruskal.test() function, we can either specify the arguments formula (rate_change ~ county_type) and data (county_rates_type) or we can specify two vectors: x, a numeric vector, and g, a factor representing the group. For the aov() function, we specify the test using a formula and the data. To see the p-value, we have to use the summary() function to print the result. Again, both tests suggest that we fail to reject the null hypothesis at the 0.05 level.\n\nkruskal.test(county_rates_type$rate_change, \n             county_rates_type$county_type)\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  county_rates_type$rate_change and county_rates_type$county_type\n#&gt; Kruskal-Wallis chi-squared = 2, df = 2, p-value = 0.3\n\n\naov_res &lt;- aov(rate_change ~ county_type, \n               data = county_rates_type)\nsummary(aov_res)\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; county_type   2      7    3.36    0.53   0.59\n#&gt; Residuals   245   1547    6.31\n\n\n11.3.1 Practice Question\nUse an appropriate test to determine whether the ITOP rates in 2016 significantly differed by race. The test statistic should be 263.53 with associated p-value &lt; 2.2e-16.\n\n# Insert your solution here:\n\n\n\n11.3.2 Two-Sample Variance Tests\nWe could also test whether the variance of a continuous variable is equal between groups. To start, we compare the variance in abortion rates in 2021 between urban and rural counties using an F test. Our null hypothesis for this test is that the variance in both groups is equal. The function var.test() implements an F test and has the same main arguments as the t.test() function: vectors x and y OR a formula and data, the alternative hypothesis alternative, and conf.level. Additionally, we can specify the hypothesized ratio of the variances through the arugment ratio (default value 1). Note that this function assumes that the two samples come from normally distributed populations. We fail to reject the null hypothesis that the variance in rates are equal at the 0.05 level and print the estimate of the ratio of variances, which is around 1.11.\n\nf_test &lt;- var.test(y2021 ~ urban, county_rates_type)\nf_test\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  y2021 by urban\n#&gt; F = 1, num df = 187, denom df = 59, p-value = 0.6\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  0.719 1.657\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;               1.12\nf_test$estimate\n#&gt; ratio of variances \n#&gt;               1.12\n\nLastly, we implement a Levene’s test to test whether group variances are equal when there are more than two groups. This test can be specified using a formula and data set, as demonstrated, or by providing two vectors y, a numeric vector, and g, a vector specifying the groups. This test is from the car package and has slightly different output than other tests. In particular, to access the p-value, we need to access the value named 'Pr(&gt;F)'. In this case, we actually do reject the null hypothesis at the 0.05 level.\n\nlevene_test &lt;- leveneTest(y2021 ~ as.factor(county_type), \n                              county_rates_type)\nprint(levene_test)\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)  \n#&gt; group   2    3.41  0.034 *\n#&gt;       245                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlevene_test[['Pr(&gt;F)']]\n#&gt; [1] 0.0345     NA",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#two-sample-tests-for-categorical-variables",
    "href": "book/hypothesis_tests.html#two-sample-tests-for-categorical-variables",
    "title": "11  Hypothesis Testing",
    "section": "11.4 Two-Sample Tests for Categorical Variables",
    "text": "11.4 Two-Sample Tests for Categorical Variables\nIn the previous two-sample tests, we were comparing the distributions of continuous variables. We now look at comparing distributions of categorical variables. We first categorize counties by their abortion rate in 2020 being above or below 11.2, which was the national average rate that year. We display the distribution of this variable by the urban/rural grouping using a contingency table below.\n\ncounty_rates_type$below_nat_avg &lt;- \n  ifelse(county_rates_type$y2020 &gt; 11.2, \"Above Nat Avg\", \n         \"Below Nat Avg\")\ntable(county_rates_type$below_nat_avg, county_rates_type$urban)\n#&gt;                \n#&gt;                 Rural Urban\n#&gt;   Above Nat Avg     3     4\n#&gt;   Below Nat Avg   185    56\n\nWe can use a Fisher’s exact test to test whether the classifications of being above and below the national average and being rural and urban are associated with each other. In this case, the null hypothesis is that the odds or being below the national average is equal between rural and urban counties. The fisher.test() function can either take in a contingency table as a matrix or can be specified by two factor vectors x and y, which is how we implement it in the following code. Additionally, there is the option to specify the alternative and conf.level arguments. We do not see a statistically significant difference between urban and rural counties at the 0.05 level with the estimated odds ratio is around 0.23.\n\nfisher_test &lt;- fisher.test(county_rates_type$urban, \n                           county_rates_type$below_nat_avg)\nfisher_test\n#&gt; \n#&gt;  Fisher's Exact Test for Count Data\n#&gt; \n#&gt; data:  county_rates_type$urban and county_rates_type$below_nat_avg\n#&gt; p-value = 0.06\n#&gt; alternative hypothesis: true odds ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  0.0325 1.3955\n#&gt; sample estimates:\n#&gt; odds ratio \n#&gt;      0.229\nfisher_test$estimate\n#&gt; odds ratio \n#&gt;      0.229\n\nAn alternative test is a Pearson’s Chi-Squared test, which can be used for large sample sizes. The counts of rural and urban counties in the ‘Above Nat Avg’ category are very small, so we recategorize our outcome to be at or above Texas’s average to avoid this complication. The chisq.test() function also takes in a contingency table as a matrix or can be specified by two factor vectors x and y. Another useful argument is correct (default is TRUE) which indicates whether to apply a continuity correction. For this test, we observe a statistically significant difference in the proportion of counties above the national average between rural and urban counties and reject the null hypothesis at the 0.05 level.\n\ntex_mean &lt;- mean(county_rates_type$y2020)\ncounty_rates_type$below_tex_avg &lt;- \n  ifelse(county_rates_type$y2020 &gt; tex_mean, \"Above Texas Ave\", \n         \"Below Texas Ave\")\ntable(county_rates_type$below_tex_avg, county_rates_type$urban)\n#&gt;                  \n#&gt;                   Rural Urban\n#&gt;   Above Texas Ave    84    39\n#&gt;   Below Texas Ave   104    21\n\n\nchi_sq &lt;- chisq.test(county_rates_type$below_tex_avg, \n           county_rates_type$urban)\nchi_sq\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  county_rates_type$below_tex_avg and county_rates_type$urban\n#&gt; X-squared = 7, df = 1, p-value = 0.01\nchi_sq$p.value\n#&gt; [1] 0.00953\n\n\n11.4.1 Practice Question\nRepeat the Chi-Squared test, but this time use the RUCC codes instead of the urban column. You should get a p-value of 0.2799. Think about what could explain the difference between these results.\n\n# Insert your solution here:",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#adding-hypothesis-tests-to-summary-tables",
    "href": "book/hypothesis_tests.html#adding-hypothesis-tests-to-summary-tables",
    "title": "11  Hypothesis Testing",
    "section": "11.5 Adding Hypothesis Tests to Summary Tables",
    "text": "11.5 Adding Hypothesis Tests to Summary Tables\nIn Chapter 4, we used the gt and gtsummary packages to create summary tables of variables. When creating a stratified table (done by adding the by argument), we can automatically add p-values for hypothesis tests comparing across populations using the add_p() function. By default, the add_p() function uses a Kruskal-Wallis rank sum test for continuous variables (or a Wilcoxon rank sum test when the by variable has two levels) and uses a Chi-Squared Contingency Table Test for categorical variables (or a Fisher’s Exact Test for categorical variables with any expected cell count less than five). The chosen test(s) are displayed as footnotes.\n\ntbl_summary(tex_itop, include = c(total_rate, white_rate, asian_rate, \n                                  hispanic_rate, black_rate, \n                                  native_american_rate),\n           by = \"year\", \n           statistic = list(all_continuous() ~ \"{mean} ({sd})\")) %&gt;% \n  add_p() %&gt;%\n  as_gt() \n\n\n\n\n\n\n\nFigure 11.1: Summary Table Stratified by Year.\n\n\n\nWe observe that a Kruskal-Wallis rank sum test was used to compare abortion rates across year for each racial group. All of the reported p-values are above 0.05 so overall it indicates that there were not statistically significant changes across years in the abortion rate.",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#recap-video",
    "href": "book/hypothesis_tests.html#recap-video",
    "title": "11  Hypothesis Testing",
    "section": "11.6 Recap Video",
    "text": "11.6 Recap Video",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/hypothesis_tests.html#exercises",
    "href": "book/hypothesis_tests.html#exercises",
    "title": "11  Hypothesis Testing",
    "section": "11.7 Exercises",
    "text": "11.7 Exercises\nFor the following exercises, we use the pain data from the HDSinRdata package.\n\ndata(pain)\n\n\nDetermine whether the presence or absence of follow-up information is significantly associated with the initial average pain intensity. What do the results suggest?\nFirst, plot PROMIS_PAIN_BEHAVIOR grouped by race (you can use the PAT_RACE_CAT variable that we defined in Chapter 8. What do you observe? Next, choose an appropriate test to determine whether this variable differs significantly by race.\nExamine the association between CCI_BIN and MEDICAID_BIN. Are these variables significantly related to each other? How would you describe their relationship?\nRecreate the summary table in Figure 11.2. Then, recreate the p-values for PROMIS_DEPRESSION, PROMIS_ANXIETY, and MEDICAID_BIN using the appropriate tests.\n\n\n\n\n\n\n\nFigure 11.2: Stratified Summary Table.\n\n\n\n\n\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2023. car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nKortsmit, Katherine. 2023. “Abortion Surveillance—United States, 2021.” MMWR. Surveillance Summaries 72.\n\n\nTexas Health & Human Services Commission. 2016-2021. “Induced Terminations of Pregnancy.” Texas Department of State Health Services. https://www.hhs.texas.gov/about/records-statistics/data-statistics/itop-statistics.",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/cs_testing.html",
    "href": "book/cs_testing.html",
    "title": "12  Case Study: Hypothesis Testing",
    "section": "",
    "text": "For this chapter, we use the NHANESsample dataset seen in Chapter 4. The sample contains lead, blood pressure, BMI, smoking status, alcohol use, and demographic variables from NHANES 1999-2018. Variable selection and feature engineering were conducted to replicate the preprocessing conducted by Huang (2022). We further replicate the regression analysis by Huang (2022) in Chapter 13. Use the help operator ?NHANESsample to read the variable descriptions. Note that we ignore survey weights for this analysis.\n\nlibrary(HDSinRdata)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtsummary)\n\ndata(\"NHANESsample\")\n\nOur analysis focuses on using hypothesis testing to look at the association between hypertension and blood lead levels by sex. We first select some demographic and clinical variables that we believe may be relevant, including age, sex, race, body mass index, and smoking status. We do a complete case analysis and drop any observations with missing data.\n\nNHANESsample &lt;- NHANESsample %&gt;%\n  select(\"AGE\", \"SEX\", \"RACE\", \"SMOKE\", \"LEAD\", \"BMI_CAT\", \n         \"HYP\", \"ALC\") %&gt;%\n  na.omit()\n\nWe begin with a summary table stratified by hypertension status. As expected, we see statistically significant differences between the two groups across all included variables. We also observe higher blood lead levels and a higher proportion of male participants for those with hypertension.\n\ntbl_summary(NHANESsample, by = c(\"HYP\"),\n            label = list(SMOKE ~ \"SMOKING STATUS\",\n                         BMI_CAT ~ \"BMI\",\n                         ALC ~ \"ALCOHOL USE\")) %&gt;%\n  add_p() %&gt;%\n  add_overall() %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \n                           \"**Hypertension Status**\") %&gt;%\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall, N = 30,4251\nHypertension Status\np-value2\n\n\n0, N = 13,7351\n1, N = 16,6901\n\n\n\n\nAGE\n48 (34, 63)\n37 (28, 50)\n57 (44, 69)\n&lt;0.001\n\n\nSEX\n\n\n\n\n\n\n&lt;0.001\n\n\n    Male\n16,031 (53%)\n6,410 (47%)\n9,621 (58%)\n\n\n\n\n    Female\n14,394 (47%)\n7,325 (53%)\n7,069 (42%)\n\n\n\n\nRACE\n\n\n\n\n\n\n&lt;0.001\n\n\n    Mexican American\n5,184 (17%)\n2,725 (20%)\n2,459 (15%)\n\n\n\n\n    Other Hispanic\n2,207 (7.3%)\n1,145 (8.3%)\n1,062 (6.4%)\n\n\n\n\n    Non-Hispanic White\n15,108 (50%)\n6,750 (49%)\n8,358 (50%)\n\n\n\n\n    Non-Hispanic Black\n5,853 (19%)\n2,077 (15%)\n3,776 (23%)\n\n\n\n\n    Other Race\n2,073 (6.8%)\n1,038 (7.6%)\n1,035 (6.2%)\n\n\n\n\nSMOKING STATUS\n\n\n\n\n\n\n&lt;0.001\n\n\n    NeverSmoke\n14,682 (48%)\n7,210 (52%)\n7,472 (45%)\n\n\n\n\n    QuitSmoke\n8,566 (28%)\n2,990 (22%)\n5,576 (33%)\n\n\n\n\n    StillSmoke\n7,177 (24%)\n3,535 (26%)\n3,642 (22%)\n\n\n\n\nLEAD\n1.39 (0.85, 2.20)\n1.14 (0.71, 1.85)\n1.59 (1.00, 2.48)\n&lt;0.001\n\n\nBMI\n\n\n\n\n\n\n&lt;0.001\n\n\n    BMI&lt;=25\n9,007 (30%)\n5,313 (39%)\n3,694 (22%)\n\n\n\n\n    25&lt;BMI&lt;30\n10,456 (34%)\n4,718 (34%)\n5,738 (34%)\n\n\n\n\n    BMI&gt;=30\n10,962 (36%)\n3,704 (27%)\n7,258 (43%)\n\n\n\n\nALCOHOL USE\n24,174 (79%)\n11,624 (85%)\n12,550 (75%)\n&lt;0.001\n\n\n\n1 Median (IQR); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\nWe also plot the distribution of blood lead levels (on a log scale) by sex and hypertension status. We can visually see that male observations tend to have higher blood lead levels and that having hypertension is associated with higher blood lead levels.\n\nggplot(NHANESsample) +\n  geom_boxplot(aes(x=LEAD,\n                   y = interaction(HYP,SEX),\n                   color = interaction(HYP,SEX))) +\n  scale_x_continuous(trans = \"log\", breaks = c(0.1, 1, 10, 50)) +\n  scale_y_discrete(labels = c(\"Male : 0\", \"Male : 1\",\n                              \"Female : 0\", \"Female : 1\")) +\n  guides(color = \"none\") +\n  labs(x=\"Blood Lead Level\",\n       y = \"Sex : Hypertension Status\")\n\n\n\n\n\n\n\n\nIn Chapter 10, we explored that log blood lead levels could be approximated by a normal distribution. To test our hypothesis that there is a difference in mean log blood lead level between those with and without hypertension, we use a two-sample unpaired t-test. This shows a statistically significant difference between the two groups at the 0.05 level.\n\nt.test(log(LEAD) ~ HYP, data = NHANESsample)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  log(LEAD) by HYP\n#&gt; t = -37, df = 28853, p-value &lt;2e-16\n#&gt; alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.314 -0.282\n#&gt; sample estimates:\n#&gt; mean in group 0 mean in group 1 \n#&gt;           0.161           0.459\n\nFinally, we repeat this test for a stratified analysis and present the results in a concise table. For both groups, we find a statistically significant difference at the 0.05 level.\n\n# stratify the data\nnhanes_male &lt;- NHANESsample[NHANESsample$SEX == \"Male\",]\nnhanes_female &lt;- NHANESsample[NHANESsample$SEX == \"Female\",]\n\n# t-test for each\ntest_male &lt;- t.test(log(LEAD) ~ HYP, data = nhanes_male)\ntest_female &lt;- t.test(log(LEAD) ~ HYP, data = nhanes_female)\n\n# create data frame\nres_df &lt;- data.frame(group = c(\"Male\", \"Female\"),\n                     statistic = signif(c(test_male$statistic,\n                                          test_female$statistic), 3),\n                     p.value = signif(c(test_male$p.value,\n                                          test_female$p.value), 3))\nres_df\n#&gt;    group statistic   p.value\n#&gt; 1   Male     -14.7  1.84e-48\n#&gt; 2 Female     -32.3 4.35e-221\n\nIn Chapter 13, we use linear regression to further explore the association between blood lead level and hypertension adjusting for other potential confounders.\n\n\n\n\nHuang, Ziyao. 2022. “Association Between Blood Lead Level with High Blood Pressure in US (NHANES 1999–2018).” Frontiers in Public Health 10: 836357.",
    "crumbs": [
      "Distributions and Hypothesis Testing",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#simple-linear-regression",
    "href": "book/linear_regression.html#simple-linear-regression",
    "title": "13  Linear Regression",
    "section": "13.1 Simple Linear Regression",
    "text": "13.1 Simple Linear Regression\nIn Chapter 4, we presented some initial exploratory analysis for this data. In this chapter, we use linear regression to understand the association between blood lead levels and systolic blood pressure, adjusting for possible confounders. Replicating the analysis of Huang (2022), we create summary columns for systolic and diastolic blood pressure. If an observation has one blood pressure reading, then we use that value. If there is more than one blood pressure reading, then we drop the first observation and average the rest. We do a complete case analysis by dropping any observation with NA values. This leaves us with 30,405 observations.\n\nNHANESsample$SBP &lt;- apply(NHANESsample[,c(\"SBP1\", \"SBP2\", \"SBP3\", \n                                          \"SBP4\")], 1, \n    function(x) case_when(sum(!is.na(x)) == 0 ~ NA, \n                          sum(!is.na(x)) == 1 ~ sum(x, na.rm = TRUE),\n                          sum(!is.na(x)) &gt; 1 ~ mean(x[-1], \n                                                    na.rm = TRUE))) \nNHANESsample$DBP &lt;- apply(NHANESsample[,c(\"DBP1\", \"DBP2\", \"DBP3\", \n                                          \"DBP4\")], 1, \n    function(x) case_when(sum(!is.na(x)) == 0 ~ NA, \n                          sum(!is.na(x)) == 1 ~ sum(x, na.rm = TRUE),\n                          sum(!is.na(x)) &gt; 1 ~ mean(x[-1], \n                                                    na.rm = TRUE))) \nnhanes_df &lt;- na.omit(subset(NHANESsample, \n                            select= -c(SBP1, SBP2, SBP3, SBP4, DBP1, \n                                       DBP2, DBP3, DBP4)))\ndim(nhanes_df)\n#&gt; [1] 30405    15\n\nNext, we make sure any categorical variables are coded as factors.\n\nnhanes_df$SEX &lt;- as.factor(nhanes_df$SEX)\nnhanes_df$RACE &lt;- as.factor(nhanes_df$RACE)\nnhanes_df$EDUCATION &lt;- as.factor(nhanes_df$EDUCATION)\nnhanes_df$BMI_CAT &lt;- as.factor(nhanes_df$BMI_CAT)\nnhanes_df$LEAD_QUANTILE &lt;- as.factor(nhanes_df$LEAD_QUANTILE)\n\nWe start with simple linear regression. In the following code, we plot the relationship between blood lead level and systolic blood pressure. For a simple linear regression scenario with a single continuous independent variable, a scatter plot allows us to easily visualize whether we meet the assumptions underlying linear regression. The survey sampling for the NHANES survey allows us to assume that each observation is independent. If we meet the assumptions of linear regression, we also expect the plot to show that the average systolic blood pressure increases linearly with blood lead level and that the observations look normally distributed with equal variance along that line. We do not observe that to be the case. We come back to this in the section on transformations and interactions.\n\nplot(nhanes_df$LEAD, nhanes_df$SBP,\n     xlab = \"Blood Lead Level\", ylab = \"Systolic Blood Pressure\", \n     pch = 16)\n\n\n\n\n\n\n\n\nDespite our observations, we continue by fitting a simple linear regression model to explain the association between SBP and LEAD. The function lm(formula = y ~ x, data) fits a linear model in R. The first argument is the formula of the linear model: on the left hand side of the ~ we put the outcome variable, and on the right hand side we put the independent variable. When we have multiple independent variables we separate them with a + (e.g. y~x1+x2). The output of this function is an lm object.\nWe can call the summary() function on this object to print a summary of the model, which includes the estimated coefficients, information about the residuals, the R-squared and adjusted R-squared values, and the F-statistic. Recall, that we previously used the summary() function to get summary statistics about a vector. This is an example of how multiple functions can have the same name. R figues out which summary() function to use by identifying that the argument we passed in is a lm object.\n\nsimp_model &lt;- lm(formula = SBP ~ LEAD, data = nhanes_df)\nsummary(simp_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = SBP ~ LEAD, data = nhanes_df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -96.36 -12.52  -2.79   9.36 140.88 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  120.665      0.149   807.1   &lt;2e-16 ***\n#&gt; LEAD           1.708      0.058    29.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.5 on 30403 degrees of freedom\n#&gt; Multiple R-squared:  0.0277, Adjusted R-squared:  0.0277 \n#&gt; F-statistic:  867 on 1 and 30403 DF,  p-value: &lt;2e-16\n\nTo visualize this model, we can add the estimated regression line to our scatter plot. In ggplot2, this can be done with the geom_smooth() function. In base R, we use the abline() function, which can take in a regression model as an input. We can see that the estimated regression line does not fit our data very well.\n\nplot(nhanes_df$LEAD, nhanes_df$SBP, \n     ylab = c(\"Systolic Blood Pressure\"),\n     xlab = c(\"Blood Lead Level\"), pch = 16)\nabline(simp_model, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n13.1.1 Practice Question\nFit a simple linear regression model with SBP as the outcome and AGE as the independent variable. The estimated coefficient for AGE should be 0.47693. Then, plot these two variables against each other and add the estimated regression line to the plot, as we did previously. You should see that this regression has a better fit than the previous one.\n\n# Insert your solution here:",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#multiple-linear-regression",
    "href": "book/linear_regression.html#multiple-linear-regression",
    "title": "13  Linear Regression",
    "section": "13.2 Multiple Linear Regression",
    "text": "13.2 Multiple Linear Regression\nWe now create a model that is similar to the previous one except that it also adjusts for age and sex. To add these variables into the model, we have to specify a new formula. In the following code chunk, we fit this model and then print a summary, again using the summary() function.\n\nadj_model &lt;- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)\nsummary(adj_model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = SBP ~ LEAD + AGE + SEX, data = nhanes_df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -65.62 -10.59  -1.55   8.55 131.60 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 101.78541    0.30353  335.34  &lt; 2e-16 ***\n#&gt; LEAD          0.40007    0.05525    7.24  4.5e-13 ***\n#&gt; AGE           0.46193    0.00557   82.97  &lt; 2e-16 ***\n#&gt; SEXFemale    -2.77774    0.19567  -14.20  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 16.6 on 30401 degrees of freedom\n#&gt; Multiple R-squared:  0.212,  Adjusted R-squared:  0.212 \n#&gt; F-statistic: 2.72e+03 on 3 and 30401 DF,  p-value: &lt;2e-16\n\nWe can also extract the estimated regression coefficients from the model using the coef() function or by using the tidy() function from the broom package. This function puts the coefficient estimates, standard errors, statistics, and p-values in a data frame. We can also add a confidence interval by specifying conf.int = TRUE. In our example, we add a 95% confidence interval (which is the default value for conf.level).\n\ncoef(adj_model)\n#&gt; (Intercept)        LEAD         AGE   SEXFemale \n#&gt;     101.785       0.400       0.462      -2.778\n\n\ntidy(adj_model, conf.int = TRUE, conf.level = 0.95)\n#&gt; # A tibble: 4 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)  102.      0.304      335.   0         101.      102.   \n#&gt; 2 LEAD           0.400   0.0552       7.24 4.54e-13    0.292     0.508\n#&gt; 3 AGE            0.462   0.00557     83.0  0           0.451     0.473\n#&gt; 4 SEXFemale     -2.78    0.196      -14.2  1.36e-45   -3.16     -2.39\n\nSome other useful summary functions are resid(), which returns the residual values for the model, and fitted(), which returns the fitted values or estimated y values. We can also predict on new data using the predict() function. In the following plot, we look at the distribution of the residual values and then plot the fitted vs. true values. We observe some extreme residual values as well as the fact that the absolute residual values increase with increased blood pressure values.\n\nsummary(resid(adj_model))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   -65.6   -10.6    -1.6     0.0     8.5   131.6\n\n\nplot(nhanes_df$SBP, fitted(adj_model), \n     xlab = \"True Systolic Blood Pressure\", \n     ylab = \"Predicted Systolic Blood Pressure\", pch = 16)\nabline(a = 0, b = 1, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nWe can next perform a nested hypothesis test between our simple linear regresion model and our adjusted model using the anova() function. We pass both models to this function along with the argument test=\"F\" to indicate that we are performing an F-test. The print() function shows the two tested models along with the associated p-value, which indicates a significantly better fit for the adjusted model.\n\nprint(anova(simp_model, adj_model, test= \"F\"))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: SBP ~ LEAD\n#&gt; Model 2: SBP ~ LEAD + AGE + SEX\n#&gt;   Res.Df      RSS Df Sum of Sq    F Pr(&gt;F)    \n#&gt; 1  30403 10375769                             \n#&gt; 2  30401  8413303  2   1962467 3546 &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe model summary for the adjusted model displays the estimated coefficient for sex as SEXFemale, which indicates that the reference level for sex is male. If we want to change our reference level, we can reorder the factor variable either by using the factor() function and specifying Female as the first level or by using the relevel() function. The ref argument in the relevel() function specifies the new reference level. Now, when we run the model, we can see that the estimated coefficient for sex is labeled as SEXMale.\n\nnhanes_df$SEX &lt;- relevel(nhanes_df$SEX, ref = \"Female\")\nadj_model2 &lt;- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)\ntidy(adj_model2)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)   99.0     0.293      338.   0       \n#&gt; 2 LEAD           0.400   0.0552       7.24 4.54e-13\n#&gt; 3 AGE            0.462   0.00557     83.0  0       \n#&gt; 4 SEXMale        2.78    0.196       14.2  1.36e-45\n\nThe formula passed to the lm() function also allows us to use the . to indicate that we would like to include all remaining columns as independent variables or the - to exclude variables. In the following code chunk, we show how we could use these to fit a model with LEAD, AGE, and SEX as included covariates by excluding all other variables instead of by specifying these three variables themselves.\n\nlm(SBP ~ . - ID - RACE - EDUCATION - INCOME - SMOKE - YEAR - BMI_CAT - \n   LEAD_QUANTILE - DBP - ALC - HYP - RACE, data = nhanes_df)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = SBP ~ . - ID - RACE - EDUCATION - INCOME - SMOKE - \n#&gt;     YEAR - BMI_CAT - LEAD_QUANTILE - DBP - ALC - HYP - RACE, \n#&gt;     data = nhanes_df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          AGE      SEXMale         LEAD  \n#&gt;      99.008        0.462        2.778        0.400",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#diagnostic-plots-and-measures",
    "href": "book/linear_regression.html#diagnostic-plots-and-measures",
    "title": "13  Linear Regression",
    "section": "13.3 Diagnostic Plots and Measures",
    "text": "13.3 Diagnostic Plots and Measures\nWe can tell from the previous plot that our model doesn’t have a great fit. We use some further diagnostic plots and measures to learn more. R has some built-in plots available for linear regression models, which can be displayed using the plot() function. Similar the summary() function, this function acts differently when passed an lm object. The four plots include (a) Residuals vs. Fitted, (b) a QQ-plot for the residuals, (c) Standardized residuals (sqrt) vs. Fitted, and (d) Standardized Residuals vs. Leverage. In the last plot, you may observe that there is a dashed line. Any points outside of these lines have a Cook’s distance of greater than 0.5. Additionally, points with labels correspond to the points with the largest residuals, so this last plot summarizes the outliers, leverage, and influential points. The plots show that our residuals do not look normally distributed and that we have may have some high leverage points.\n\npar(mfrow = c(2, 2)) # plots all four plots together\nplot(adj_model)\n\n\n\n\n\n\n\n\n\n13.3.1 Normality\nBeyond the default plots, we can also plot a histogram of the residuals and a qq-plot. The qqnorm() and qqline() functions can take in the residuals from our model as an argument. The latter adds the theoretical red line for reference. As both the histogram and qq-plot shown, the residuals are positively skewed, and thus the assumption of normality is not satisfied for our residuals. Later in this chapter, we discuss how we might transform this dataset and/or model to satisfy this assumption.\n\npar(mfrow = c(1, 2)) # plot next to each other\nhist(resid(adj_model), xlab = \"Residuals\", \n     main = \"Histogram of Residuals\") \nqqnorm(resid(adj_model))\nqqline(resid(adj_model), col = \"red\") \n\n\n\n\n\n\n\n\nInstead of using the direct residuals, we can also find the standardized residuals with the function rstandard(). The standardized residuals are the raw residuals divided by an estimate of the standard deviation for the residual, which is different for each observation.\n\npar(mfrow = c(1, 2)) \nhist(rstandard(adj_model), xlab = \"Standardized Residuals\", \n     main = \"Histogram of Standardized Residuals\") \nqqnorm(rstandard(adj_model)) \nqqline(rstandard(adj_model), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n13.3.2 Homoscedasticity, Linearity, and Collinearity\nWe can also create a residual vs. fitted plot or plot the residuals against included covariates. In the following code, we plot the blood lead level against the residuals. In both plots, we are looking for the points to be spread roughly evenly around 0 with no discerning pattern. However, both plots show a funnel shape, indicating a growing and shrinking variance of residuals by level, respectively. This indicates that we are violating the homoscedasticity assumption.\n\npar(mfrow = c(1, 2))\nplot(fitted(adj_model), resid(adj_model), \n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nplot(nhanes_df$LEAD, resid(adj_model), \n     xlab = \"Blood Lead Level\", ylab = \"Residuals\")\n\n\n\n\n\n\n\n\nTo quantify any collinearity between the included covariates, we can calculate the variance inflation factors. The vif() function in the car package allows us to calculate the variance inflation factors or generalized variance inflation factors for all covariates. In our case, all the VIF values are around 1, indicating low levels of collinearity.\n\nvif(adj_model)\n#&gt; LEAD  AGE  SEX \n#&gt; 1.12 1.07 1.05\n\n\n\n13.3.3 Practice Question\nFit a linear regression model with SBP as the outcome and with INCOME, RACE, EDUCATION, and ALC as independent variables. Then, plot the residuals vs. the fitted values as well and make a QQ plot for the standardized residuals from this model. They should look like Figure 13.1.\n\n\n\n\n\n\nFigure 13.1: Residual Plots.\n\n\n\n\n# Insert your solution here: \n\n\n\n13.3.4 Leverage and Influence\nWe may also be interested in how each observation is influencing the model. Leverage values measure how much an individual observation’s \\(y\\) value influences its own predicted value and indicate whether observations have extreme predictor values compared to the rest of the data. Leverage values range from 0 to 1 and sum to the number of estimated coefficients. Observations with high leverage have the potential to significantly impact the estimated regression coefficients and the overall fit of the model. Therefore, examining leverage values helps identify observations that may be influential or outliers. In the following code chunk, we find the ten highest leverage values and then find those observations in the data.\n\nsort(hatvalues(adj_model), decreasing = TRUE)[1:10]\n#&gt;   23016    2511    3091   21891    3661     511   21892   15321    6511 \n#&gt; 0.03899 0.02936 0.02270 0.01484 0.01443 0.01399 0.01159 0.01080 0.01022 \n#&gt;    3452 \n#&gt; 0.00968\nnhanes_df[order(hatvalues(adj_model), decreasing = TRUE),] %&gt;% \n  select(c(SBP, LEAD, AGE, SEX)) %&gt;% \n  head(10)\n#&gt;       SBP LEAD AGE  SEX\n#&gt; 23016 129 61.3  38 Male\n#&gt; 2511  139 54.0  61 Male\n#&gt; 3091  154 48.0  72 Male\n#&gt; 21891 123 38.9  54 Male\n#&gt; 3661  101 38.0  39 Male\n#&gt; 511   118 37.3  34 Male\n#&gt; 21892 107 33.7  21 Male\n#&gt; 15321 104 33.1  39 Male\n#&gt; 6511  175 33.0  71 Male\n#&gt; 3452  113 31.4  38 Male\n\nSome other measures of influence are the DFBETAs and Cook’s distance, which measure how much each observation influences the estimated coefficients and the estimated y values, respectively. The influence.measures() function provides a set of measures that quantify the influence of each observation on a linear regression model: these include the DFBETAS for each model variable, DFFITS, covariance ratios, Cook’s distances, and the leverage values. The output returns the values in a matrix called infmat, which we convert to a data frame.\n\ninf_mat &lt;- influence.measures(adj_model)[['infmat']]\nas.data.frame(inf_mat) %&gt;% head()\n#&gt;      dfb.1_  dfb.LEAD   dfb.AGE  dfb.SEXF    dffit cov.r   cook.d\n#&gt; 1  0.013880 -0.017564 -1.68e-02  0.008319 -0.03427 1.000 2.93e-04\n#&gt; 2 -0.000732  0.000348 -3.92e-05  0.001051 -0.00150 1.000 5.59e-07\n#&gt; 3  0.022137  0.005749 -1.45e-02 -0.016843  0.02964 0.999 2.19e-04\n#&gt; 4  0.000499  0.001043 -2.07e-03  0.001631 -0.00312 1.000 2.43e-06\n#&gt; 5  0.002259 -0.002725 -2.50e-03  0.000973 -0.00498 1.000 6.20e-06\n#&gt; 6 -0.001283 -0.000559  1.65e-03 -0.002929 -0.00441 1.000 4.87e-06\n#&gt;        hat\n#&gt; 1 1.90e-04\n#&gt; 2 6.61e-05\n#&gt; 3 8.28e-05\n#&gt; 4 1.18e-04\n#&gt; 5 2.35e-04\n#&gt; 6 8.09e-05",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#interactions-and-transformations",
    "href": "book/linear_regression.html#interactions-and-transformations",
    "title": "13  Linear Regression",
    "section": "13.4 Interactions and Transformations",
    "text": "13.4 Interactions and Transformations\nWe now try to improve our model. To start, we look at potential transformations for our outcome variable. We consider a log transformation for both our outcome, systolic blood pressure, and our predictor of interest, blood lead level. Both of these variables have a fairly skewed distribution and may benefit from such a transformation. In the following code, you can see that the transformed variables have distributions that are more symmetrical.\n\npar(mfrow=c(2,2))\nhist(nhanes_df$SBP, xlab = \"Systolic Blood Pressure\", \n     main = \"\")\nhist(log(nhanes_df$SBP), xlab = \"Log Systolic Blood Pressure\", \n     main = \"\")\nhist(nhanes_df$LEAD, xlab = \"Blood Lead Level\", \n     main = \"\")\nhist(log(nhanes_df$LEAD), xlab = \"Log Blood Lead Level\", \n     main = \"\")\n\n\n\n\n\n\n\n\nTo add a transformation to a model, we can simply apply the transformation in the formula for lm(). We find the adjusted R-squared for each potential model to compare their fits in addition to plotting the four qq-plots. Both indicate that the model with the log-log transformation (that is, with a log transformation applied to both the SBP and the LEAD variables) is the best fit though the model with just a log transformation for SBP has a similar qq-plot.\n\nmodel_nlog_nlog &lt;- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)\nmodel_log_nlog &lt;- lm(log(SBP) ~ LEAD + AGE + SEX, data = nhanes_df)\nmodel_nlog_log &lt;- lm(SBP ~ log(LEAD) + AGE + SEX, data = nhanes_df)\nmodel_log_log &lt;- lm(log(SBP) ~ log(LEAD) + AGE + SEX, \n                    data = nhanes_df)\n\n\nsummary(model_nlog_nlog)$adj.r.squared\n#&gt; [1] 0.212\nsummary(model_log_nlog)$adj.r.squared\n#&gt; [1] 0.215\nsummary(model_nlog_log)$adj.r.squared\n#&gt; [1] 0.212\nsummary(model_log_log)$adj.r.squared\n#&gt; [1] 0.215\n\n\npar(mfrow=c(2,2))\nqqnorm(rstandard(model_nlog_nlog), main = \"Original Model\") \nqqline(rstandard(model_nlog_nlog), col = \"red\")\nqqnorm(rstandard(model_log_nlog), main = \"Log SBP\") \nqqline(rstandard(model_log_nlog), col = \"red\")\nqqnorm(rstandard(model_nlog_log), main = \"Log Lead\") \nqqline(rstandard(model_nlog_log), col = \"red\")\nqqnorm(rstandard(model_log_log), main = \"Log SBP, Log Lead\") \nqqline(rstandard(model_log_log), col = \"red\")\n\n\n\n\n\n\n\n\n\n13.4.1 Practice Question\nInstead of adding in a log transformation for LEAD like we did previously, try a square root transformation sqrt(LEAD) and an inverse transformation 1/LEAD while keeping the log transformation for the outcome log(SBP). Which model fits better according to the adjusted R-squared? The resulting QQ plots should look like Figure 13.2.\n\n\n\n\n\n\nFigure 13.2: QQ Plots for Possible Transformations.\n\n\n\n\n# Insert your solution here:\n\nAdditionally, we might consider polynomial transformations. The poly(x, degree=1) function allows us to specify a polynomial transformation where we might have higher degree terms. We do not pursue this approach for this particular example, but we show some example code for creating such a transformation (in this case, a cubic transformation for blood lead level).\n\nmodel_poly &lt;- lm(SBP ~ poly(LEAD, 3) + AGE + SEX, data = nhanes_df)\n\nWe can summarize the outcome for our log-log model using the tidy() function again. We observe small p-values for each estimated coefficient.\n\ntidy(model_log_log)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)  4.62    0.00239     1932.   0       \n#&gt; 2 log(LEAD)    0.00891 0.00118        7.53 5.34e-14\n#&gt; 3 AGE          0.00349 0.0000457     76.4  0       \n#&gt; 4 SEXMale      0.0254  0.00155       16.4  2.06e-60\n\nAnother component that we may want to add to our model is an interaction term. For example, we may consider an interaction between sex and blood lead level. We add an interaction to the formula using a : between the two variables. The output shows that the coefficient for this interaction is indeed significant.\n\nmodel_interaction &lt;- lm(log(SBP) ~ log(LEAD) + AGE + SEX + \n                          SEX:log(LEAD), data=nhanes_df) \nsummary(model_interaction)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(SBP) ~ log(LEAD) + AGE + SEX + SEX:log(LEAD), \n#&gt;     data = nhanes_df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.6981 -0.0816 -0.0049  0.0752  0.6599 \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        4.62e+00   2.39e-03  1936.2   &lt;2e-16 ***\n#&gt; log(LEAD)          2.36e-02   1.68e-03    14.1   &lt;2e-16 ***\n#&gt; AGE                3.45e-03   4.58e-05    75.3   &lt;2e-16 ***\n#&gt; SEXMale            3.32e-02   1.67e-03    19.9   &lt;2e-16 ***\n#&gt; log(LEAD):SEXMale -2.66e-02   2.16e-03   -12.3   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.128 on 30400 degrees of freedom\n#&gt; Multiple R-squared:  0.219,  Adjusted R-squared:  0.219 \n#&gt; F-statistic: 2.13e+03 on 4 and 30400 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#evaluation-metrics",
    "href": "book/linear_regression.html#evaluation-metrics",
    "title": "13  Linear Regression",
    "section": "13.5 Evaluation Metrics",
    "text": "13.5 Evaluation Metrics\nIn addition to adjusted R-squared, there are a few other metrics that can help us to understand how well our model fits the data and to help with model selection. The AIC() and BIC() functions find the Akaike information criterion (AIC) and Bayesian information criterion (BIC) values, respectively. Both AIC and BIC balance the trade-off between model complexity and goodness of fit. AIC takes into account both the goodness of fit (captured by the likelihood of the model) and the complexity of the model (captured by the number of parameters used). Lower AIC values are preferable. BIC is similar to AIC but has a stronger penalty for model complexity compared to AIC. Both measures indicate a preference for keeping the interaction term.\n\nAIC(model_log_log)\n#&gt; [1] -38610\nAIC(model_interaction)\n#&gt; [1] -38760\n\n\nBIC(model_log_log)\n#&gt; [1] -38569\nBIC(model_interaction)\n#&gt; [1] -38710\n\nThe predict() function allows us to calculate the predicted y values. When called on a model with no data specified, it returns the predicted values for the training data. We could also specify new data using the newdata argument. The new data provided must contain the columns given in the model formula. We use the predict() function to find the predicted values from our model and then calculate the mean absolute error (MAE) and mean squared error (MSE) for our model. MAE is less sensitive to outliers compared to MSE. The mean absolute error indicates that our model has fairly high residuals on average. While this model may be helpful for understanding the relationship between blood lead level and systolic blood pressure, it would not be very useful as a tool to predict the latter.\n\npred_y &lt;- predict(model_interaction)\n\n\nmae &lt;- mean(abs(nhanes_df$SBP - pred_y))\nmae\n#&gt; [1] 119\n\n\nmse &lt;- mean((nhanes_df$SBP - pred_y)^2)\nmse\n#&gt; [1] 14502",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#stepwise-selection",
    "href": "book/linear_regression.html#stepwise-selection",
    "title": "13  Linear Regression",
    "section": "13.6 Stepwise Selection",
    "text": "13.6 Stepwise Selection\nSo far we have ignored the other variables in the data frame. When performing variable selection, there are multiple methods to use. We conclude this chapter by demonstrating how to implement one such method, stepwise selection, in R. Chapter 15 expands upon this model selection technique by showing how to implement regularized models in R.\nThe step() function takes in an initial model to perform stepwise selection on along with a direction direction (“forward”, “backward”, or “both”), and a scope scope. The scope specifies the lower and upper model formulas to consider. In the following example, we use forward selection so the lower formula is the formula for our current model and the upper formula contains the other covariates we are considering adding in. These two formulas must be nested - that is, all terms in the lower formula must be contained in the upper formula.\nBy default, the step() function prints each step in the process and uses AIC to guide its decisions. We can set trace=0 to avoid the print behavior and update the argument k to log(n) to use BIC, where n is the number of observations. In the output, we see that the algorithm first adds in race, then BMI, then income, then education, and then smoking status. In fact, all variables were added to the model! The final output is an lm object that we can use just like the ones earlier in this chapter. We get the summary of the final model and see that the adjusted R-squared has improved to 0.2479.\n\nlower_formula &lt;- \"log(SBP) ~ log(LEAD) + AGE + SEX:log(LEAD)\"\nupper_formula &lt;- \"log(SBP) ~ log(LEAD) + AGE + SEX:log(LEAD) + SEX + \n  RACE + EDUCATION + SMOKE + INCOME + BMI_CAT\"\nmod_step &lt;- step(model_interaction, direction = 'forward', \n                 scope = list(lower = lower_formula, \n                              upper = upper_formula))\n#&gt; Start:  AIC=-125048\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + SEX:log(LEAD)\n#&gt; \n#&gt;             Df Sum of Sq RSS     AIC\n#&gt; + RACE       4      9.16 488 -125605\n#&gt; + BMI_CAT    2      8.97 488 -125597\n#&gt; + INCOME     1      2.87 494 -125222\n#&gt; + EDUCATION  2      1.90 495 -125160\n#&gt; + SMOKE      2      0.35 497 -125065\n#&gt; &lt;none&gt;                   497 -125048\n#&gt; \n#&gt; Step:  AIC=-125605\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + RACE + log(LEAD):SEX\n#&gt; \n#&gt;             Df Sum of Sq RSS     AIC\n#&gt; + BMI_CAT    2      7.16 481 -126050\n#&gt; + INCOME     1      1.80 486 -125715\n#&gt; + EDUCATION  2      1.34 487 -125684\n#&gt; + SMOKE      2      0.13 488 -125609\n#&gt; &lt;none&gt;                   488 -125605\n#&gt; \n#&gt; Step:  AIC=-126050\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + RACE + BMI_CAT + log(LEAD):SEX\n#&gt; \n#&gt;             Df Sum of Sq RSS     AIC\n#&gt; + INCOME     1     1.617 479 -126151\n#&gt; + EDUCATION  2     1.112 480 -126117\n#&gt; + SMOKE      2     0.261 481 -126063\n#&gt; &lt;none&gt;                   481 -126050\n#&gt; \n#&gt; Step:  AIC=-126151\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + RACE + BMI_CAT + INCOME + \n#&gt;     log(LEAD):SEX\n#&gt; \n#&gt;             Df Sum of Sq RSS     AIC\n#&gt; + EDUCATION  2     0.418 479 -126173\n#&gt; + SMOKE      2     0.258 479 -126163\n#&gt; &lt;none&gt;                   479 -126151\n#&gt; \n#&gt; Step:  AIC=-126173\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + RACE + BMI_CAT + INCOME + \n#&gt;     EDUCATION + log(LEAD):SEX\n#&gt; \n#&gt;         Df Sum of Sq RSS     AIC\n#&gt; + SMOKE  2     0.286 479 -126187\n#&gt; &lt;none&gt;               479 -126173\n#&gt; \n#&gt; Step:  AIC=-126187\n#&gt; log(SBP) ~ log(LEAD) + AGE + SEX + RACE + BMI_CAT + INCOME + \n#&gt;     EDUCATION + SMOKE + log(LEAD):SEX\n\n\nsummary(mod_step)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(SBP) ~ log(LEAD) + AGE + SEX + RACE + BMI_CAT + \n#&gt;     INCOME + EDUCATION + SMOKE + log(LEAD):SEX, data = nhanes_df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.6713 -0.0799 -0.0039  0.0738  0.6797 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)             4.61e+00   3.32e-03 1391.51  &lt; 2e-16 ***\n#&gt; log(LEAD)               2.28e-02   1.69e-03   13.47  &lt; 2e-16 ***\n#&gt; AGE                     3.48e-03   4.85e-05   71.87  &lt; 2e-16 ***\n#&gt; SEXMale                 3.47e-02   1.65e-03   20.94  &lt; 2e-16 ***\n#&gt; RACEOther Hispanic     -7.11e-03   3.22e-03   -2.20    0.027 *  \n#&gt; RACENon-Hispanic White -4.45e-03   2.20e-03   -2.02    0.043 *  \n#&gt; RACENon-Hispanic Black  3.37e-02   2.47e-03   13.66  &lt; 2e-16 ***\n#&gt; RACEOther Race          6.27e-03   3.39e-03    1.85    0.064 .  \n#&gt; BMI_CAT25&lt;BMI&lt;30        1.51e-02   1.84e-03    8.23  &lt; 2e-16 ***\n#&gt; BMI_CATBMI&gt;=30          3.78e-02   1.83e-03   20.62  &lt; 2e-16 ***\n#&gt; INCOME                 -3.89e-03   5.00e-04   -7.78  7.6e-15 ***\n#&gt; EDUCATIONHS            -1.94e-05   2.19e-03   -0.01    0.993    \n#&gt; EDUCATIONMoreThanHS    -8.69e-03   2.07e-03   -4.20  2.6e-05 ***\n#&gt; SMOKEQuitSmoke         -7.56e-03   1.80e-03   -4.21  2.6e-05 ***\n#&gt; SMOKEStillSmoke        -4.04e-03   1.94e-03   -2.08    0.038 *  \n#&gt; log(LEAD):SEXMale      -2.61e-02   2.12e-03  -12.28  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.126 on 30389 degrees of freedom\n#&gt; Multiple R-squared:  0.248,  Adjusted R-squared:  0.248 \n#&gt; F-statistic:  669 on 15 and 30389 DF,  p-value: &lt;2e-16",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#recap-video",
    "href": "book/linear_regression.html#recap-video",
    "title": "13  Linear Regression",
    "section": "13.7 Recap Video",
    "text": "13.7 Recap Video",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/linear_regression.html#exercises",
    "href": "book/linear_regression.html#exercises",
    "title": "13  Linear Regression",
    "section": "13.8 Exercises",
    "text": "13.8 Exercises\nFor these exercises, we continue using the nhanes_df data.\n\nConstruct a linear model using DBP as the output and LEAD, AGE, and EVER_SMOKE as features, and print the output.\nUse forward stepwise selection to add possible interactions to the linear model from the previous question.\nDraw a QQ plot for the model in Question 2, and describe the distribution that you observe.\nReport the MAE and MSE of the model developed in Question 2. Then, find the row numbers of the observations with the top 5 Cook’s Distance values for this model.\nLook at some diagnostic plots for the model and use what you observe from these plots to choose a transformation that improves the fit of this model. Then, fit and summarize this new model with the transformation included. How do the MSE and MAE of the new model compare to the previous one? Note that your predictions will be on the transformed scale so you’ll need to convert them to the correct scale.\n\n\n\n\n\nHuang, Ziyao. 2022. “Association Between Blood Lead Level with High Blood Pressure in US (NHANES 1999–2018).” Frontiers in Public Health 10: 836357.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#generalized-linear-models-in-r",
    "href": "book/logistic_regression.html#generalized-linear-models-in-r",
    "title": "14  Logistic Regression",
    "section": "14.1 Generalized Linear Models in R",
    "text": "14.1 Generalized Linear Models in R\nThe glm(formula, data, family) function in R is used to fit generalized linear models. The three main arguments we must specify to the function are the\n\nformula - specifies the relationship between the independent variables and the outcome of interest,\n\ndata - the dataset used to train the model, and\n\nfamily - a description of the error distribution and link function to be used in the model.\n\nIn binary logistic regression, we assume a binomial outcome and use the logit link function. We can specify this by setting family = binomial. By default, this assumes the link function is the logit function. Note that we can even use the glm() function to implement linear regression by setting family = gaussian. Using our example from Chapter 13, running glm(SBP ~ LEAD, data = nhanes_df, family = gaussian) would be equivalent to lm(SBP ~ LEAD, data = nhanes_df).\nOur outcome of interest is current e-cigarette use, e_cig_use, so we need to create this variable from the variables that are currently in the data. We set e_cig_use to 0 if the respondent answered that they have not used e-cigarettes in the last 30 days and 1 otherwise. We can see that there are only 1,435 respondents who reported e-cigarette use. This is a low percentage of the overall sample, which will likely impact our results.\n\nnyts$e_cig_use &lt;- as.factor(ifelse(nyts$num_e_cigs == 0, \"0\", \"1\"))\ntable(nyts$e_cig_use)\n#&gt; \n#&gt;     0     1 \n#&gt; 18683  1435\n\nLooking at the covariate of interest, survey setting, we can see that there are 85 respondents that took the survey in “Some other place”. Since we are interested in the impact of taking the survey at school compared to other settings, we simplify this variable to have two levels: “school” and “home/other”.\n\ntable(nyts$location)\n#&gt; \n#&gt;     At home (virtual learning) In a school building/classroom \n#&gt;                           8738                          10737 \n#&gt;               Some other place \n#&gt;                             85\nnyts$location &lt;- ifelse(nyts$location == \n                          \"In a school building/classroom\",\n                        \"school\", \"home/other\")\nnyts$location &lt;- as.factor(nyts$location)\n\nTo start, we create a model to predict e-cigarette use from school setting adjusting for the covariates sex, school level, race, and ethnicity. Note that we specify our formula and data as with the lm() function. We then use the summary() function again to print a summary of this fitted model. The output is slightly different from an lm object. We can see the null and residual deviances are reported along with the AIC. Adding transformations and interactions is equivalent to that in the lm() function and is not demonstrated in this chapter.\n\nmod_start &lt;- glm(e_cig_use ~ grade + sex + race_and_ethnicity + \n                   location, data = nyts, family = binomial)\nsummary(mod_start)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = e_cig_use ~ grade + sex + race_and_ethnicity + \n#&gt;     location, family = binomial, data = nyts)\n#&gt; \n#&gt; Coefficients:\n#&gt;                                           Estimate Std. Error z value\n#&gt; (Intercept)                                -4.6017     0.1539  -29.91\n#&gt; grade7th                                    0.4461     0.1753    2.54\n#&gt; grade8th                                    0.9677     0.1607    6.02\n#&gt; grade9th                                    1.3830     0.1549    8.93\n#&gt; grade10th                                   1.9183     0.1513   12.68\n#&gt; grade11th                                   2.1385     0.1491   14.34\n#&gt; grade12th                                   2.4286     0.1492   16.28\n#&gt; gradeUngraded or Other Grade                2.5213     0.4487    5.62\n#&gt; sexFemale                                   0.1922     0.0580    3.32\n#&gt; race_and_ethnicitynon-Hispanic Black       -0.6614     0.1121   -5.90\n#&gt; race_and_ethnicitynon-Hispanic other race  -0.1021     0.1515   -0.67\n#&gt; race_and_ethnicitynon-Hispanic White        0.1983     0.0739    2.68\n#&gt; locationschool                              0.7223     0.0648   11.14\n#&gt;                                           Pr(&gt;|z|)    \n#&gt; (Intercept)                                &lt; 2e-16 ***\n#&gt; grade7th                                   0.01095 *  \n#&gt; grade8th                                   1.7e-09 ***\n#&gt; grade9th                                   &lt; 2e-16 ***\n#&gt; grade10th                                  &lt; 2e-16 ***\n#&gt; grade11th                                  &lt; 2e-16 ***\n#&gt; grade12th                                  &lt; 2e-16 ***\n#&gt; gradeUngraded or Other Grade               1.9e-08 ***\n#&gt; sexFemale                                  0.00091 ***\n#&gt; race_and_ethnicitynon-Hispanic Black       3.6e-09 ***\n#&gt; race_and_ethnicitynon-Hispanic other race  0.50061    \n#&gt; race_and_ethnicitynon-Hispanic White       0.00726 ** \n#&gt; locationschool                             &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 9754.9  on 18746  degrees of freedom\n#&gt; Residual deviance: 8886.8  on 18734  degrees of freedom\n#&gt;   (1666 observations deleted due to missingness)\n#&gt; AIC: 8913\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\nWe can use the tidy() function from the broom package to display the estimated coefficients from the previous model. This time we add the exponentiate = TRUE argument to exponentiate our coefficients so we can interpret them as estimated change in odds rather than log odds. For example, we can see that those who answered at school have double the estimated odds of reporting e-cigarette use compared to those who took the survey at home/other, adjusting for grade, sex, and race and ethnicity.\n\ntidy(mod_start, exponentiate = TRUE)\n#&gt; # A tibble: 13 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   0.0100     0.154    -29.9  1.68e-196\n#&gt; 2 grade7th      1.56       0.175      2.54 1.10e-  2\n#&gt; 3 grade8th      2.63       0.161      6.02 1.73e-  9\n#&gt; 4 grade9th      3.99       0.155      8.93 4.41e- 19\n#&gt; 5 grade10th     6.81       0.151     12.7  7.94e- 37\n#&gt; # ℹ 8 more rows\n\n\n14.1.1 Practice Question\nFit a logistic regression model with cigarette use as the outcome and age, race_and_ethnicity, LGBT, and family_affluence as well as an interaction between family_affluence and race_and_ethnicity as independent variables. Your AIC should be 2430.8.\n\n# Insert your solution here:",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#residuals-discrimination-and-calibration",
    "href": "book/logistic_regression.html#residuals-discrimination-and-calibration",
    "title": "14  Logistic Regression",
    "section": "14.2 Residuals, Discrimination, and Calibration",
    "text": "14.2 Residuals, Discrimination, and Calibration\nNext, we look at the distribution of the residuals. The resid() function can be used to find the residuals again, but this time we might want to specify the Pearson and deviance residuals by specifying the type argument. We plot histograms for both of these residual types using the following code. In both plots, we can observe a multi-modal distribution, which reflects the binary nature of our outcome.\n\npar(mfrow=c(1,2))\nhist(resid(mod_start, type = \"pearson\"), main = \"Pearson Residuals\")\nhist(resid(mod_start, type = \"deviance\"), main = \"Deviance Residuals\")\n\n\n\n\n\n\n\n\nTo further evaluate the fit of our model, we may want to observe the predicted probabilities. The predict() function by default returns the predicted value on the scale of the linear predictors. In this case, that is the predicted log odds. If we want to find the predicted probabilities, we can update the argument by specifying type=\"response\". Additionally, we can predict on data not used to train the model by using the argument newdata. Note that there are only 18,747 predicted probabilities despite our training data having more observations. This is because the glm() function (and lm() function) drop any observations with NA values when training. In the last chapter, we omitted incomplete cases prior to analysis so that the predicted probabilities corresponded directly to the rows in our data.\n\npred_probs &lt;- predict(mod_start, type = \"response\")\nlength(pred_probs)\n#&gt; [1] 18747\n\nIf we want to find the class for each observation used in fitting the model, we can use the model’s output, which stores the model matrix x and the outcome vector y. We plot the distribution of estimated probabilities for each class. Note that all the predicted probabilities are below 0.5, the typical cut-off for prediction. This is in part due to the fact that we have such an imbalanced outcome.\n\nggplot() + \n  geom_histogram(aes(x = pred_probs, fill = as.factor(mod_start$y)),\n                bins = 30) +\n  scale_fill_discrete(name = \"E-Cig Use\") + \n  labs(x = \"Predicted Probabilities\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n14.2.1 Receiver Operating Characteristic (ROC) Curve\nWe now plot the receiver operating characteristic (ROC) curve and compute the area under the curve (AUC). The roc() function from the pROC package builds a ROC curve. The function has several ways to specify a response and predictor. For example, we can specify the response vector response and predictor vector predictor. By default, with a 0/1 outcome, the roc() function assumes class 0 is controls and class 1 is cases. We can also specify this in the levels argument to specify the value of the response for controls and cases, respectively. Additionally, the function assumes the predictor vector specifies predicted probabilities for the class 1. We can change the argument direction = \"&gt;\" if the opposite is true. We can plot the ROC curve by calling the plot() function. We can add some extra information by adding the AUC (print.auc = TRUE) and the threshold that maximizes sensitivity + specificity (print.thres = TRUE).\n\nroc_mod &lt;- roc(predictor = pred_probs, \n               response = as.factor(mod_start$y), \n               levels = c(0,1), direction = \"&lt;\")\nplot(roc_mod, print.auc = TRUE, print.thres = TRUE)\n\n\n\n\n\n\n\n\nIf we want to understand more about the curve, we can use the coords() function to find the coordinates for each threshold used to create the curve. The argument x= \"all\" specifies we want to find all thresholds, but we could also specify only to return local maxima.\n\nroc_vals &lt;- coords(roc = roc_mod, x = \"all\")\nhead(roc_vals)\n#&gt;   threshold specificity sensitivity\n#&gt; 1      -Inf     0.00000       1.000\n#&gt; 2   0.00569     0.00523       1.000\n#&gt; 3   0.00713     0.01070       1.000\n#&gt; 4   0.00850     0.01547       0.999\n#&gt; 5   0.00934     0.01835       0.998\n#&gt; 6   0.00982     0.02404       0.996\n\nFor example, we could use this information to find the highest threshold with a corresponding sensitivity above 0.75. This returns a threshold of 0.062. If we were to predict class 1 for all observations with a predicted probability above 0.062, then we would achieve a sensitivity of 0.77 and specificity of 0.56 on the training data.\n\nroc_vals[roc_vals$sensitivity &gt; 0.75, ] %&gt;% tail(n = 1)\n#&gt;    threshold specificity sensitivity\n#&gt; 63     0.062       0.555       0.768\n\nWe use the threshold of 0.080 indicated on our ROC curve to create predicted classes for our response. By comparing the result to our outcome using the table() function, we can directly calculate measures like sensitivity, specificity, positive and negative predictive values, and overall accuracy.\n\npred_ys &lt;- ifelse(pred_probs &gt; 0.08, 1, 0)\ntab_outcome &lt;- table(mod_start$y, pred_ys)\ntab_outcome\n#&gt;    pred_ys\n#&gt;         0     1\n#&gt;   0 11992  5395\n#&gt;   1   455   905\n\n\nsens &lt;- tab_outcome[2, 2]/(tab_outcome[2, 1]+tab_outcome[2, 2])\nspec &lt;- tab_outcome[1, 1]/(tab_outcome[1, 1]+tab_outcome[1, 2])\nppv &lt;- tab_outcome[2, 2]/(tab_outcome[1, 2]+tab_outcome[2, 2])\nnpv &lt;- tab_outcome[1, 1]/(tab_outcome[1, 1]+tab_outcome[2, 1])\nacc &lt;- (tab_outcome[1, 1]+tab_outcome[2, 2])/sum(tab_outcome)\n\n\ndata.frame(Measures = c(\"Sens\", \"Spec\", \"PPV\", \"NPV\", \"Acc\"),\n          Values = round(c(sens, spec, ppv, npv, acc),3))\n#&gt;   Measures Values\n#&gt; 1     Sens  0.665\n#&gt; 2     Spec  0.690\n#&gt; 3      PPV  0.144\n#&gt; 4      NPV  0.963\n#&gt; 5      Acc  0.688\n\n\n\n14.2.2 Calibration Plot\nAnother useful plot is a calibration plot. This type of plot groups the data by the estimated probabilities and compares the mean probability with the observed proportion of observations in class 1. It visualizes how close our estimated distribution and true distribution are to each other. There are several packages that can create calibration plots, but we demonstrate how to do this using the ggplot2 package. First, we create a data frame with the predicted probabilities and the outcome variable. Additionally, we group this data into num_cuts groups based on the predicted probabilities using the cut() function. Within each group, we find the model’s predicted mean along with the observed proportion and estimated standard errors.\n\nnum_cuts &lt;- 10\ncalib_data &lt;-  data.frame(prob = pred_probs,\n                          bin = cut(pred_probs, breaks = num_cuts),\n                          class = mod_start$y)\ncalib_data &lt;- calib_data %&gt;% \n             group_by(bin) %&gt;% \n             summarize(observed = sum(class)/n(), \n                       expected = sum(prob)/n(), \n                       se = sqrt(observed * (1-observed) / n()))\ncalib_data\n#&gt; # A tibble: 10 × 4\n#&gt;   bin              observed expected      se\n#&gt;   &lt;fct&gt;               &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (0.00488,0.0322]   0.0212   0.0203 0.00188\n#&gt; 2 (0.0322,0.0592]    0.0440   0.0441 0.00328\n#&gt; 3 (0.0592,0.0862]    0.0621   0.0708 0.00451\n#&gt; 4 (0.0862,0.113]     0.0986   0.0988 0.00587\n#&gt; 5 (0.113,0.14]       0.131    0.123  0.0131 \n#&gt; # ℹ 5 more rows\n\nNext, we plot the observed vs expected proportions. We also used the estimated standard error to create corresponding 95% confidence intervals. The red line indicates a perfect fit where our estimated and true distributions match. Overall, the plot shows that our model could be better calibrated.\n\nggplot(calib_data) + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  geom_errorbar(aes(x = expected, ymin = observed - 1.96 * se, \n                    ymax = observed + 1.96 * se), \n                colour=\"black\", width=.01)+\n  geom_point(aes(x = expected, y = observed)) +\n  labs(x = \"Expected Proportion\", y = \"Observed Proportion\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.2.3 Practice Question\nCreate a calibration plot with 5 cuts for your model from the previous practice question (recall that this model should have cigarette use as the outcome and age, race_and_ethnicity, LGBT, and family_affluence as well as an interaction between family_affluence and race_and_ethnicity as independent variables). It should look like Figure 14.1.\n\n\n\n\n\n\nFigure 14.1: Calibration Plot.\n\n\n\n\n# Insert your solution here:",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#variable-selection-and-likelihood-ratio-tests",
    "href": "book/logistic_regression.html#variable-selection-and-likelihood-ratio-tests",
    "title": "14  Logistic Regression",
    "section": "14.3 Variable Selection and Likelihood Ratio Tests",
    "text": "14.3 Variable Selection and Likelihood Ratio Tests\nIn the last chapter, we introduced the step() function to implement stepwise variable selection. This function also works with glm objects. In this case, we use this function to implement backward selection from a larger set of covariates. We first remove any observations with NA values to ensure that our training data does not change size as the formula changes.\n\nnyts_sub &lt;- nyts %&gt;% \n  dplyr::select(location, sex, grade, otherlang, grades_in_past_year, \n                perceived_e_cig_use, race_and_ethnicity, LGBT, \n                psych_distress, family_affluence, e_cig_use) %&gt;%\n  na.omit()\nhead(nyts_sub)\n#&gt; # A tibble: 6 × 11\n#&gt;   location sex   grade otherlang grades_in_past_year perceived_e_cig_use\n#&gt;   &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;                             &lt;dbl&gt;\n#&gt; 1 school   Male  6th   No        Mostly A's                            0\n#&gt; 2 school   Fema… 6th   No        Mostly A's                            0\n#&gt; 3 school   Fema… 6th   No        Mostly C's                            0\n#&gt; 4 school   Fema… 6th   No        Mostly A's                            0\n#&gt; 5 school   Fema… 6th   No        Mostly B's                            0\n#&gt; 6 school   Male  6th   No        Not Sure                              0\n#&gt; # ℹ 5 more variables: race_and_ethnicity &lt;chr&gt;, LGBT &lt;chr&gt;,\n#&gt; #   psych_distress &lt;chr&gt;, family_affluence &lt;chr&gt;, e_cig_use &lt;fct&gt;\n\nTo implement backward selection, we first create a model with all the covariates included. The period . in the formula indicates that we want to include all variables. Next, we use the step() function. Since we are using backward selection, we only need to specify the lower formula in the scope.\n\nmodel_full &lt;- glm(e_cig_use ~ ., data = nyts_sub, family = binomial)\nmod_step &lt;- step(model_full, direction = 'backward', \n                 scope = list(lower = \"e_cig_use ~ sex + grade + \n                 race_and_ethnicity + location\"))\n#&gt; Start:  AIC=6093\n#&gt; e_cig_use ~ location + sex + grade + otherlang + grades_in_past_year + \n#&gt;     perceived_e_cig_use + race_and_ethnicity + LGBT + psych_distress + \n#&gt;     family_affluence\n#&gt; \n#&gt;                       Df Deviance  AIC\n#&gt; - family_affluence     2     6038 6090\n#&gt; &lt;none&gt;                       6037 6093\n#&gt; - otherlang            1     6042 6096\n#&gt; - LGBT                 2     6051 6103\n#&gt; - psych_distress       3     6106 6156\n#&gt; - grades_in_past_year  6     6126 6170\n#&gt; - perceived_e_cig_use  1     6416 6470\n#&gt; \n#&gt; Step:  AIC=6090\n#&gt; e_cig_use ~ location + sex + grade + otherlang + grades_in_past_year + \n#&gt;     perceived_e_cig_use + race_and_ethnicity + LGBT + psych_distress\n#&gt; \n#&gt;                       Df Deviance  AIC\n#&gt; &lt;none&gt;                       6038 6090\n#&gt; - otherlang            1     6043 6093\n#&gt; - LGBT                 2     6052 6100\n#&gt; - psych_distress       3     6106 6152\n#&gt; - grades_in_past_year  6     6128 6168\n#&gt; - perceived_e_cig_use  1     6418 6468\n\nStepwise selection keeps most variables in the model and only drops family affluence. In the following output, we can see the AUC for this model has improved to 0.818.\n\nroc_mod_step &lt;- roc(predictor = predict(mod_step, type = \"response\"), \n                    response = as.factor(mod_step$y), \n                    levels = c(0, 1), direction = \"&lt;\")\nplot(roc_mod_step, print.auc = TRUE, print.thres = TRUE)\n\n\n\n\n\n\n\n\nIf we want to compare this model to our previous one, we could use a likelihood ratio test since the two models are nested. The lrtest() function from the lmtest package allows us to input two nested glm models and performs a corresponding Chi-squared likelihood ratio test. First, we need to ensure that our initial model is fit on the same data used in the stepwise selection. The output indicates a statistically significant improvement in the model likelihood with the inclusion of the other variables.\n\nmod_start2 &lt;- glm(e_cig_use ~ grade + sex + race_and_ethnicity + \n                    location, data = nyts_sub, family = binomial)\n\n\nprint(lrtest(mod_start2, mod_step))\n#&gt; Likelihood ratio test\n#&gt; \n#&gt; Model 1: e_cig_use ~ grade + sex + race_and_ethnicity + location\n#&gt; Model 2: e_cig_use ~ location + sex + grade + otherlang + grades_in_past_year + \n#&gt;     perceived_e_cig_use + race_and_ethnicity + LGBT + psych_distress\n#&gt;   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n#&gt; 1  13  -3369                        \n#&gt; 2  26  -3019 13   701     &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#extending-beyond-binary-outcomes",
    "href": "book/logistic_regression.html#extending-beyond-binary-outcomes",
    "title": "14  Logistic Regression",
    "section": "14.4 Extending Beyond Binary Outcomes",
    "text": "14.4 Extending Beyond Binary Outcomes\nThe glm() function can be used to fit models for other possible families and non-binary outcomes. For example, we can fit models where the outcome might follow a Poisson distribution or negative binomial distribution by updating the family argument. In the following code, we fit a Poisson model to model the number of e-cigarettes used in the last 30 days by setting family = poisson. However, despite our outcome being a count value, this model does not appear to be a good fit for our data.\n\nmod_poisson &lt;- glm(num_e_cigs ~ grade + sex + race_and_ethnicity + \n                     location, data = nyts, family = poisson)\n\n\npar(mfrow=c(1,2))\nhist(predict(mod_poisson, type = \"response\"), main = \"Model\", \n     xlab = \"Predicted Values\")\nhist(nyts$num_e_cigs, main = \"Observed\", xlab = \"Number E-Cigs\")",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#recap-video",
    "href": "book/logistic_regression.html#recap-video",
    "title": "14  Logistic Regression",
    "section": "14.5 Recap Video",
    "text": "14.5 Recap Video",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/logistic_regression.html#exercises",
    "href": "book/logistic_regression.html#exercises",
    "title": "14  Logistic Regression",
    "section": "14.6 Exercises",
    "text": "14.6 Exercises\n\nCreate a new variable tobacco_use representing any tobacco use in the past 30 days (including e-cigarettes, cigarettes, and/or cigars) as well as a new variable perceived_tobacco_use equal to the maximum of the perceived cigarette and e-cig use. Then, create a new data frame nyts_sub that contains these two new columns as well as columns for sex, grades in the past year, psych distress, and family affluence. Finally, fit a logistic regression model with this new tobacco use variable as the outcome and all other selected variables as independent variables.\nPerform stepwise selection on your model from Question 1 with direction = \"both\", setting the upper scope of the model selection procedure to be a model including all two-way interactions and the lower scope to be a model including only an intercept. To specify all possible interactions you can use the formula \"tobacco_use ~ .^2\". Use the tidy() function to display the exponentiated estimated coefficients for the resulting model along with a confidence interval.\nAccording to your model from Question 2, what is the estimated probability of tobacco use for a girl with mostly C’s, moderate psych distress, and a perceived tobacco use of 0.5? Use the predict() function to answer this question.\nConstruct a ROC curve for the model from Question 2 and find the AUC as well as the threshold that maximizes sensitivity and specificity.\n\n\n\n\n\nCenters for Disease Control and Prevention (CDC). 2021. “National Youth Tobacco Survey (NYTS).” U.S. Department of Health; Human Services. https://www.cdc.gov/tobacco/data_statistics/surveys/nyts/index.htm.\n\n\nHothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint Cummins. 2022. lmtest: Testing Linear Regression Models. https://CRAN.R-project.org/package=lmtest.\n\n\nPark-Lee, Eunice, Andrea S Gentzke, Chunfeng Ren, Maria Cooper, Michael D Sawdey, S Sean Hu, and Karen A Cullen. 2023. “Impact of Survey Setting on Current Tobacco Product Use: National Youth Tobacco Survey, 2021.” Journal of Adolescent Health 72 (3): 365–74.\n\n\nRobin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez, and Markus Müller. 2023. pROC: Display and Analyze ROC Curves. http://expasy.org/tools/pROC/.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/model_selection.html#regularized-regression",
    "href": "book/model_selection.html#regularized-regression",
    "title": "15  Model Selection",
    "section": "15.1 Regularized Regression",
    "text": "15.1 Regularized Regression\nSuppose we have a numeric data matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) and outcome vector \\(y \\in \\mathbb{R}^n\\). We let \\(x_i\\) denote the vector representing the \\(i\\)th row of \\(X\\). This corresponds to the \\(i\\)th observation. When we refer to regularized regression, we are referring to solving the following optimization problem that minimizes the average loss plus a penalty term.\n\\[\n\\min_{ (\\beta_0, \\beta) \\in \\mathbb{R}^{p+1}} \\frac{1}{n} \\sum_{i=1}^n l(y_i, \\beta_0 + \\beta^T x_i) + \\text{Pen}(\\beta)\n\\tag{15.1}\\]\nThe function \\(l(y_i, \\beta_0 + \\beta^T x_i)\\) represents the loss function. For linear regression, this corresponds to the squared error \\((y_i - \\beta_0 - \\beta^T x_i)^2\\). For logistic regression, this loss corresponds to the logistic loss function.\nThe penalty terms we implement include the following:\n\nL0 Norm: \\(||\\beta ||_0 = \\sum_{j=1}^p 1(\\beta_j \\neq 0)\\), the number of non-zero coefficients,\nL1 Norm: \\(||\\beta ||_1 = \\sum_{j=1}^p |\\beta_j|\\), the sum of absolute values of the coefficients, and\nSquared L2 Norm: \\(||\\beta ||_2^2 = \\sum_{j=1}^p \\beta_j^2\\), the sum of squared coefficients.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "book/model_selection.html#elastic-net",
    "href": "book/model_selection.html#elastic-net",
    "title": "15  Model Selection",
    "section": "15.2 Elastic Net",
    "text": "15.2 Elastic Net\nWe first consider L1 and L2 regularization. In particular, consider the following penalty term, referred to as elastic net regularization,\n\\[\n\\lambda \\left[ \\alpha ||\\beta||_1 + (1-\\alpha) ||\\beta||^2_2 \\right],\n\\]\nwhere \\(\\lambda\\) is a complexity parameter and \\(\\alpha\\) controls the balance between the two norms. A model with only L1 regularization (\\(\\alpha = 1\\)) corresponds to lasso regression while a model with only L2 regularization (\\(\\alpha=0\\)) corresponds to ridge regression. Note that the penalty depends on the scale of \\(X\\) and we typically assume each column has been standardized.\nThe glmnet package implements elastic net regularization. It assumes our data are in the form described previously. Therefore, we first create our numeric data matrix x and output vector y. Some of our variables are categorical, so in order to create a numeric matrix we need to one-hot encode them. We can do so using the model.matrix() function which takes in a formula and a data frame and creates the corresponding design matrix including creating dummy variables from factor variables and implementing any transformations. Note that we drop the first generated column which corresponds to the intercept. The transformation to our outcome does not impact the result.\n\nx &lt;- model.matrix(log(SBP) ~ ., nhanes)[, -1]\nhead(x)\n#&gt;   AGE SEXFemale RACEOther Hispanic RACENon-Hispanic White\n#&gt; 1  77         0                  0                      1\n#&gt; 2  49         0                  0                      1\n#&gt; 3  37         0                  0                      1\n#&gt; 4  70         0                  0                      0\n#&gt; 5  81         0                  0                      1\n#&gt; 6  38         1                  0                      1\n#&gt;   RACENon-Hispanic Black RACEOther Race EDUCATIONHS EDUCATIONMoreThanHS\n#&gt; 1                      0              0           0                   1\n#&gt; 2                      0              0           0                   1\n#&gt; 3                      0              0           0                   1\n#&gt; 4                      0              0           0                   0\n#&gt; 5                      0              0           0                   0\n#&gt; 6                      0              0           0                   1\n#&gt;   INCOME SMOKEQuitSmoke SMOKEStillSmoke  LEAD BMI_CAT25&lt;BMI&lt;30\n#&gt; 1   5.00              0               0 1.609                0\n#&gt; 2   5.00              1               0 0.470                1\n#&gt; 3   4.93              0               0 0.875                0\n#&gt; 4   1.07              1               0 0.470                1\n#&gt; 5   2.67              0               1 1.705                1\n#&gt; 6   4.52              0               1 0.405                1\n#&gt;   BMI_CATBMI&gt;=30 ALCYes\n#&gt; 1              0      1\n#&gt; 2              0      1\n#&gt; 3              1      1\n#&gt; 4              0      1\n#&gt; 5              0      1\n#&gt; 6              0      1\n\nOur outcome vector corresponds to log transformed systolic blood pressure.\n\ny &lt;- log(nhanes$SBP)\n\nThe glmnet() function fits an elastic net regression model. This requires us to specify our input matrix x and response variable y. Additionally, we can specify the assumed distribution for y using the family argument. In our subsequent example, we fit this model with \\(\\alpha = 1\\) and 25 different values of \\(\\lambda\\). By default, glmnet() sets \\(\\alpha\\) to 1 and creates a grid of 100 different values of lambda. It is also the default to standardize x, which we can turn off by specifying standardize = FALSE in our function call.\n\nmod_lasso &lt;- glmnet(x, y, family = \"gaussian\", alpha = 1,\n                    nlambda = 25)\n\nIf we plot the resulting object, we can see the model coefficients for each resulting model by plotting how the coefficient for each variable changes with the value of \\(\\lambda\\). The plot() function by default plots these against the penalty term but we can also specify to plot against the \\(\\lambda\\) values on the log scale. The label argument adds a label to each line, though these are often hard to read. The numbers at the top of the plot indicate how many non-zero coefficients were included in the model for different \\(\\lambda\\) values. Read the documentation ?glmnet to see the other possible inputs including the penalty.factor and weights arguments.\n\nplot(mod_lasso, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nWe can also print our results. This prints a matrix with the values of \\(\\lambda\\) used. For each \\(\\lambda\\) value we can also see the number of nonzero coefficients (Df) and the percent deviance explained (%dev).\n\nprint(mod_lasso)\n#&gt; \n#&gt; Call:  glmnet(x = x, y = y, family = \"gaussian\", alpha = 1, nlambda = 25) \n#&gt; \n#&gt;    Df %Dev Lambda\n#&gt; 1   0  0.0 0.0674\n#&gt; 2   1 11.6 0.0459\n#&gt; 3   1 17.0 0.0313\n#&gt; 4   1 19.5 0.0213\n#&gt; 5   4 21.0 0.0145\n#&gt; 6   5 23.1 0.0099\n#&gt; 7   7 24.3 0.0067\n#&gt; 8   8 24.9 0.0046\n#&gt; 9   9 25.2 0.0031\n#&gt; 10  9 25.4 0.0021\n#&gt; 11 12 25.6 0.0014\n#&gt; 12 13 25.6 0.0010\n#&gt; 13 14 25.7 0.0007\n#&gt; 14 14 25.7 0.0005\n#&gt; 15 14 25.7 0.0003\n#&gt; 16 14 25.7 0.0002\n#&gt; 17 14 25.7 0.0001\n#&gt; 18 14 25.7 0.0001\n#&gt; 19 15 25.7 0.0001\n#&gt; 20 15 25.7 0.0000\n\nWe can extract the model for a particular value of \\(\\lambda\\) using the coef() function. The argument s specifies the value of \\(\\lambda\\). For the particular value of \\(\\lambda\\) chosen in the following code, only age has a non-zero coefficient. Note that the coef() function returns the coefficients on the original scale.\n\ncoef(mod_lasso, s = 0.045920)\n#&gt; 16 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                             s1\n#&gt; (Intercept)            4.75241\n#&gt; AGE                    0.00121\n#&gt; SEXFemale              .      \n#&gt; RACEOther Hispanic     .      \n#&gt; RACENon-Hispanic White .      \n#&gt; RACENon-Hispanic Black .      \n#&gt; RACEOther Race         .      \n#&gt; EDUCATIONHS            .      \n#&gt; EDUCATIONMoreThanHS    .      \n#&gt; INCOME                 .      \n#&gt; SMOKEQuitSmoke         .      \n#&gt; SMOKEStillSmoke        .      \n#&gt; LEAD                   .      \n#&gt; BMI_CAT25&lt;BMI&lt;30       .      \n#&gt; BMI_CATBMI&gt;=30         .      \n#&gt; ALCYes                 .\n\nWe can also use the predict() function to predict blood pressure for this particular model. In the function call, we have specified our value of \\(\\lambda\\) as well as our data matrix x as the data to predict on.\n\npred_lasso &lt;- predict(mod_lasso, s = 0.045920, newx = x)\n\nThis shows our observed model fit for a fairly high penalty term. In order to choose the best value of \\(\\lambda\\), we use 5-fold cross-validation. First, we randomly assign each observation to one of five folds using the sample() function. We can see that this splits the data into folds of roughly equal size.\n\nset.seed(1)\nfoldid &lt;- sample(1:5, size = nrow(x), replace = TRUE)\ntable(foldid)\n#&gt; foldid\n#&gt;    1    2    3    4    5 \n#&gt; 6081 5967 6048 6188 6121\n\nNext, we call the cv.glmnet() function which implements k-fold cross-validation across a grid of \\(\\lambda\\) values. Similar to before, we specified x, y, and alpha = 1. This time, we also include the measure to use for cross-validation (mse indicates mean squared error) and provide the fold vector foldid. If you do not want to provide folds, you can instead use the nfolds argument to specify the number of folds and the function creates them. Plotting the returned object shows us the estimated mean squared error for different values of \\(\\lambda\\) as well as error bars for each estimate. Similar to before it also shows the number of non-zero coefficients at the top.\n\ncv_lasso &lt;- cv.glmnet(x, y, foldid = foldid, alpha = 1,\n                      type.measure = \"mse\")\nplot(cv_lasso)\n\n\n\n\n\n\n\n\nThere are two vertical dashed lines included in the plot. These correspond to two values of \\(\\lambda\\) that are stored in our object. The first is lambda.min. This corresponds to the \\(\\lambda\\) value with the lowest estimated mean squared error. The other is lambda.1se. This corresponds to the largest \\(\\lambda\\) value whose estimated mean squared error is within one standard error of the lowest value. As indicated in the plot, this corresponds to a model with fewer included coefficients and higher regularization.\nWe use the lambda.min value as our chosen \\(\\lambda\\) value. To extract the coefficients corresponding to this \\(\\lambda\\) value we again use the coef() function. However, this \\(\\lambda\\) might not be one of the initial 25 used for our mod_lasso object. In this case, the coef() function uses linear interpolation to get the estimated coefficients. If we want to refit our model for this particular value of \\(\\lambda\\) we can instead specify the argument exact = TRUE and provide x and y.\n\nlasso_coef &lt;- coef(mod_lasso, s = cv_lasso$lambda.min,\n                   exact = TRUE, x = x, y = y)\nlasso_coef\n#&gt; 16 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                              s1\n#&gt; (Intercept)             4.63458\n#&gt; AGE                     0.00367\n#&gt; SEXFemale              -0.02652\n#&gt; RACEOther Hispanic     -0.00677\n#&gt; RACENon-Hispanic White -0.00531\n#&gt; RACENon-Hispanic Black  0.03257\n#&gt; RACEOther Race          0.00493\n#&gt; EDUCATIONHS             .      \n#&gt; EDUCATIONMoreThanHS    -0.00942\n#&gt; INCOME                 -0.00396\n#&gt; SMOKEQuitSmoke         -0.00753\n#&gt; SMOKEStillSmoke        -0.00397\n#&gt; LEAD                    0.00880\n#&gt; BMI_CAT25&lt;BMI&lt;30        0.01452\n#&gt; BMI_CATBMI&gt;=30          0.03688\n#&gt; ALCYes                  0.00547\n\nWe now repeat the same process for a model with \\(\\alpha = 0\\) and \\(\\alpha = 0.5\\). In this case, we call the glmnet() function with our chosen \\(\\lambda\\) value to find the coefficients. This is equivalent to what we did for our lasso model. Last, we create a data frame with the estimated coefficients for each model. The coef() function returns a matrix so this requires converting these to numeric vectors.\n\n# cross validation using same folds\ncv_ridge &lt;- cv.glmnet(x, y, foldid = foldid, alpha = 0,\n                      type.measure = \"mse\")\ncv_elastic &lt;- cv.glmnet(x, y, foldid = foldid, alpha = 0.5,\n                      type.measure = \"mse\")\n\n# Refit model on full data with chosen lambda\nmod_ridge &lt;- glmnet(x, y, alpha = 0, lambda = cv_ridge$lambda.min)\nmod_elastic &lt;- glmnet(x, y, alpha = 0.5, lambda = cv_ridge$lambda.min)\n\n# extract coefficients for a table\nres_coef &lt;- data.frame(name = c(\"Intercept\", colnames(x)),\n                       lasso = round(as.numeric(lasso_coef), 3),\n                       ridge = round(as.numeric(coef(mod_ridge)), 3),\n                       elastic = round(as.numeric(coef(mod_elastic)), \n                                       3))\nres_coef\n#&gt;                      name  lasso  ridge elastic\n#&gt; 1               Intercept  4.635  4.646   4.650\n#&gt; 2                     AGE  0.004  0.003   0.003\n#&gt; 3               SEXFemale -0.027 -0.025  -0.020\n#&gt; 4      RACEOther Hispanic -0.007 -0.007   0.000\n#&gt; 5  RACENon-Hispanic White -0.005 -0.005  -0.002\n#&gt; 6  RACENon-Hispanic Black  0.033  0.031   0.027\n#&gt; 7          RACEOther Race  0.005  0.004   0.000\n#&gt; 8             EDUCATIONHS  0.000  0.000   0.000\n#&gt; 9     EDUCATIONMoreThanHS -0.009 -0.010  -0.006\n#&gt; 10                 INCOME -0.004 -0.004  -0.002\n#&gt; 11         SMOKEQuitSmoke -0.008 -0.006   0.000\n#&gt; 12        SMOKEStillSmoke -0.004 -0.005   0.000\n#&gt; 13                   LEAD  0.009  0.011   0.008\n#&gt; 14       BMI_CAT25&lt;BMI&lt;30  0.015  0.014   0.000\n#&gt; 15         BMI_CATBMI&gt;=30  0.037  0.035   0.022\n#&gt; 16                 ALCYes  0.005  0.004   0.000\n\nThe coefficients between the models are not so different. We can also compare their mean squared errors, which are also similar. Since our lasso model was fit on a grid of \\(\\lambda\\) values, we again have to specify this value.\n\nmean((nhanes$SBP - exp(predict(mod_lasso, newx = x,\n                               s = cv_lasso$lambda.min)))^2)\n#&gt; [1] 268\nmean((nhanes$SBP - exp(predict(mod_ridge, newx = x)))^2)\n#&gt; [1] 268\nmean((nhanes$SBP - exp(predict(mod_elastic, newx = x)))^2)\n#&gt; [1] 270",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "book/model_selection.html#best-subset",
    "href": "book/model_selection.html#best-subset",
    "title": "15  Model Selection",
    "section": "15.3 Best Subset",
    "text": "15.3 Best Subset\nThe second package we introduce in this chapter is one that allows us to fit models with an L0 penalty term. The package L0Learn considers penalties of the following forms.\n\nL0 only: \\(\\lambda ||\\beta||_0\\)\nL0L1: \\(\\lambda ||\\beta ||_0 + \\gamma ||\\beta||_1\\)\nL0L2: \\(\\lambda ||\\beta||_0 + \\gamma ||\\beta ||_2^2\\)\n\nTo fit a model with an L0 penalty term, we use the L0Learn.fit() function. Similar to glmnet(), we need to specify our input matrix x and response vector y as well as our penalty using the penalty argument. We can also specify a number of \\(\\lambda\\) values to consider nLambda and specify the family through the loss function loss.\n\nmod_l0 &lt;- L0Learn.fit(x, y, penalty = \"L0\", loss = \"SquaredError\",\n                      nLambda = 20)\n\nPlotting the returned object also shows how the coefficients for each variable change with the penalty term, given by the support size or number of non-zero coefficients in this case. We can also print the returned object to see the different values of \\(\\lambda\\) used and the corresponding number of included variables.\n\nplot(mod_l0)\n\n\n\n\n\n\n\n\n\nprint(mod_l0)\n#&gt;      lambda gamma suppSize\n#&gt; 1  1.08e-01     0        0\n#&gt; 2  1.07e-01     0        1\n#&gt; 3  6.51e-03     0        2\n#&gt; 4  5.00e-03     0        3\n#&gt; 5  4.22e-03     0        4\n#&gt; 6  1.75e-03     0        5\n#&gt; 7  5.36e-04     0        7\n#&gt; 8  3.23e-04     0        8\n#&gt; 9  1.92e-04     0        9\n#&gt; 10 1.42e-04     0       10\n#&gt; 11 1.04e-04     0       11\n#&gt; 12 6.80e-05     0       12\n#&gt; 13 1.75e-05     0       14\n#&gt; 14 4.06e-07     0       15\n#&gt; 15 3.94e-07     0       15\n#&gt; 16 3.03e-08     0       15\n#&gt; 17 5.87e-09     0       15\n#&gt; 18 9.43e-10     0       15\n#&gt; 19 3.48e-10     0       15\n#&gt; 20 2.19e-10     0       15\n\nTo extract the model for a particular value, we can use the coef() function and specify the \\(\\lambda\\) and \\(\\gamma\\) value to use.\n\ncoef_l0 &lt;- coef(mod_l0, lambda = 1.75475e-03, gamma = 0)\n\nUnfortunately, this output doesn’t include variable names so we add them manually.\n\nrownames(coef_l0) &lt;- c(\"Intercept\", colnames(x))\ncoef_l0\n#&gt; 16 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                                \n#&gt; Intercept               4.63847\n#&gt; AGE                     0.00379\n#&gt; SEXFemale              -0.03130\n#&gt; RACEOther Hispanic      .      \n#&gt; RACENon-Hispanic White  .      \n#&gt; RACENon-Hispanic Black  0.03753\n#&gt; RACEOther Race          .      \n#&gt; EDUCATIONHS             .      \n#&gt; EDUCATIONMoreThanHS     .      \n#&gt; INCOME                 -0.00535\n#&gt; SMOKEQuitSmoke          .      \n#&gt; SMOKEStillSmoke         .      \n#&gt; LEAD                    .      \n#&gt; BMI_CAT25&lt;BMI&lt;30        .      \n#&gt; BMI_CATBMI&gt;=30          0.02760\n#&gt; ALCYes                  .\n\nIf instead we want to include a penalty with an L0 and L2 norm term, we can change our penalty argument to penalty = L0L2 and specify a number of \\(\\gamma\\) values to test.\n\nmod_l0l2 &lt;- L0Learn.fit(x, y, penalty = \"L0L2\", \n                        loss = \"SquaredError\",\n                        nLambda = 20, nGamma = 10)\n\nThe L0Learn package also includes a function to use cross-validation to choose these parameters. Unfortunately, it does not include an option to specify your own folds. Instead, we use the nFolds argument to specify to run 5-fold cross-validation. This function also allows us to specify a number of \\(\\lambda\\) and \\(\\gamma\\) values, or we can use the default number. Plotting the result shows the estimated mean squared error with error bars for each model and the corresponding support size.\n\ncv_l0l2 = L0Learn.cvfit(x, y, loss = \"SquaredError\",\n                        nFolds = 5, penalty = \"L0L2\")\nplot(cv_l0l2)\n\n\n\n\n\n\n\n\nThe returned results are stored in cvMeans. This is a list of matrices - one for each value of \\(\\gamma\\). To extract these into a more manageable form, we use the sapply() function to convert each matrix to a numeric vector and create a matrix of results. The columns of this matrix correspond to the 10 \\(\\gamma\\) values used and each column of this matrix corresponds to 100 \\(\\lambda\\) values chosen for that particular value of \\(\\gamma\\). We use the which() function to find which one has the lowest estimated mean squared error.\n\ncv_res &lt;- sapply(cv_l0l2$cvMeans, as.numeric)\nmin_ind &lt;- which(cv_res == min(cv_res), arr.ind = TRUE)\nmin_ind\n#&gt;      row col\n#&gt; [1,]  11  10\n\nWe can then extract out the corresponding \\(\\gamma\\) and \\(\\lambda\\) values through the fit object returned in our result.\n\ngamma_min &lt;- cv_l0l2$fit$gamma[[min_ind[2]]]\nlambda_min &lt;- cv_l0l2$fit$lambda[[min_ind[2]]][min_ind[1]]\n\nLast, we find the estimated coefficients for this model using the coef() function on the cross-validation object cv_l0l2.\n\ncv_coef_l0 &lt;- coef(cv_l0l2, gamma = gamma_min, lambda = lambda_min)\nrownames(cv_coef_l0) &lt;- c(\"Intercept\", colnames(x))\ncv_coef_l0\n#&gt; 16 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                               \n#&gt; Intercept              4.61671\n#&gt; AGE                    0.00382\n#&gt; SEXFemale              .      \n#&gt; RACEOther Hispanic     .      \n#&gt; RACENon-Hispanic White .      \n#&gt; RACENon-Hispanic Black 0.04216\n#&gt; RACEOther Race         .      \n#&gt; EDUCATIONHS            .      \n#&gt; EDUCATIONMoreThanHS    .      \n#&gt; INCOME                 .      \n#&gt; SMOKEQuitSmoke         .      \n#&gt; SMOKEStillSmoke        .      \n#&gt; LEAD                   .      \n#&gt; BMI_CAT25&lt;BMI&lt;30       .      \n#&gt; BMI_CATBMI&gt;=30         .      \n#&gt; ALCYes                 .\n\nLast, we use the predict() function on cv_l0l2 to evaluate our resulting model. To do so, we need to specify \\(\\lambda\\) and \\(\\gamma\\) as well as our data x. This model is much sparser than the ones in the previous section but our mean squared error on our training data is a little higher.\n\npred_l0l2 &lt;- predict(cv_l0l2, gamma = gamma_min, \n                     lambda = lambda_min, newx = x)\nmean((nhanes$SBP - exp(pred_l0l2))^2)\n#&gt; [1] 275",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "book/model_selection.html#exercises",
    "href": "book/model_selection.html#exercises",
    "title": "15  Model Selection",
    "section": "15.4 Exercises",
    "text": "15.4 Exercises\nFor these exercises, we use the nyts data from Chapter 14. Our outcome of interest is whether or not someone uses any tobacco product.\n\nCreate a new variable tobacco_use representing any tobacco use in the past 30 days (including e-cigarettes, cigarettes, and/or cigars). Then, create a new data frame nyts_sub that contains this new column as well as columns for location, age, sex, race, whether someone identifies with the LGBT community, grades in the past year, perceived_cigarette use, perceived e-cigarette use, psych distress, and family affluence.\nCreate an outcome vector y corresponding to the column tobacco_use and a model matrix x containing all other covariates.\nFit a L1 (lasso), L2 (ridge), and L0 (best subset) penalized regression model using 5-fold cross-validation. Create a data frame with the corresponding coefficients for all models. Be sure to update the loss function to reflect our binary outcome.\nReport the AUC and accuracy of these three models on the training data.\n\n\n\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1–22. https://doi.org/10.18637/jss.v033.i01.\n\n\nHazimeh, Hussein, Rahul Mazumder, and Tim Nonet. 2023. L0Learn: Fast Algorithms for Best Subset Selection. https://CRAN.R-project.org/package=L0Learn.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "book/cs_regression.html#model-selection",
    "href": "book/cs_regression.html#model-selection",
    "title": "16  Case Study: Regression",
    "section": "16.1 Model Selection",
    "text": "16.1 Model Selection\nOur goal is to predict TB diagnosis. We compare two risk models: a logistic regression model and a lasso logistic regression model. For both of these models, we fit our model on the training data. For the lasso model, we use 5-fold cross-validation to choose the penalty parameter. In the following code, we create a table with the estimated exponentiated coefficients.\n\n# fit logistic model \nmod_logistic &lt;- glm(tb ~ ., data = tb_train, family = binomial)\n\n# fit lasso model with CV\nX_train &lt;- model.matrix(tb~., data = tb_train)[, -1]\ny_train &lt;- tb_train[,1]\nmod_lasso_cv &lt;- cv.glmnet(X_train, y_train, alpha = 1, \n                       family = \"binomial\", nfolds = 5)\n\n# refit for given lambda\nmod_lasso &lt;- glmnet(X_train, y_train, alpha = 1, family = \"binomial\",\n                    lambda = mod_lasso_cv$lambda.min)\n\n# create data frame\ncoef_df &lt;- data.frame(Logistic = signif(exp(coef(mod_logistic)), 3),\n                      Lasso = \n                        signif(exp(as.numeric(coef(mod_lasso))), 3))\ncoef_df\n#&gt;                  Logistic   Lasso\n#&gt; (Intercept)        0.0531  0.0823\n#&gt; age_group[15,25)   1.5000  1.2000\n#&gt; age_group[25,35)   2.0400  1.6400\n#&gt; age_group[35,45)   2.0700  1.6900\n#&gt; age_group[45,55)   1.1200  1.0000\n#&gt; hiv_pos1           2.6300  2.5000\n#&gt; diabetes1          1.4500  1.1600\n#&gt; ever_smoke1        0.8770  1.0000\n#&gt; past_tb1           1.0500  1.0000\n#&gt; male1              2.5100  2.1700\n#&gt; hs_less1           1.2500  1.1100\n#&gt; two_weeks_symp1    2.5000  2.3300\n#&gt; num_symptoms2      2.1100  1.9200\n#&gt; num_symptoms3      6.6500  5.8800\n#&gt; num_symptoms4     11.9000 10.0000\n\nAfter fitting both models, we evaluate model performance on the withheld test set using an ROC curve. The ROC curve shows similar discrimination for both models. Therefore, we choose the lasso model for its potential sparsity and parsimony.\n\npar(mfrow = c(1,2))\n\n# logistic regression model ROC\npred_test_logistic &lt;- predict(mod_logistic, tb_test, \n                              type = \"response\")\nroc_test_logistic &lt;- roc(predictor = pred_test_logistic,\n                     response = tb_test$tb,\n                     levels = c(0,1), direction = \"&lt;\")\n\nplot(roc_test_logistic, print.auc = TRUE)\n\n\n# lasso model ROC\nX_test &lt;- model.matrix(tb~., data = tb_test)[,-1]\npred_test_lasso &lt;- as.numeric(predict(mod_lasso, newx = X_test, \n                                 type = \"response\"))\nroc_test_lasso &lt;- roc(predictor = pred_test_lasso,\n                     response = tb_test$tb,\n                     levels = c(0,1), direction = \"&lt;\")\n\nplot(roc_test_lasso, print.auc = TRUE)\n\n\n\n\n\n\n\n\nWe refit the lasso model on the full data from South Africa and present the updated model in the subsequent code chunk.\n\n# fit lasso model with CV\nX_train_full &lt;- model.matrix(tb~., data = tb_southafrica)[, -1]\ny_train_full &lt;- tb_southafrica[,1]\nmod_cv_full &lt;- cv.glmnet(X_train_full, y_train_full, alpha = 1, \n                       family = \"binomial\", nfolds = 5)\n\n# refit for given lambda\nmod_full &lt;- glmnet(X_train_full, y_train_full, alpha = 1, \n                   family = \"binomial\", \n                   lambda = mod_cv_full$lambda.min)\n\n# create data frame\ncoef_df &lt;- data.frame(\n  Variable = c(\"Intercept\", colnames(X_train_full)),\n  Lasso = signif(exp(as.numeric(coef(mod_full))), 3))\ncoef_df\n#&gt;            Variable   Lasso\n#&gt; 1         Intercept  0.0541\n#&gt; 2  age_group[15,25)  1.3400\n#&gt; 3  age_group[25,35)  2.5600\n#&gt; 4  age_group[35,45)  1.8100\n#&gt; 5  age_group[45,55)  1.2300\n#&gt; 6          hiv_pos1  2.5900\n#&gt; 7         diabetes1  1.9400\n#&gt; 8       ever_smoke1  0.7730\n#&gt; 9          past_tb1  1.2200\n#&gt; 10            male1  2.5000\n#&gt; 11         hs_less1  1.2500\n#&gt; 12  two_weeks_symp1  2.5200\n#&gt; 13    num_symptoms2  1.8700\n#&gt; 14    num_symptoms3  5.9400\n#&gt; 15    num_symptoms4 10.3000",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Case Study: Regression</span>"
    ]
  },
  {
    "objectID": "book/cs_regression.html#evaluate-model-on-validation-data",
    "href": "book/cs_regression.html#evaluate-model-on-validation-data",
    "title": "16  Case Study: Regression",
    "section": "16.2 Evaluate Model on Validation Data",
    "text": "16.2 Evaluate Model on Validation Data\nWe then evaluate the lasso model on the withheld validation data. These data comes from clinics in urban Uganda and contains only 387 observations. The generated table shows that this population differs from our training population including having a lower proportion of patients diagnosed with TB.\n\ntbl_summary(tb_diagnosis, by = c(country),\n            label = list(tb ~ \"TB Diagnosis\",\n                         age_group ~ \"Age\",\n                         hiv_pos ~ \"HIV Positive\",\n                         diabetes ~ \"Diabetes\",\n                         ever_smoke ~ \"Ever Smoked\",\n                         past_tb ~ \"Past TB Diagnosis\",\n                         male ~ \"Male\",\n                         hs_less ~ \"&lt; HS Education\",\n                         two_weeks_symp ~ \"Symptoms for Two Weeks\",\n                         num_symptoms ~ \"Number of TB Symptoms\")) %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Country**\") %&gt;%\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nCountry\n\n\nSouth Africa, N = 1,4071\nUganda, N = 3871\n\n\n\n\nTB Diagnosis\n\n\n\n\n\n\n    0\n705 (50%)\n281 (73%)\n\n\n    1\n702 (50%)\n106 (27%)\n\n\nAge\n\n\n\n\n\n\n    [55,99)\n272 (19%)\n20 (5.2%)\n\n\n    [15,25)\n206 (15%)\n86 (22%)\n\n\n    [25,35)\n286 (20%)\n129 (33%)\n\n\n    [35,45)\n338 (24%)\n99 (26%)\n\n\n    [45,55)\n305 (22%)\n53 (14%)\n\n\nHIV Positive\n\n\n\n\n\n\n    0\n850 (60%)\n256 (66%)\n\n\n    1\n557 (40%)\n131 (34%)\n\n\nDiabetes\n\n\n\n\n\n\n    0\n1,360 (97%)\n383 (99%)\n\n\n    1\n47 (3.3%)\n4 (1.0%)\n\n\nEver Smoked\n\n\n\n\n\n\n    0\n911 (65%)\n328 (85%)\n\n\n    1\n496 (35%)\n59 (15%)\n\n\nPast TB Diagnosis\n\n\n\n\n\n\n    0\n1,187 (84%)\n331 (86%)\n\n\n    1\n220 (16%)\n56 (14%)\n\n\nMale\n\n\n\n\n\n\n    0\n670 (48%)\n199 (51%)\n\n\n    1\n737 (52%)\n188 (49%)\n\n\n&lt; HS Education\n\n\n\n\n\n\n    0\n119 (8.5%)\n30 (7.8%)\n\n\n    1\n1,288 (92%)\n357 (92%)\n\n\nSymptoms for Two Weeks\n\n\n\n\n\n\n    0\n364 (26%)\n63 (16%)\n\n\n    1\n1,043 (74%)\n324 (84%)\n\n\nNumber of TB Symptoms\n\n\n\n\n\n\n    1\n601 (43%)\n156 (40%)\n\n\n    2\n344 (24%)\n128 (33%)\n\n\n    3\n266 (19%)\n68 (18%)\n\n\n    4\n196 (14%)\n35 (9.0%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\nThe ROC curve shows that the AUC on the validation data is lower than on the training data but still maintains meaningful discrimination.\n\n# lasso validation roc\nX_val &lt;- model.matrix(tb~., data = tb_uganda)[, -1]\npred_val &lt;- as.numeric(predict(mod_full, newx = X_val, \n                                      type = \"response\"))\nroc_val_lasso &lt;- roc(predictor = pred_val,\n                     response = tb_uganda$tb,\n                     levels = c(0,1), direction = \"&lt;\")\n\nplot(roc_val_lasso, print.auc = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nBaik, Yeonsoo, Hannah M Rickman, Colleen F Hanrahan, Lesego Mmolawa, Peter J Kitonsa, Tsundzukana Sewelana, Annet Nalutaaya, et al. 2020. “A Clinical Score for Identifying Active Tuberculosis While Awaiting Microbiological Results: Development and Validation of a Multivariable Prediction Model in Sub-Saharan Africa.” PLoS Medicine 17 (11): e1003420.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Case Study: Regression</span>"
    ]
  },
  {
    "objectID": "book/control_flows.html#logic-and-conditional-expressions",
    "href": "book/control_flows.html#logic-and-conditional-expressions",
    "title": "17  Logic and Loops",
    "section": "17.1 Logic and Conditional Expressions",
    "text": "17.1 Logic and Conditional Expressions\nYou may recall that we introduced logical operators in Chapter 3. We used these operators through conditional expressions such as when we indexed a data frame or the ifelse() or casewhen() functions. For example, in the following code we have vectors of systolic and diastolic blood pressure measurements and we write a logical operator to check if at least one of the systolic measurements is above 140 or if at least one of the diastolic measurements is above 90.\n\nsbp_measurements &lt;- c(131, 110, 125, 145, NA, 130)\ndbp_measurements &lt;- c(70, NA, 80)\nany(sbp_measurements &gt; 140, na.rm = TRUE) | \n  any(dbp_measurements &gt; 90, na.rm = TRUE)\n#&gt; [1] TRUE\n\nLet’s look at another example. Suppose these blood pressure measurements were taken consecutively but may have missing values. We want to create a single value to summarize the blood pressure for the patient. If we only have one blood pressure reading, then we use that value. However, if there is more than one blood pressure reading, then we drop the first observation and average the rest. We assume that not all values are NA. The following code uses an ifelse() function to do this by first checking if there is a single reading. If so, it takes the sum removing NA values to find that value. If not, we find all non-NA values and remove the first one before averaging.\n\nsbp_measurements &lt;- c(131, 110, 125, 145, NA, 130)\nifelse(sum(!is.na(sbp_measurements)) == 1, \n       sum(sbp_measurements, na.rm = TRUE),\n       mean(sbp_measurements[!is.na(sbp_measurements)][-1]))\n#&gt; [1] 128\n\nWe could also accomplish the same thing using a control flow called an if-else statement. An if-else statement follows the following structure. First, we have a conditional statement. If the conditional statement is true, then the code in the if statement, the code within the first set of curly braces, is run. If not, then the code in the else statement is run. In this way, the control flow controls how our code is executed.\nif (conditional statement){\nblock of code if the statement is TRUE\n} else{\nblock of code if the statement is FALSE\n}\nThe next code chunk shows an example where the conditional statement is the same as previously. Note that since either the code in the if or else statement is run, the object avg_val is always defined.\n\nsbp_measurements &lt;- c(131, 110, 125, 145, NA, 130)\nif(sum(!is.na(sbp_measurements)) == 1){\n  avg_val &lt;- sum(sbp_measurements, na.rm = TRUE)\n} else{\n  avg_val &lt;- mean(sbp_measurements[!is.na(sbp_measurements)][-1])\n}\navg_val\n#&gt; [1] 128\n\nOne of the things to notice is that an if statement can only take in a single Boolean. It cannot take in a vector of Boolean values like the ifelse() and case_when() functions can. In that way, the ifelse() function is useful because it can be applied to multiple instances, but it isn’t as flexible if you want to run multiple lines of code depending on the logical statement since it doesn’t allow you to include a code block.\nLet’s do another example of both an if-else statement and the ifelse() function to demonstrate this. In the following code, we use an if-else statement to determine if someone has hypertension. Note that here we have two lines of code that are run in each part - one line printing the result and the other is storing a 0/1 value. Try changing the values of sbp and dbp.\n\nsbp &lt;- 130\ndbp &lt;- 80\nif(sbp &gt; 140 | dbp &gt; 90){\n  print(\"Hypertension\")\n  hyp &lt;- 1\n} else{\n  print(\"No Hypertension\")\n  hyp &lt;- 0\n}\n#&gt; [1] \"No Hypertension\"\nhyp\n#&gt; [1] 0\n\nNow let’s replicate this with the ifelse() function which allows us to take in paired vectors of blood pressure measurements and return a vector of 0/1 values for each observation. The difference here is that we cannot include a print statement since we are only allowed one return value.\n\nsbp_measurements &lt;- c(131, 110, 125, 145, 130)\ndbp_measurements &lt;- c(90, 75, 80, 90, 80)\nhyp &lt;- ifelse(sbp_measurements &gt; 140 | dbp_measurements &gt; 90, 1, 0)\nhyp\n#&gt; [1] 0 0 0 1 0\n\nNote that in the previous code we ignored NA values. In this case, changing sbp or dbp to NA causes an error in the if-else statement. This is because it does not understand which code block to run. The ifelse() can handle NA values and returns NA for observations with no TRUE/FALSE value. To accomplish this with the if-else statement, we can add in multiple conditions. In particular, we can add in more statements as follows. In this case, the first time we reach a true conditional statement, we run the code in that block. If no statements are true, then we run the last block of code. So we always run exactly one block of code.\nif (conditional statement A){\nblock of code if the statement A is TRUE\n} else if (conditional statement B){\nblock of code if the statement B is TRUE and statement A is FALSE\n} else if (conditional statement C){\nblock of code if the statement C is TRUE and statement A and B are FALSE\n} else{\nblock of code if statements A, B, and C are all FALSE\n}\nLet’s use this with our hypertension example. In this case, we want to return NA if the answer is not known. Change the values so that you reach each code block. The order of the conditions matters because if the first statement is false, then we know at least one value is not NA. This also means that we would only check the fourth condition if the first three are false, which means that neither of the values can be NA.\n\nsbp &lt;- 130\ndbp &lt;- 80\nif(is.na(sbp) & is.na(dbp)){\n  # Both are NA\n  hyp &lt;- NA\n} else if ((is.na(sbp) & dbp &lt;= 90) | (is.na(dbp) & sbp &lt;= 140)){\n  # One is NA and the other is below the threshold\n  hyp &lt;- \"Inconclusive\"\n} else if ((is.na(sbp) & dbp &gt; 90) | (is.na(dbp) & sbp &gt; 140)){\n  # One is NA and the other is above the threshold\n  hyp &lt;- \"Hypertension\"\n} else if (dbp &gt; 90 | sbp &gt; 140){\n  # Neither are NA and at least one is above the threshold\n  hyp &lt;- \"Hypertension\"\n} else{\n  # Neither are NA and neither is above the threshold\n  hyp &lt;- \"No Hypertension\"\n}\nhyp\n#&gt; [1] \"No Hypertension\"\n\nWe can rearrange these conditions to have one less condition. In the following code chunk, we first check if both are NA. Then we check that at least one value is above the threshold. This statement uses the fact that both can’t be NA since the first condition must be false. Next, in the third statement, if at least one value is NA then that must mean the other is below the threshold so the result is inconclusive.\n\nsbp &lt;- 130\ndbp &lt;- 80\nif(is.na(sbp) & is.na(dbp)){\n  # Both are NA\n  hyp &lt;- NA\n} else if (sum(dbp &gt; 90, sbp &gt; 140, na.rm=TRUE) &gt;= 1){\n  # At least one is above the threshold - sum removes NA values\n  hyp &lt;- \"Hypertension\"\n} else if (is.na(sbp) | is.na(dbp)){\n  # Inconclusive\n  hyp &lt;- \"Inconclusive\"\n} else{\n  # Neither are NA and neither is above the threshold\n  hyp &lt;- \"No Hypertension\"\n}\nhyp\n#&gt; [1] \"No Hypertension\"\n\nThis can still seem like a lot of conditions to replicate what we did in a single line with an ifelse() function. In general, we prefer a simpler format. Consider the following code. In this case, we have two vectors x and y that we want to plot. First, we check whether these vectors are numeric. If not, we convert them to factors. Rather than returning a value as we do with an ifelse() function, we are changing our data depending on the type of x and y. Note that these statements do not contain an else statement. That is because we don’t want to run any code when the condition is false. For these single-expression if statements, we technically don’t need to the curly braces for R to understand what code to run, but we consider it good practice to always wrap your code in curly braces when writing control flows.\n\n# example x and y vectors\ny &lt;- factor(rbinom(100, 1, 0.3))\nx &lt;- rnorm(100, ifelse(y == 0, 0, 0.75)) \n# change x to factor(rbinom(100, 1, 0.3)) to observe\n\n# convert x and y to factors if not numeric!\nif (!is.numeric(x)){ x &lt;- as.factor(x) }\nif (!is.numeric(y)){ y &lt;- as.factor(y) }\n\n# find type of plot\nif(is.factor(x) & is.factor(y)){\n  # bar plot \n  p &lt;- ggplot() + geom_bar(aes(x = x, fill = y), position = \"dodge\")\n} else if (!is.factor(y) | !(is.factor(x))){\n  # box plot when one numeric, one factor\n  p &lt;- ggplot() + geom_boxplot(aes(x = x, y = y))\n} else{\n  # scatter plot when both numeric\n  p &lt;- ggplot() + geom_point(aes(x = x, y = y))\n}\np\n\n\n\n\n\n\n\n\n\n17.1.1 Practice Question\nUse both an if-else statement and a case_when() function to find y as given by the following function. \\[y = \\begin{cases} 1 & x &gt; 0 \\\\ 0 & x =0 \\\\ 0.1 & x &lt; 0  \\end{cases} \\]\n\n# Insert your solution here:\nx &lt;- 2 # change x to different values to check your solution!",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logic and Loops</span>"
    ]
  },
  {
    "objectID": "book/control_flows.html#loops",
    "href": "book/control_flows.html#loops",
    "title": "17  Logic and Loops",
    "section": "17.2 Loops",
    "text": "17.2 Loops\nAnother common control flow we use are loops. Loops capture code chunks we want to run multiple times. For this example, we use the NHANESSample data from the HDSinRdata package.\n\nnhanes &lt;- NHANESsample %&gt;% \n  select(c(RACE, SEX, SBP1, DBP1, HYP, LEAD)) %&gt;%\n  na.omit()\n\nIn the following code, we are fitting a simple linear regression model for systolic blood pressure with the single covariate of blood lead level for each race group and storing the associated coefficients and p-values. This code is repetitive since we repeat the same steps for each group and the only element that is changing is the race group. This makes our code cluttered but also means we are prone to introducing errors. In fact, you can see that we have the wrong coefficient and p-value for the fourth model.\n\ndat1 &lt;- nhanes[nhanes$RACE == \"Mexican American\", ]\nmod1 &lt;- summary(lm(SBP1 ~ LEAD, data = dat1))\ncoef1 &lt;- mod1$coefficients[2, 1]\npval1 &lt;- mod1$coefficients[2, 4]\n\ndat2 &lt;- nhanes[nhanes$RACE == \"Non-Hispanic White\", ]\nmod2 &lt;- summary(lm(SBP1 ~ LEAD, data = dat2))\ncoef2 &lt;- mod2$coefficients[2, 1]\npval2 &lt;- mod2$coefficients[2, 4]\n\ndat3 &lt;- nhanes[nhanes$RACE == \"Non-Hispanic Black\", ]\nmod3 &lt;- summary(lm(SBP1 ~ LEAD, data = dat3))\ncoef3 &lt;- mod3$coefficients[2, 1]\npval3 &lt;- mod3$coefficients[2, 4]\n\ndat4 &lt;- nhanes[nhanes$RACE == \"Other Hispanic\", ]\nmod4 &lt;- summary(lm(SBP1 ~ LEAD, data = dat4))\ncoef4 &lt;- mod3$coefficients[2, 1]\npval4 &lt;- mod3$coefficients[2, 4]\n\ndat5 &lt;- nhanes[nhanes$RACE == \"Other Race\", ]\nmod5 &lt;- summary(lm(SBP1 ~ LEAD, data = dat5))\ncoef5 &lt;- mod5$coefficients[2, 1]\npval5 &lt;- mod5$coefficients[2, 4]\n\ndata.frame(\n  group = c(\"Mexican American\", \"Non-Hispanic White\", \n            \"Non-Hispanic Black\", \"Other Hispanic\", \"Other Race\"),\n  coefs = c(coef1, coef2, coef3, coef4, coef5),\n  pvals = c(pval1, pval2, pval3, pval4, pval5))\n#&gt;                group coefs     pvals\n#&gt; 1   Mexican American 0.783  3.97e-11\n#&gt; 2 Non-Hispanic White 2.500 7.81e-138\n#&gt; 3 Non-Hispanic Black 2.005  1.83e-51\n#&gt; 4     Other Hispanic 2.005  1.83e-51\n#&gt; 5         Other Race 1.927  1.06e-11\n\nWe can rewrite this code slightly. In this case, we create an object i which represents the index of the group. This change means that the only thing that changes for each group is that we update the value of i. This is much less prone to errors, but still long.\n\n# Initialize results data frame\nrace_values &lt;- c(\"Mexican American\", \"Non-Hispanic White\", \n            \"Non-Hispanic Black\", \"Other Hispanic\", \"Other Race\")\ndf &lt;- data.frame(\n  group = race_values,\n  coefs = 0,\n  pvals = 0)\n\ni &lt;- 1\ndat &lt;- nhanes[nhanes$RACE == df$group[i], ]\nmod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\ndf$coef[i] &lt;- mod$coefficients[2, 1]\ndf$pval[i] &lt;- mod$coefficients[2, 4]\n\ni &lt;- 2\ndat &lt;- nhanes[nhanes$RACE == df$group[i], ]\nmod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\ndf$coef[i] &lt;- mod$coefficients[2, 1]\ndf$pval[i] &lt;- mod$coefficients[2, 4]\n\ni &lt;- 3\ndat &lt;- nhanes[nhanes$RACE == df$group[i], ]\nmod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\ndf$coef[i] &lt;- mod$coefficients[2, 1]\ndf$pval[i] &lt;- mod$coefficients[2, 4]\n\ni &lt;- 4\ndat &lt;- nhanes[nhanes$RACE == df$group[i], ]\nmod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\ndf$coef[i] &lt;- mod$coefficients[2, 1]\ndf$pval[i] &lt;- mod$coefficients[2, 4]\n\ni &lt;- 5\ndat &lt;- nhanes[nhanes$RACE == df$group[i], ]\nmod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\ndf$coef[i] &lt;- mod$coefficients[2, 1]\ndf$pval[i] &lt;- mod$coefficients[2, 4]\n\ndf\n#&gt;                group coefs pvals  coef      pval\n#&gt; 1   Mexican American     0     0 0.783  3.97e-11\n#&gt; 2 Non-Hispanic White     0     0 2.500 7.81e-138\n#&gt; 3 Non-Hispanic Black     0     0 2.005  1.83e-51\n#&gt; 4     Other Hispanic     0     0 1.242  8.46e-09\n#&gt; 5         Other Race     0     0 1.927  1.06e-11\n\nWe now write this code as a for loop. A for loop contains two pieces. First, we have an iterator. An iterator traverses an object that has a natural order. Most of the time we traverse over vectors but we could also have a list object. The second piece is a code block. This code is run for each value of the iterator.\nfor (iterator_name in object){\ncode to run for each value of the iterator\n}\nTwo simple for loops are given in the next code chunk. In the first loop, our iterator goes through the vector 1:5 whereas in the second one our iterator iterates through the vector of names. In the first loop, we traverse the numbers 1 to 5 and for each number we run the code that squares the number. In each iteration, we name the current number we are on to be i. That means that the first time through the loop i is equal to 1, the second time i has value 2, etc. In the second loop, our iterator is also a vector but this time it is names. In this case, in each iteration the object name represents the current name we are on as we traverse the vector of names. In particular, the first time through the loop name is equal to “Alice”, the second time name has value “Bob”, and so forth.\n\nfor (i in 1:5){\n  print(sqrt(i))\n}\n#&gt; [1] 1\n#&gt; [1] 1.41\n#&gt; [1] 1.73\n#&gt; [1] 2\n#&gt; [1] 2.24\n\n\nnames &lt;- c(\"Alice\", \"Bob\", \"Carol\")\nfor (name in names){\n  print(paste(\"Hello,\", name))\n}\n#&gt; [1] \"Hello, Alice\"\n#&gt; [1] \"Hello, Bob\"\n#&gt; [1] \"Hello, Carol\"\n\nLet’s apply this to our example. First, we use a numeric iterator i that takes on values 1 to 5. This directly replicates our previous code in which the value of i changed for each race group. Our result matches our previous result.\n\ndf &lt;- data.frame(group = race_values, coefs = 0, pvals = 0)\n\nfor (i in 1:5){\n  dat &lt;- nhanes[nhanes$RACE == df$group[i], ]\n  mod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\n  df$coef[i] &lt;- mod$coefficients[2, 1]\n  df$pval[i] &lt;- mod$coefficients[2, 4]\n}\ndf\n#&gt;                group coefs pvals  coef      pval\n#&gt; 1   Mexican American     0     0 0.783  3.97e-11\n#&gt; 2 Non-Hispanic White     0     0 2.500 7.81e-138\n#&gt; 3 Non-Hispanic Black     0     0 2.005  1.83e-51\n#&gt; 4     Other Hispanic     0     0 1.242  8.46e-09\n#&gt; 5         Other Race     0     0 1.927  1.06e-11\n\nLet’s show a different way we could write the same loop. This time we make our iterator be the race group name. In this case, we update how we are storing the coefficients and p-values because we are not iterating over an index.\n\ncoefs &lt;- c()\npvals &lt;- c()\n\nfor (group in race_values){\n  dat &lt;- nhanes[nhanes$RACE == group, ]\n  mod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\n  coefs &lt;- c(coefs, mod$coefficients[2, 1])\n  pvals &lt;- c(pvals, mod$coefficients[2, 4])\n}\n\ndata.frame(group = race_values, coefs = coefs, pvals = pvals)\n#&gt;                group coefs     pvals\n#&gt; 1   Mexican American 0.783  3.97e-11\n#&gt; 2 Non-Hispanic White 2.500 7.81e-138\n#&gt; 3 Non-Hispanic Black 2.005  1.83e-51\n#&gt; 4     Other Hispanic 1.242  8.46e-09\n#&gt; 5         Other Race 1.927  1.06e-11\n\nAnother type of loop is a while loop. A while loop does not have an iterator. Instead, a while loop checks a condition. If the condition is true, the loop runs the code in the code block. If the condition is false, it stops and breaks out of the loop. That is, the code is run until the condition is no longer met.\nwhile (condition){\ncode to run each iteration\n}\nThe following code gives an example of a simple while loop. In this case, the loop keeps dividing x by 2 until it is below a certain value of 3. In this case, x starts above 3 so the condition starts off being true and we would divide x by 2 to get 50. Since 50 is still greater than 3, the code block is run again, etc. Once x reaches a value of 1.5625 the condition no longer holds and the code stops. Note that if the condition was x &gt; -1 it would hold indefinitely, creating what is called an infinite loop.\n\nx &lt;- 100\nwhile(x &gt; 3){\n  x &lt;- x/2\n}\nx\n#&gt; [1] 1.56\n\nLet’s do another example with a bigger code block. The following code creates a Poisson process of arrivals where in each iteration we generate the next arrival time by drawing from an exponential distribution. Once we reach the end of the time interval (i.e. the current time is greater than 10) we stop. If we re-run this chunk of code, we might get a different length vector.\n\narrivals &lt;- c()\ntime &lt;- 0\nnext_arrival &lt;- rexp(1, rate = 3)\n\n# Find the time of all arrivals in the time period [0,10]\nwhile(time+next_arrival &lt;= 10){\n  # Update list of arrivals and current time\n  arrivals &lt;- c(arrivals, next_arrival)\n  time &lt;- time + next_arrival\n  \n  # Generate the next arrival\n  next_arrival &lt;- rexp(1, rate = 3)\n}\n\nGiven we have two types of loops, how do you know which to use? You should use a for loop when you know how many times you go through the loop and/or if there is a clear object to iterate through. On the other hand, while loops are useful if you don’t know how many times you go through the loop and you want to iterate through the loop until something happens. Within a for loop, you can also break out early using the break operator. This stops the loop similar to a while loop but is sometimes less succinct. The following code loops through the blood pressure measurements we defined earlier to find if any of the observations meet the criteria for hypertension.\n\n# Start with assumption that the result is FALSE\nres &lt;- FALSE\nfor (i in 1:length(sbp_measurements)){\n  \n  # If above threshold, update the result and stop the loop\n  if (sbp_measurements[i] &gt; 140 | dbp_measurements[i] &lt; 90){\n    res &lt;- TRUE\n    break \n  }\n}\n\n\n17.2.1 Practice Question\nUse a loop to find the smallest integer number x such that \\(2.3^x \\geq 100\\). The answer should be 6.\n\n# Insert your solution here:",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logic and Loops</span>"
    ]
  },
  {
    "objectID": "book/control_flows.html#avoiding-control-flows-with-functions",
    "href": "book/control_flows.html#avoiding-control-flows-with-functions",
    "title": "17  Logic and Loops",
    "section": "17.3 Avoiding Control Flows with Functions",
    "text": "17.3 Avoiding Control Flows with Functions\nWe just introduced logic and loops and now I’m going to tell you to avoid them when you can. Control flows are very useful programming structures, but sometimes the same thing can be done without them. For example, we can find whether there is at least one observation that has hypertension using a single line of code.\n\nany(sbp_measurements &gt; 140 | dbp_measurements &gt; 90)\n#&gt; [1] TRUE\n\nAnother example we saw previously was using an ifelse() or case_when() function instead of an if-else statement. These two functions are vectorized functions. That means that the function is evaluated on a vector of values rather than having to loop through each value separately. Vectorized functions return a vector or results of the same size as your input. That means that if you needed to do a computation on every element of a vector you could either loop through all the elements and call that function or you can take advantage of the vectorized structure and call the function on the whole vector. This is generally cleaner and more efficient. The any() function up not a vectorized function since it returns a single TRUE/FALSE value but it also helps to make our code cleaner.\nAnother tool that can help with brevity in this manner is the family of apply functions. These are loop-hiding functions. In Chapter 3, we saw the apply(X, MARGIN, FUN) function. This function called the function FUN on either the rows (MARGIN = 1) or columns (MARGIN = 2) of X, which is data frame or matrix X. In the next code chunk, we generate a random matrix X and compute the column means using a loop and using the apply() function. We can see that the version with the apply() function is simpler.\n\nX &lt;- matrix(rnorm(100), nrow = 20, ncol = 5)\n\n# Apply mean function  \napply(X, 2, mean)\n#&gt; [1] -0.0854  0.0068 -0.0944 -0.0528 -0.4305\n\n# Loop through columns\nmeans &lt;- rep(0, ncol(X))\nfor (i in 1:ncol(X)){\n  means[i] &lt;- mean(X[, i])\n}\nmeans\n#&gt; [1] -0.0854  0.0068 -0.0944 -0.0528 -0.4305\n\nAnother loop-hiding function is lapply(X, FUN). This function applies the function X to each element of X. In this case, X functions like an iterator and FUN is a function representing what we want to do in each iteration. The result is returned as a list of the function output for each value of X. We use this function in the regression context we saw earlier. Here, X is our vector of groups and we have written a custom function to be able to call that code on each group. We learn how to write our own functions in Chapter 18.\n\nfind_lm_results &lt;- function(group){\n  # Runs simple linear regression and returns coefficient and p-value\n  dat &lt;- nhanes[nhanes$RACE == group, ]\n  mod &lt;- summary(lm(SBP1 ~ LEAD, data = dat))\n  return(mod$coefficients[2, c(1, 4)])\n}\n\nlapply(race_values, find_lm_results)\n#&gt; [[1]]\n#&gt; Estimate Pr(&gt;|t|) \n#&gt; 7.83e-01 3.97e-11 \n#&gt; \n#&gt; [[2]]\n#&gt;  Estimate  Pr(&gt;|t|) \n#&gt;  2.50e+00 7.81e-138 \n#&gt; \n#&gt; [[3]]\n#&gt; Estimate Pr(&gt;|t|) \n#&gt; 2.01e+00 1.83e-51 \n#&gt; \n#&gt; [[4]]\n#&gt; Estimate Pr(&gt;|t|) \n#&gt; 1.24e+00 8.46e-09 \n#&gt; \n#&gt; [[5]]\n#&gt; Estimate Pr(&gt;|t|) \n#&gt; 1.93e+00 1.06e-11\n\nAnother useful function is sapply(X, FUN). This function operates similarly to lapply(). However, it then tries to simplify the output to be either a vector or matrix. You can remember the difference by remembering the l in lapply() stands for list and the s in sapply() stands for simplify.\n\nsapply(race_values, find_lm_results)\n#&gt;          Mexican American Non-Hispanic White Non-Hispanic Black\n#&gt; Estimate         7.83e-01           2.50e+00           2.01e+00\n#&gt; Pr(&gt;|t|)         3.97e-11          7.81e-138           1.83e-51\n#&gt;          Other Hispanic Other Race\n#&gt; Estimate       1.24e+00   1.93e+00\n#&gt; Pr(&gt;|t|)       8.46e-09   1.06e-11\n\nThe last loop-hiding function we introduce is replicate(n, expr). This runs the code expression expr n times and returns the results. By default, this simplifies the output similar to sapply(). If you set simplify=FALSE, it returns a list. The following code generates a random matrix and computes the column means six times.\n\nreplicate(6, colMeans(matrix(rnorm(100), ncol = 5)))\n#&gt;          [,1]    [,2]    [,3]    [,4]    [,5]     [,6]\n#&gt; [1,]  0.00805  0.0292 -0.0140 -0.0759  0.2768  0.08852\n#&gt; [2,]  0.30355  0.0738  0.0671 -0.1637 -0.2121  0.20518\n#&gt; [3,] -0.21610  0.3548  0.4920  0.3131  0.0158  0.00204\n#&gt; [4,] -0.25750 -0.1556 -0.1095 -0.0724 -0.1706 -0.50488\n#&gt; [5,]  0.01583  0.0959 -0.2109 -0.1672 -0.3063 -0.03680",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logic and Loops</span>"
    ]
  },
  {
    "objectID": "book/control_flows.html#exercises",
    "href": "book/control_flows.html#exercises",
    "title": "17  Logic and Loops",
    "section": "17.4 Exercises",
    "text": "17.4 Exercises\nFor these exercises, we use the pain data from the HDSinRdata package. You can use the help operator ?pain to learn more about the source of these data and to read its column descriptions.\n\nCreate a new column PAT_RACE_SIMP that represents a patient’s race using three categories: White, Black, or Other. First, do this using the case_when() function. Then, use a loop and if-else statement to accomplish the same thing.\nFor each category of your new column PAT_RACE_SIMP, subset the data to that group and find the five body regions with the highest proportion of patients with pain. Your solution should use two nested loops. Then, rewrite your code without using a loop.\nThe following code sorts a numeric vector x but is missing comments to explain the steps. Read through the code and add your own comments to explain how this works.\n\nx &lt;- c(1,3,0,3,2,6,4)\n\nn &lt;- length(x)\nfor (i in 1:(n-1)){\n\n  next_ind &lt;- i\n  for (j in (i+1):n){\n    if (x[j] &lt; x[next_ind]){\n      next_ind &lt;- j\n    }\n  }\n\n  temp &lt;- x[i]\n  x[i] &lt;- x[next_ind]\n  x[next_ind] &lt;- temp\n}\n\nx \n#&gt; [1] 0 1 2 3 3 4 6\n\nWrite code using a loop that generates a series of Bernoulli random variables with probability of success of 0.5 until at least \\(r &lt;- 6\\) successes occur. What distribution does this correspond to?",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Logic and Loops</span>"
    ]
  },
  {
    "objectID": "book/functions.html#components-of-a-function",
    "href": "book/functions.html#components-of-a-function",
    "title": "18  Functions",
    "section": "18.1 Components of a Function",
    "text": "18.1 Components of a Function\nTo start storing functions we need to give them a name and define their input (arguments) and output (return values). To do so we assign a function name to a function object as shown in the following code. This example function has two arguments arg1 and arg2 and returns output.\nfunction_name &lt;- function(arg1, arg2){\n  code to compute output from arguments\n  return(output)\n}\nTake a look at the following simple function. The name of this function is say_hello and there is no input (arguments) or output (return values) associated with this function. Instead, it just prints out a hello statement.\n\nsay_hello &lt;- function(){\n  print(\"Hello!\")\n}\n\nRunning the previous code creates an object called say_hello of the class function. We can run this function by calling it using empty parenthesis (since there are no input arguments).\n\nclass(say_hello)\n#&gt; [1] \"function\"\n\n\nsay_hello()\n#&gt; [1] \"Hello!\"\n\nWe can add to this function by instead adding our first argument called name which is a string and then printing “Hello, [name]!”. In the next code chunk, we use the paste0() function which concatenates the string arguments into a single string.\n\nsay_hello &lt;- function(name){\n  print(paste0(\"Hello, \", name, \"!\"))\n}\nsay_hello(\"Weici\")\n#&gt; [1] \"Hello, Weici!\"\n\n\n18.1.1 Arguments\nArguments are inputs passed to functions so that they can complete the desired computation. We can also have default values for these arguments. In this case, those arguments do not have to be specified when calling the function. For example, rnorm(10) uses the default value for the mean to understand which distribution we want to use. In the following function, we find the Euclidean distance from a given (x,y,z) coordinate and the origin (0,0,0) with a default value of zero for all values.\n\ndist_to_origin &lt;- function(x = 0,y = 0,z = 0){\n  return(((x - 0)^2 + (y - 0)^2 + (z - 0)^2)^(0.5))\n}\n\nIf we call this function with no arguments, it uses all the default values.\n\ndist_to_origin()\n#&gt; [1] 0\n\nHowever, if we call the function with one argument, the function assumes this first argument is x. Similarly, it assumes the second value is y and the third value is z. If we want to give the arguments without worrying about the order, we can specify them using their names (see the last line in the following code chunk).\n\ndist_to_origin(1)\n#&gt; [1] 1\ndist_to_origin(1, 2)\n#&gt; [1] 2.24\ndist_to_origin(1, 2, 3)\n#&gt; [1] 3.74\ndist_to_origin(y = 2, z = 3)\n#&gt; [1] 3.61\n\nBesides passing in numeric values, strings, data frames, lists, or vectors, we can also pass other types of objects in as arguments to a function. For example, we can take another function in as an argument. In the next example, we create two functions. The first function calculates the euclidean distance between two points. The second one computes the distance from a given point to the origin. Note that this updated function to find the distance to the origin is more flexible and written in a cleaner manner. First, it allows us to input a point of any length. Second, it allows us to specify the distance function used. This also demonstrates calling a function within another function.\nTry out calling euclidean_dist() and dist_to_origin() on different values.\n\neuclidean_dist &lt;- function(pt1, pt2){\n  # Finds the Euclidean distance from pt1 to pt2\n  return(sqrt(sum((pt1- pt2)^2)))\n}\n\ndist_to_origin &lt;- function(pt1, dist_func = euclidean_dist){\n  # Finds the distance from pt1 to the origin\n  origin &lt;- rep(0, length(pt1))\n  return(dist_func(pt1, origin))\n}\n\ndist_to_origin(c(1,1))\n#&gt; [1] 1.41\n\n\n\n18.1.2 Practice Question\nWrite a function that calculates the Manhattan distance between two points pt1 and pt2, where the Manhattan distance is the sum of absolute differences between points across all the dimensions. To check your solution, you should check that the distance between points pt1 &lt;- c(1,-1,1.5) and pt2 &lt;- c(0.5, 2.5, -1) is 6.5.\n\n# Insert your solution here:\n\nAnother type of argument to a function can be a formula. If you have used linear regression in R, you have seen this in practice. In the following code, we fit a simple linear model where we specify a model formula y~x as the first argument. We are also using default arguments for rnorm() on the first two lines.\n\nx &lt;- rnorm(mean = 3, n = 100)\ny &lt;- x + rnorm(sd = 0.2, n = 100)\n\nlm(y ~ x)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;     -0.0476       1.0138\n\n\n\n18.1.3 Return Values\nIf you read the documentation for lm() by calling ?lm we can see that there are a lot of arguments that have default values. The other thing to note about the lm documentation is that there are multiple values returned. In fact, the type of object returned is a list containing all the different things we want to know about the results such as the coefficients.\n\nsimp_model &lt;- lm(y ~ x)\nsimp_model$coefficients\n#&gt; (Intercept)           x \n#&gt;     -0.0476      1.0138\n\nSince R only allows you to return one object, packaging the return values into a list is a useful way to return multiple outputs from a function. In the following example, we create a function coin_flips() that takes in a probability prob and a number of iterations n (with default value 10) and simulates n coin flips where the coin has a probability of prob of landing on heads. The function returns the percentage of trials that were heads and the results of the coin flips. We can access each of these returned values by using the names percent_heads and results.\n\ncoin_flips &lt;- function(prob, n = 10){\n  # Flips a coin with probability prob of heads for n trials\n  \n  results &lt;- rbinom(n = n, size = 1, prob = prob)\n  return(list(percent_heads = sum(results)/n, results = results))\n}\ntrial &lt;- coin_flips(0.6)\ntrial$percent_heads\n#&gt; [1] 0.4\ntrial$results\n#&gt;  [1] 0 0 1 0 0 0 1 1 1 0\n\nOne important thing to know about R and return values: if you don’t specify a return statement but assign the output of our function to an object, it will assign the value to the last computed object by default. In the following code, the value returned is 3. Avoid unexpected behavior by always using the return() function.\n\nex_return &lt;- function(){\n  x &lt;- 2\n  y &lt;- 3\n}\nresult &lt;- ex_return()\nresult\n#&gt; [1] 3\n\n\n\n18.1.4 Scope of Objects\nWhen working within functions and calling functions, we want to remember the scope of our objects. Global objects are objects defined outside of functions. These values can also be accessed outside or inside functions. For example, the object y is defined outside of a function and so is a global object meaning we can use its value inside the function.\n\ny &lt;- \"Cassandra\"\n\nex_scope &lt;- function(){\n  return(paste(\"Hey,\", y))\n}\n\nex_scope()\n#&gt; [1] \"Hey, Cassandra\"\n\nIf we change the value of a global object within a function however, it does not update the value outside of the function. In the subsequent code chunk, we add 1 to y inside the function but it does not change the value of y after the function is done. Every time we run a function, R creates a new sub-environment inside, which can access the values of global objects but also creates its own local objects. In this case, the function creates its own object y, which is a local object that is a copy of the original object.\nAs another example, the function also creates a local object called z which ceases to exist after we run the function. If we try to print z on the last line we would get an error that z is not found. All objects created inside functions only exist in that sub-environment and are erased when we are no longer in the function. Therefore, we want to make sure we return any values we want to store.\n\ny &lt;- 5\n\nex_local &lt;- function(x){\n  y &lt;- y + 1\n  z &lt;- x * y\n  return(y + z)\n}\n\nex_local(2)\n#&gt; [1] 18\ny\n#&gt; [1] 5\n\nTo update global objects within a function, you can use the &lt;&lt;- operator (global assignment operator). This looks for an object in the global environment and updates its value (or creates an object with this value if none is found). For example, the following function updates the value of the global objects y. As a general practice, we should be careful using global objects within a function and it often is safer to use input arguments and return values instead.\n\ny &lt;- 5\n\nex_update_global &lt;- function(x){\n  y &lt;&lt;- y + 1\n  return(y + x)\n}\n\nex_update_global(2)\n#&gt; [1] 8\ny\n#&gt; [1] 6\n\n\n\n18.1.5 Functions within Functions and Returning Functions\nSometimes we see functions written inside other functions. Writing functions within functions can be useful to separate out some part of the code or to give that function access to the local environment objects. In the following example, the inner function has access to the value of x even though we have not passed it as an argument. The downside of this structure is the function add_x() does not exist outside the function so we cannot call it in other code.\n\nadd_x_seq &lt;- function(x){\n  # Adds x to 1:10 and returns\n  add_x &lt;- function(y){\n      return(y + x)\n  }\n    \n  return(add_x(1:10))\n}\n\nadd_x_seq(3)\n#&gt;  [1]  4  5  6  7  8  9 10 11 12 13\n#add_x(3) # would return an error\n\nIf we want to use the created function, we can return it. In the updated example, we return an anonymous function. By doing so, we create a unique function for each x value.\n\nadd_x &lt;- function(x){\n  # Returns a function to add x to any value\n  return(function(y) y + x)\n}\n\nadd2 &lt;- add_x(2)\nadd2(1:10)\n#&gt;  [1]  3  4  5  6  7  8  9 10 11 12\n\nadd10 &lt;- add_x(10)\nadd10(1:10)\n#&gt;  [1] 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "book/functions.html#documenting-functions",
    "href": "book/functions.html#documenting-functions",
    "title": "18  Functions",
    "section": "18.2 Documenting Functions",
    "text": "18.2 Documenting Functions\nThe functions we wrote had minimal comments or documentation. When creating functions, we should document them including any information about the format of the input and output. We do so using comments that precede the function and start with #'. This style of function documentation is called roxygen. The following code chunk shows an example for our Euclidean distance function. You can see we provided information about the two arguments and the return value. The roxygen style is the style used for published R packages.\n\n#' Euclidean distance\n#' \n#' @description Calculates the Euclidean distance between two points\n#'\n#' @param pt1 numeric vector \n#' @param pt2 numeric vector \n#' @return the Euclidean distance from pt1 to pt2\neuclidean_dist &lt;- function(pt1, pt2){\n  return((sum((pt1 - pt2)^2))^0.5)\n}\n\nEach comment should start with a pound sign and back-tick. The first block of lines is the introduction and the first line of the comment block is reserved for the title. This is the first information we want the user to know about the function and should be a single line. For all other information besides the title, we use certain tags.\n\n@description This tag should be placed first and is a place where you can briefly put more information about the function beyond the title. You can also add more details with the @details tag.\n@param This tag comes before each input argument’s description. For each argument, we want to include the name and type, but we might also include information on how that argument is used.\n@return This tag documents the returned object and specifies the type. If we are returning a list, then we might include information about each object in the list.\n\n\n18.2.1 Practice Question\nWrite the documentation for the following coin flip function.\n\n#' Insert your solution here:\ncoin_flips &lt;- function(prob, n = 10){\n  results &lt;- rbinom(n = n, size = 1, prob = prob)\n  return(list(percent_heads = sum(results)/n, results = results))\n}",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "book/functions.html#debugging-and-testing",
    "href": "book/functions.html#debugging-and-testing",
    "title": "18  Functions",
    "section": "18.3 Debugging and Testing",
    "text": "18.3 Debugging and Testing\nAs we write more complex code and functions, we want to learn how to test our code. When it comes to testing code, a good mantra is “test early and test often.” So avoid writing too much code before running and checking that the results match what you expect. Here are some simple principles that are applicable to debugging in any setting.\n\nStart simple and build up in steps.\nCheck your syntax by checking that all parentheses (), brackets [], and curly braces {} match where you expect.\nCheck that object names are correct and you don’t have any accidental typos or that you are accidentally using the same name for different objects.\nRestart your R session and re-run all code.\nCheck if you use the same object name for different objects.\nLocalize your error by printing out the values of objects at each stage or use break points in R.\nModify your code one piece at a time and check all test values again to avoid introducing new errors.\n\nFor example, suppose we want to write a function that finds any pairs of numeric columns with a Pearson correlation with absolute value above a certain threshold. We want our code to be structured so that it makes sense, is flexible to our needs, and avoids unnecessary work. To start building up this function, we need to first think about the inputs and outputs we want. This is called a top-down approach. Sketching out the overall steps your code needs to complete before writing any of them can help to improve your structure and avoid having to rewrite large pieces.\nIn this case, our input to this function is a data frame which could contain a mixture of numeric and categorical columns and a threshold correlation value with a default value of 0.6. We want to return a data frame with the pairs and their correlation. This gives us a template for our documentation.\n\n#' Find pairs of columns with strong correlation\n#' \n#' @description Finds all pairs of numeric columns with strong Pearson \n#' correlation and returns the pairs in a data frame\n#' \n#' @param df data frame\n#' @param threshold positive numeric threshold value to define a \n#' strong correlation as one with absolute value above the threshold\n#' @return data frame with one row for each pair of columns\n#' with high correlation containing the names of the columns \n#' and the corresponding correlation\nhigh_cor &lt;- function(df, threshold = 0.6){\n  return()\n}\n\nNext, to start simple we want to create some artificial data we can use to test our function. We use the mvnrorm() function from the **MASS* package (Venables and Ripley 2002) to control the correlation between our columns and add in a categorical column that should be ignored by our function.\n\nset.seed(4)\ncor_mat &lt;- matrix(c(1, 0.9, 0.4, 0,\n                    0.9, 1, 0.3, 0,\n                    0.4, 0.3, 1.0, 0,\n                    0, 0, 0, 1.0), nrow = 4)\nm &lt;- round(MASS::mvrnorm(100, c(0,0,0,0), Sigma = cor_mat), 3)\ntest_df &lt;- as.data.frame(m)\ntest_df$V5 &lt;- sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)\nhigh_cor(test_df)\n#&gt; NULL\n\nWe can use the cor() function to find the Pearson correlation.\n\ncor(test_df[,-5])\n#&gt;         V1      V2     V3      V4\n#&gt; V1  1.0000  0.9006  0.324 -0.0797\n#&gt; V2  0.9006  1.0000  0.291 -0.0702\n#&gt; V3  0.3241  0.2912  1.000 -0.2306\n#&gt; V4 -0.0797 -0.0702 -0.231  1.0000\n\nGreat, now let’s roughly sketch out the steps we need to complete.\n\nSubset the data to only numeric columns.\nFind the correlation of all pairs of columns.\nCheck if a pair has a strong correlation.\nIf so, add it to our results.\n\nLet’s start with step 1. Before putting code into our function, we are going to test our steps on our example data. To do so, we use the select_if() function from the dplyr package.\n\ndf_numeric &lt;- select_if(test_df, is.numeric)\nhead(df_numeric)\n#&gt;       V1     V2     V3     V4\n#&gt; 1 -0.656  0.417  1.188  0.685\n#&gt; 2  0.117 -0.336 -1.611 -0.115\n#&gt; 3  1.229  0.617  0.241 -0.356\n#&gt; 4  0.478  0.121  1.176 -0.106\n#&gt; 5  1.546  1.914  0.366  0.045\n#&gt; 6  0.416  0.110  1.630 -1.726\n\nThis worked. Next we need to find the strong correlations. Now, we can use the correlation function to find the correlations. We then want to iterate through all pairs to check if the absolute value of the correlation is above our threshold. We use a loop for this. In our first attempt, we create a nested for loop where i is the index of one column and j is the index of the second column in the pair. We can see we must have some mistakes because we are getting pairs where i and j are equal to each other and we are also getting zeros. Note how we are using print statements. This helps us to identify that we need to add parentheses for the first for loop and we need to update j to start at i+1.\n\ncor_mat &lt;- cor(df_numeric)\nfor (i in 1:nrow(cor_mat) - 1){\n  for (j in (i:nrow(cor_mat))){\n    print(paste(i, j))\n  }\n}\n#&gt; [1] \"0 0\"\n#&gt; [1] \"0 1\"\n#&gt; [1] \"0 2\"\n#&gt; [1] \"0 3\"\n#&gt; [1] \"0 4\"\n#&gt; [1] \"1 1\"\n#&gt; [1] \"1 2\"\n#&gt; [1] \"1 3\"\n#&gt; [1] \"1 4\"\n#&gt; [1] \"2 2\"\n#&gt; [1] \"2 3\"\n#&gt; [1] \"2 4\"\n#&gt; [1] \"3 3\"\n#&gt; [1] \"3 4\"\n\nLet’s try again. This time we create a variable n which is the number of columns.\n\ncor_mat &lt;- cor(df_numeric)\nn &lt;- nrow(cor_mat)\nfor (i in 1:(n-1)){\n  for (j in ((i+1):n)){\n    print(paste(i, j))\n  }\n}\n#&gt; [1] \"1 2\"\n#&gt; [1] \"1 3\"\n#&gt; [1] \"1 4\"\n#&gt; [1] \"2 3\"\n#&gt; [1] \"2 4\"\n#&gt; [1] \"3 4\"\n\nNext, we need to add an if statement to check whether there is a correlation above the given threshold. In this case to check if our code is working correctly, we use a print statement for only those that meet the condition.\n\nn &lt;- nrow(cor_mat)\nfor (i in 1:(n-1)){\n  for (j in ((i+1):n)){\n    if(abs(cor_mat[i,j]) &gt; 0.6){\n      print(paste(i, j))\n    }\n  }\n}\n#&gt; [1] \"1 2\"\n\nWe have sketched out our code more thoroughly so we have a good idea of how we want to compute our result. We now move to writing our function. Importantly, we need to make sure that we use our input arguments now rather than our test values. In the subsequent version, we also add in a results data frame that we use to keep track of our results and we add an additional argument for how to deal with NA values in calculating the correlations. The output matches what we expect for our test data frame.\n\n#' Find pairs of columns with strong correlation\n#' \n#' @description Finds all pairs of numeric columns with strong Pearson \n#' correlation and returns the pairs in a data frame\n#' \n#' @param df data frame\n#' @param threshold positive numeric threshold value to define a \n#' strong correlation as one with absolute value above the threshold\n#' @param use an optional character string giving a method for \n#' computing correlations in the presence of missing values. \n#' This must be (an abbreviation of) one of the strings \"everything\", \n#' \"all.obs\", \"complete.obs\", \"na.or.complete\", or \n#' \"pairwise.complete.obs\".\n#' @return data frame with one row for each pair of columns\n#' with high correlation containing the names of the columns \n#' and the corresponding correlation\nhigh_cor &lt;- function(df, threshold = 0.6, use = \"everything\"){\n  \n  # create result data frame\n  res &lt;- data.frame(name1 = vector(\"character\"),\n                    name2 = vector(\"character\"),\n                    cor = vector(\"numeric\"))\n  \n  # subset to numeric columns\n  df_numeric &lt;- select_if(df, is.numeric)\n  \n  # find correlations and variable names\n  cor_mat &lt;- cor(df_numeric, use = use)\n\n  # go through pairs to find those with high correlations\n  n &lt;- nrow(cor_mat)\n  for (i in 1:(n-1)){\n    for (j in ((i+1):n)){\n      if(abs(cor_mat[i,j]) &gt; threshold){\n        res &lt;- add_row(res, \n                       name1 = colnames(cor_mat)[i],\n                       name2 = colnames(cor_mat)[j],\n                       cor = cor_mat[i,j])\n      }\n    }\n  }\n  return(res)\n}\nhigh_cor(test_df)\n#&gt;   name1 name2   cor\n#&gt; 1    V1    V2 0.901\n\nWe can prevent unexpected behavior of our functions by using stop() functions to limit a function to be run on certain types of arguments. This is helpful if other people will use your function or if you might forget any assumptions you built into the function. The stop() function stops the execution of the current expression and returns a message. In the following example, we check to make sure that the point given is a numeric vector. Further, we check to see whether the vector has length zero and return 0 if it does.\n\n#' Distance to the origin\n#' \n#' @description Calculates the distance from a single numeric vector \n#' to the origin\n#'\n#' @param pt1 numeric vector \n#' @param dist_fun function to compute the distance with, default is\n#' Euclidean distance\n#' @return the distance from pt1 to origin in the same dimension\ndist_to_origin &lt;- function(pt1, dist_func = euclidean_dist){\n  \n  # check format of input\n  if(!(is.vector(pt1) & is.numeric(pt1))){\n    stop(\"pt1 must be a numeric vector\")\n  }\n  if(length(pt1) == 0){\n    return(0)\n  }\n  \n  # calculate the distance\n  origin &lt;- rep(0, length(pt1))\n  return(dist_func(pt1, origin))\n}\n\n\n18.3.1 Unit tests\nWe know our function high_cor() works on a single example. To thoroughly test our functions, we want to run them on several different input values. These types of tests are called unit tests. We try to vary these test values to cover a wide range of possibilities. For example, for a numeric argument, test positive and negative input values. For a vector input, test an empty vector, a vector of length one, and a vector with multiple values. If you discover an error, we need to go back to debugging mode to resolve it.\nTo test our function, we use the testthat package (Wickham 2011). This includes several functions that can check our expectations. For example, there is the expect_equal(object, expected) function which checks whether object matches expected up to a given numeric tolerance. If we want to only check the values of our objects but not the attributes, we can set the argument ignore_attr = FALSE. Other functions from this package are expect_error(object) which can be used to test that an error message was returned and expect_true(object) which can be used to test whether a condition is met.\n\ntestthat::expect_equal(paste0(\"A\",\"B\"), \"AB\")\ntestthat::expect_true(mean(c(1,2,3)) &gt; 1)\n\nIn the following code chunk, we demonstrate some tests for the dist_to_origin() function. Our tests here focus on the format of the vector. We should test each function separately so we would write a separate batch of tests for the Euclidean distance function.\n\n# check error message for character vector\nexpect_error(dist_to_origin(c(\"A\")), \n                       \"pt1 must be a numeric vector\")\n\n# check error message for not a vector\nexpect_error(dist_to_origin(matrix(0)), \n                       \"pt1 must be a numeric vector\")\n\n# check for empty numeric vector\ntestthat::expect_equal(dist_to_origin(vector(\"numeric\")), 0)\n\n# check length 1 vector\ntestthat::expect_equal(dist_to_origin(c(2)), 2)\n                       \n# check length 3 vector\ntestthat::expect_equal(dist_to_origin(c(2,8.5,3)), 9.233093,\n                       tolerance = 0.0001)\n\n\n\n18.3.2 Practice Question\nFollowing are a series of tests for the function high_cor() with different data frame sizes and types of columns. Unfortunately, not all the tests are working. Use your debugging skills to fix the function to pass the tests and write at least one additional test.\n\n# mixed data frame - should return a data frame with three columns\nexpect_equal(high_cor(test_df), \n             data.frame(name1 = \"V1\", name2 = \"V2\",\n                        cor = 0.9011631), tolerance = 0.001)\n\n# change threshold - lower, should have three pairs\nexpect_equal(high_cor(test_df, 0.2)$cor, \n             c(0.9011631, 0.31765407, 0.30748633), \n             tolerance = 0.001)\n\n# change threshold - higher, should be empty\nexpect_equal(high_cor(test_df, 0.95)$cor, vector(\"numeric\"), \n             tolerance = 0.001)\n\n# single numeric column - should return an empty data frame\nexpect_equal(high_cor(test_df[,4:5])$cor, vector(\"numeric\"))\n\n# single row - should return an empty data frame\nexpect_equal(high_cor(test_df[1,])$cor, numeric())",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "book/functions.html#exercises",
    "href": "book/functions.html#exercises",
    "title": "18  Functions",
    "section": "18.4 Exercises",
    "text": "18.4 Exercises\nFor each question, be sure to document your function(s) using roxygen style documentation.\n\nStandardizing a variable means subtracting the mean and then dividing through by the standard deviation. Create a function called standardize_me() that takes a numeric vector as an argument, and returns the standardized version of the vector. Write at least three unit tests to check that the result is correct.\nSuppose we have two binary vectors x and y each of length \\(n\\). Let \\(m_{1}\\) be the number of indices where x or y has a 1 and \\(m_2\\) be the number of indices where both x and y equal 1. For example, if x &lt;- (1,1,0) and y &lt;- c(0,1,0) then \\(m_1 = 2\\) and \\(m_2 =1\\). The Jaccard distance is defined as \\(1-m_2/m_1\\) and measures the dissimilarity between two binary vectors. Write a function jaccard_dist() that takes in two binary vectors and returns the Jaccard distance between the two.\nFor this question we use a subset of the pain data from the HDSinRdata package. Recall, that this data contains binary variables representing where people experienced pain. Write a function that takes in a data frame with all binary columns and returns a matrix with the Jaccard distance between all observations. That is, if D is the returned matrix, then D[i,j] is the Jaccard distance between observation i and observation j. Apply your function to the pain data and plot the distribution of these distances.\n\nlibrary(HDSinRdata)\npain_sub &lt;- pain[1:500,]\n\nThe function in the following code chunk is supposed to take in a positive integer and calculate how many positive integer divisors it has (other than 1 and itself). However, the function is not getting the right results. Debug the function. Then, think about ways you could improve this function by changing the structure, documentation, and adding argument checks.\n\ntotal &lt;- 0\ndivisors &lt;- function(x){\n  for(i in 1:x){\n    if (i %% x){\n      total &lt;- total + 1\n      i &lt;- i + 1\n    }\n  }\n  return(total)\n  }\n\ndivisors(2)\n#&gt; [1] 1\ndivisors(6)\n#&gt; [1] 5\n\nIn this problem, you create our own summary table. To start, create a function that takes in a data frame and returns a summary table that reports the mean and standard deviation for all continuous variables and the count and percentage for all categorical variables. An example is given in Figure 18.1. Call your function on the NHANES dataset with the columns selected in the subsequent code to match what is shown in the figure. \n\nnhanes_df &lt;- NHANESsample %&gt;%\n  select(c(AGE, SEX, LEAD, HYP, SMOKE))\n\n\n\n\n\n\n\n\nFigure 18.1: Example Summary Table.\n\n\n\n\n\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nWickham, Hadley. 2011. “Testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "book/cs_simulation.html#outlining-our-approach",
    "href": "book/cs_simulation.html#outlining-our-approach",
    "title": "19  Case Study: Designing a Simulation Study",
    "section": "19.1 Outlining Our Approach",
    "text": "19.1 Outlining Our Approach\nBefore coding our method, let’s recap the steps we will need to perform for a single simulation and practice top-down programming. In a single simulation, we need to generate our training and test data, fit our models, and store the results in a way that we can use later. There are two potential sketches of how we can program this code shown in Figure 19.1. Take a look at the differences. In the first, we are storing the data we generate and have a separate function for each method. In the second, we have a function that calculates our end metrics for an inputted model and we have a function that runs through the different methods. A benefit of the first approach is that it will be more flexible; if we think of another method we want to compare we would be easily able to add it without having to re-run any other code. A benefit of the second approach is that we are ensuring that the results stored for each method are the same. Of course, you could also do a hybrid between the two and use a metrics function in the first approach.\n\n\n\n\n\n\nFigure 19.1: Example Approaches to Sketching out Functions.\n\n\n\nLet’s take a closer look at the second approach. For our metrics function, we have missed some inputs we will need. In particular, in order to calculate our end metrics, we will need to know the true coefficients \\(\\beta\\), the covariance matrix \\(\\Sigma\\), and the level of noise \\(\\sigma\\). Therefore, rather than returning a data frame, we will return a list that will contain \\(X\\), \\(y\\), and these values. For the first approach, this would require saving this information in a text file. Comparing between our two options, we will implement the second approach that does not store data. Another thing we can notice in our current sketch is that we likely want to store the results as a csv file, so rather than returning a list, we should return a vector that will correspond to a row in this file. Our final sketch is in Figure 19.2.\n\n\n\n\n\n\nFigure 19.2: Updated Function Sketch.",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Case Study: Designing a Simulation Study</span>"
    ]
  },
  {
    "objectID": "book/cs_simulation.html#coding-our-simulation-study",
    "href": "book/cs_simulation.html#coding-our-simulation-study",
    "title": "19  Case Study: Designing a Simulation Study",
    "section": "19.2 Coding Our Simulation Study",
    "text": "19.2 Coding Our Simulation Study\nWe first load in the packages we will use. We will use the MASS package for the mvrnorm() function, which generates data from a multivariate normal distribution, we will use the glmnet package to implement our lasso and ridge models, and we will use the tidyverse and patchwork packages for summarizing and plotting the results.\n\nlibrary(MASS) \nlibrary(tidyverse) \nlibrary(patchwork) \nlibrary(glmnet)  \n\nWe start by writing our function to generate our data. Our input here will be the parameters \\(n\\), \\(p\\), \\(s\\), \\(\\rho\\), and \\(\\nu\\) and our output will be a list.\n\n#' Simulate data\n#'\n#' @param n Number of observations\n#' @param p Number of variables\n#' @param s Sparsity level (number of nonzero coefficients)\n#' @param snr Signal-to-noise ratio\n#' @param rho Predictor correlation level\n#' @return List containing simulated covariate matrix `X`, \n#' outcome vector `y`, true coefficient vector `beta`, \n#' covariate matrix `Sigma`, and variance of y `sigma`\nsimulate_data &lt;- function(n, p, s, snr, rho) {\n  \n  # Generate covariance matrix\n  cov_mat = matrix(0, nrow = p, ncol = p)\n  for (row in 1:p) {\n    for (col in 1:p) {\n      cov_mat[row, col] = rho^(abs(row-col))\n    }\n  }\n  \n  # Generate X \n  x &lt;- mvrnorm(n=n, mu=rep(0,p), Sigma = cov_mat)\n\n  # Generate beta values \n  b &lt;- rep(0, p)\n  b[1:s] &lt;- 1\n \n  # find values\n  mu &lt;- x %*% b\n  intercept &lt;- -mean(mu)\n  \n  # Calculate variance\n  var &lt;- as.numeric((t(b) %*% cov_mat %*% b)/snr)\n\n  # Generate y values\n  y &lt;- mvrnorm(mu = mu, Sigma = var*diag(n))\n  \n  return(list(X = x, y = y, beta = b, Sigma = cov_mat, sigma = var))\n}\n\nNext, we will write a function for our model metrics. The only input we need from our model is the estimated coefficients. Otherwise, all of the information comes from the data we generate with the function we just wrote. We will utilize this list format to extract out the values needed for our formulas.\n\n#' Return model metrics\n#'\n#' @param coef_est Vector with estimated coefficients \n#' @param test_data Withheld test set (`simulate_data()` output)\n#' @return Vector with relative test error (RTE) and proportion \n#' of variance explained (PVE). \nget_metrics &lt;- function(coef_est, test_data) {\n  \n  # Extract out values needed\n  coef_true &lt;- test_data$beta\n  Sigma &lt;- test_data$Sigma\n  var_y &lt;- test_data$sigma\n\n  # Calculate relative test error\n  RTE &lt;- (t(coef_est - coef_true) %*% Sigma %*% \n            (coef_est - coef_true) + var_y) / \n            var_y\n    \n  # Calculate PVE\n  # Proportion of variance explained\n  PVE &lt;- 1 - (t(coef_est - coef_true) %*% Sigma %*%\n                (coef_est - coef_true) + var_y) / \n    (var_y + t(coef_true %*% Sigma %*% coef_true))\n\n  \n  return(c(RTE = RTE, PVE = PVE))\n}\n\nNext, we will write a function that takes in the given parameters, fits the two models, and outputs the evaluation metrics. In this case, we will let the parameters be a named vector that contains all the components needed for the data simulation. We will also include an optional argument to set the random seed. In the code below, we find the time it takes to fit each model using Sys.time(). This function finds the current system time. Therefore, we can find the difference between them using the difftime() function. We also make sure to format the lasso and ridge results in the same manner.\n\n#' Model selection simulation\n#'\n#' @param params named vector containing all parameters needed for \n#' data generation (rho, snr, n, p, s)\n#' @param seed (optional) random seed to set before setting folds, \n#' by default not used\n#' @return Vector with parameter values, results\nmodel_selection &lt;- function(params, seed = NULL) {\n  \n  # Extract out parameters\n  n &lt;- params['n']\n  p &lt;- params['p']\n  s &lt;- params['s']\n  snr &lt;- params['snr']\n  rho &lt;- params['rho']\n\n  # Generate training and test data\n  train &lt;- simulate_data(n, p, s, snr, rho)\n  test &lt;- simulate_data(n, p, s, snr, rho)\n  \n  # Set folds, if needed\n  if (!is.null(seed)){\n    set.seed(seed)\n  }\n  k &lt;- 5\n  folds &lt;- sample(1:k, nrow(train$X), replace=TRUE)\n  \n  # Lasso model\n  start_lasso &lt;- Sys.time()\n  lasso_cv &lt;- cv.glmnet(train$X, train$y, nfolds = k, foldid = folds, \n                           alpha = 1, family = \"gaussian\", \n                        intercept=FALSE) \n  lasso_mod &lt;- glmnet(train$X, train$y, lambda = lasso_cv$lambda.min,\n                      alpha = 1, family = \"gaussian\", intercept=FALSE)\n  end_lasso &lt;- Sys.time()\n  \n  # Get lasso results\n  lasso_time &lt;- as.numeric(difftime(end_lasso, start_lasso, \n                                    units = \"secs\"))\n  lasso_results &lt;- c(lasso_time, \n                     get_metrics(coef(lasso_mod)[-1], test))\n  names(lasso_results) &lt;- c(\"lasso_sec\", \"lasso_RTE\", \"lasso_PVE\")\n  \n  # Ridge model \n  start_ridge &lt;- Sys.time()\n  ridge_cv &lt;- cv.glmnet(train$X, train$y, nfolds = k, foldid = folds, \n                        alpha = 0, family = \"gaussian\", \n                        intercept=FALSE)\n  ridge_mod &lt;- glmnet(train$X, train$y, lambda = ridge_cv$lambda.min,\n                      alpha = 0, family = \"gaussian\", intercept=FALSE)\n  end_ridge &lt;- Sys.time()\n  \n  # Get ridge results\n  ridge_time &lt;- as.numeric(difftime(end_ridge, start_ridge, \n                                    units = \"secs\"))\n  ridge_results &lt;- c(ridge_time, \n                     get_metrics(coef(ridge_mod)[-1], test))\n  names(ridge_results) &lt;- c(\"ridge_sec\", \"ridge_RTE\", \"ridge_PVE\")\n  \n  # Full results\n  res &lt;- c(n, p, s, snr, rho, lasso_results, ridge_results)\n  \n  return(res)\n}",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Case Study: Designing a Simulation Study</span>"
    ]
  },
  {
    "objectID": "book/cs_simulation.html#results",
    "href": "book/cs_simulation.html#results",
    "title": "19  Case Study: Designing a Simulation Study",
    "section": "19.3 Results",
    "text": "19.3 Results\nNow it’s time to run our simulation! We first need to find the combinations of parameters we want to use in our simulation design. In our case, we will set \\(n = 500\\), \\(\\rho = 0.35\\), and \\(s = 10\\). We will vary \\(p \\in \\{50, 100\\}\\) and the signal-to-noise ratio \\(\\nu \\in \\{0.1, 0.5, 1.5\\}\\). We also want to run each possible combination of parameters ten times so that we can average across the results. We use the expand.grid() function to create a matrix that contains a row for each simulation.\n\n# Set up parameter grid\nrho_grid &lt;- c(0.35)\nsnr_grid &lt;- c(0.1, 0.5, 1.5)\nn_grid &lt;- c(500)\np_grid &lt;- c(50, 100)\ns_grid = c(10)\niter_grid &lt;- 1:5\nparam_grid &lt;- expand.grid(rho = rho_grid, snr = snr_grid, n = n_grid,\n                          p = p_grid, s = s_grid, iter = iter_grid)\n\n# convert to numeric\nparam_grid &lt;- as.matrix(param_grid)\nhead(param_grid)\n#&gt;       rho snr   n   p  s iter\n#&gt; [1,] 0.35 0.1 500  50 10    1\n#&gt; [2,] 0.35 0.5 500  50 10    1\n#&gt; [3,] 0.35 1.5 500  50 10    1\n#&gt; [4,] 0.35 0.1 500 100 10    1\n#&gt; [5,] 0.35 0.5 500 100 10    1\n#&gt; [6,] 0.35 1.5 500 100 10    1\n\nRecall that our main function took in a named vector that contained all needed parameters. This allows us to use an apply() function to run our simulation. In order to summarize by method, we pivot the results to a longer form with a column for method.\n\n# Run experiments\nresults &lt;- apply(param_grid, 1, model_selection) %&gt;% t() \n\n# Convert to long data frame\nresults &lt;- as.data.frame(results) %&gt;% \n  pivot_longer(cols = starts_with(c(\"lasso\", \"ridge\")),\n               names_to = c(\"method\", \".value\"), names_sep=\"_\")\n\nFinally, we summarize our results. For example, we can create a table with the average time for each method grouped by the data dimensions. We observe that ridge regression was slower on average than lasso.\n\navg_time &lt;- results %&gt;%\n  group_by(method, n, p) %&gt;%\n  summarize(avg_seconds = round(mean(sec),3)) %&gt;%\n  ungroup()\navg_time\n#&gt; # A tibble: 4 × 4\n#&gt;   method     n     p avg_seconds\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 lasso    500    50       0.019\n#&gt; 2 lasso    500   100       0.03 \n#&gt; 3 ridge    500    50       0.022\n#&gt; 4 ridge    500   100       0.048\n\nWe can also create summary plots of our evaluation metrics similar to Hastie, Tibshirani, and Tibshirani (2020). To do so, we create one last function that will create a plot of the relative test error and percentage of variance explained across different signal-to-noise ratios. This allows us to regenerate this plot for different parameter settings.\n\n#' Generate RTE and PVE plots for a given set of parameters\n#'\n#' @param results Data frame with simulation results  \n#' @param n_input Number of observations\n#' @param p_input Number of variables\n#' @param s_input Sparsity level\n#' @return ggplot object\ngenerate_plot &lt;- function(results, n_input, p_input, s_input) {\n \n  setting &lt;- results %&gt;%\n    filter(n == n_input, p == p_input, s == s_input) %&gt;%\n    group_by(method, snr) %&gt;%\n    summarize(mean_RTE = mean(RTE, na.rm = TRUE), \n              sd_RTE = sd(RTE, na.rm = TRUE),\n              mean_PVE = mean(PVE, na.rm = TRUE), \n              sd_PVE = sd(PVE, na.rm = TRUE))\n\n  rte_plot &lt;- ggplot(setting) + \n    geom_point(aes(x = snr, y = mean_RTE, color = method)) + \n    geom_errorbar(aes(x = snr, ymin = mean_RTE - sd_RTE, \n                      ymax = mean_RTE + sd_RTE, color = method),\n                  alpha = 0.8, width = 0.2) + \n    geom_line(aes(x = snr, y = mean_RTE, color = method)) + \n    theme_bw() + \n    theme(legend.position = \"bottom\") + \n    labs(x = \"SNR\", y = \"RTE\", color = \"\")\n  \n  \n  pve_plot &lt;- ggplot(setting) + \n    geom_point(aes(x = snr, y = mean_PVE, color = method)) + \n    geom_errorbar(aes(x = snr, ymin = mean_PVE - sd_PVE, \n                      ymax = mean_PVE + sd_PVE, color = method),\n                  alpha = 0.8, width = 0.2) + \n    geom_line(aes(x = snr, y = mean_PVE, color = method)) + \n    theme_bw() + \n    theme(legend.position = \"bottom\") + \n    labs(x = \"SNR\", y = \"PVE\", color = \"\")\n\n  full_plot &lt;- rte_plot + pve_plot \n  \n  return(full_plot)\n}\ngenerate_plot(results, 500, 50, 10)\n\n\n\n\n\n\n\n\n\ngenerate_plot(results, 500, 100, 10)\n\n\n\n\n\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Ryan Tibshirani. 2020. “Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons.” Statistical Science 35 (4).",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Case Study: Designing a Simulation Study</span>"
    ]
  },
  {
    "objectID": "book/efficiency.html#use-fast-and-vectorized-functions",
    "href": "book/efficiency.html#use-fast-and-vectorized-functions",
    "title": "20  Writing Efficient Code",
    "section": "20.1 Use Fast and Vectorized Functions",
    "text": "20.1 Use Fast and Vectorized Functions\nR is known as a slower programming language. In part, R sacrifices computation time to make it more welcoming to new programmers. However, not all of R is actually written in R. Many functions that are in base R are actually written in C or Fortran. These functions are significantly faster than if we wrote them ourselves. When possible, we should use base R functions rather than writing our own. Let’s take a look at the difference in time when we write our own summation function compared to using the sum() function.\n\nmy_sum &lt;- function(x){\n  out &lt;- 0\n  for (i in 1:length(x)){\n    out &lt;- out + x[i]\n  }\n  return(out)\n}\n\nx &lt;- 1:100000\nmicrobenchmark(sum_function = my_sum(x),\n               builtin_sum = sum(x),\n               unit = \"ms\")\n#&gt; Unit: milliseconds\n#&gt;          expr      min       lq     mean   median       uq     max\n#&gt;  sum_function 2.193213 2.236509 2.352294 2.269289 2.342966 4.33112\n#&gt;   builtin_sum 0.000041 0.000082 0.000613 0.000082 0.000328 0.00701\n#&gt;  neval\n#&gt;    100\n#&gt;    100\n\nThis also simplifies our code to a single line. Looking back at our first example, we can also simplify our code. We did not need to loop through our data. Instead, we should use a vectorized approach that checks for pain improvement across all observations and then sum up the TRUE/FALSE values.\n\nmicrobenchmark({\n  vector_pain = sum(pain$PAIN_INTENSITY_AVERAGE &lt;= \n                      pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP)\n}, unit = \"ms\")\n#&gt; Unit: milliseconds\n#&gt;                                                                                             expr\n#&gt;  {     vector_pain = sum(pain$PAIN_INTENSITY_AVERAGE &lt;= pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP) }\n#&gt;     min     lq   mean median     uq    max neval\n#&gt;  0.0143 0.0145 0.0148 0.0146 0.0146 0.0263   100\n\nThis is much faster. Loops are notoriously slow in R so you often hear the advice to avoid loops. Using apply functions like apply() and sapply() are loop-hiding functions. Using these functions instead of an explicit loop doesn’t improve the efficiency of our code except that by using these functions we often simplify our code as a byproduct of rewriting our code.\nThe true functions that can improve efficiency over loops are vectorized functions, which can be evaluated on a vector of values. Vectorizing our code means that we are thinking about a vector approach rather than computing something for each element of the vector and looping through these values. A vectorized function returns output that is the same dimensions as the input and operates on these elements in an efficient manner.\nThe following code chunk shows a simple example of comparing taking the square root of each individual element or calling the sqrt() function, which is a vectorized function. Vectorized functions still have to operate on each element, but that loop is often written in C or Fortran rather than R making it significantly faster so this is a special case where we can utilize the speed of some functions.\n\nmicrobenchmark(loop_sqrt = {\n  for (i in 1:length(x)){\n    x[i] &lt;- sqrt(x[i])\n  }\n  }, sqrt = sqrt(x), unit = \"ms\")\n#&gt; Unit: milliseconds\n#&gt;       expr  min    lq  mean median    uq   max neval\n#&gt;  loop_sqrt 4.43 4.508 4.701  4.574 4.775 7.010   100\n#&gt;       sqrt 0.10 0.136 0.151  0.145 0.158 0.281   100\n\n\n20.1.1 Practice Question\nFirst, read the documentation of the function tapply(). This is another function in the apply function library that was not covered in Chapter 17. Rewrite the following code without using a loop using the tapply() function. Would you expect this approach to be faster? Why or why not?\n\nmean_pain &lt;- c()\npat_races &lt;- unique(pain$PAT_RACE)\n\nfor (r in pat_races){\n  mean_pain[r] &lt;- mean(pain$PAIN_INTENSITY_AVERAGE[pain$PAT_RACE == r])\n}",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Writing Efficient Code</span>"
    ]
  },
  {
    "objectID": "book/efficiency.html#avoid-copies-and-duplicates",
    "href": "book/efficiency.html#avoid-copies-and-duplicates",
    "title": "20  Writing Efficient Code",
    "section": "20.2 Avoid Copies and Duplicates",
    "text": "20.2 Avoid Copies and Duplicates\nAnother aspect of our programs that can slow down our operations is any time we need to create a large object. Look at the following code. First, we create a matrix m. We then create a matrix n that is equal to m. Last, we update n by taking the logarithm of all elements plus one, differentiating it from m. R creates copies upon modification. That means, that when we initialize n we have not actually created a second matrix in memory. Instead, we have two names for the same matrix. On the third line, we want to update n so we need to actually create a second matrix that is different from m.\n\nm &lt;- matrix(rpois(100000, 6), ncol=1000)\nn &lt;- m\nn &lt;- log(n + 1)\n\nThis is different from the subsequent code which updates m itself. In this case, R can modify the matrix in place by going through each element and updating its value rather than creating a new matrix. As the size of our data grows, creating copies can be expensive. Imagine, m being a large genetic data set with RNA sequencing data. First, this object may take up a lot of memory so creating a copy may mean we could run out of memory. Second, copying over all this information is expensive.\n\nm &lt;- log(m + 1)\n\nLet’s consider another case. In the following code, we have two functions that both find the proportion of people within 1, 2, and \\(&gt;2\\) standard deviations from the mean for one of the PROMIS instrument variables. When we input pain as an argument to the first function, we do not create a copy of it since we haven’t modified the data frame. However, once we create a new column, this means that we have to copy the full data frame. The second function instead takes in a single column, requiring us to copy only this information. The difference in execution time shows an edge to the second method, but the difference is small. This indicates that actually computing this new column and finding the proportions takes more time than the duplication.\n\ncode_promis1 &lt;- function(df){\n  # create new column with categories\n  df$PAIN_PHYSICAL_FUNCTION_CUT &lt;- case_when(\n    abs(df$PROMIS_PHYSICAL_FUNCTION-50) &lt;= 10 ~ \"&lt;= 1SD\",\n    abs(df$PROMIS_PHYSICAL_FUNCTION-50) &lt;= 20 ~ \"&lt;= 2 SD\",\n    TRUE ~ \"&gt; 2SD\")\n  \n  # get proportions\n  res &lt;- prop.table(table(df$PAIN_PHYSICAL_FUNCTION_CUT))\n  return(res)\n}\n\ncode_promis2 &lt;- function(v){\n  # create new column with categories\n  v &lt;- case_when(\n    abs(v-50) &lt;= 10 ~ \"&lt;= 1SD\",\n    abs(v-50) &lt;= 20 ~ \"&lt;= 2 SD\",\n    TRUE ~ \"&gt; 2SD\")\n  \n  # get proportions\n  res &lt;- prop.table(table(v))\n  return(res)\n}\n\nmicrobenchmark(code_promis1(pain), \n               code_promis2(pain$PROMIS_PHYSICAL_FUNCTION), \n               unit = \"ms\")\n#&gt; Unit: milliseconds\n#&gt;                                         expr   min    lq  mean median\n#&gt;                           code_promis1(pain) 0.662 0.695 0.799  0.710\n#&gt;  code_promis2(pain$PROMIS_PHYSICAL_FUNCTION) 0.629 0.670 0.760  0.683\n#&gt;     uq  max neval\n#&gt;  0.741 3.57   100\n#&gt;  0.704 3.24   100\n\nAnother time when we create copies of objects is when we modify their size. Functions like cbind(), rbind(), and c() create a new object that needs to copy over information to create one vector, matrix, or data frame. If we know the size of the final vector, matrix, or data frame, we can pre-allocate that space and fill in values. This means that the computer won’t have to repeatedly find more space. For example, take a look at the two ways to simulate a random walk in one dimension in the following code chunk. In the first method, the length of the vector v changes on each iteration of the loop whereas in the second v always has length n.\n\nrw1 &lt;- function(n){\n  v &lt;- c(0)\n  for (i in 2:n){\n    v[i] &lt;- v[i-1] + rbinom(1, 1, 0.5)\n  }\n  return(v)\n}\n\nrw2 &lt;- function(n){\n  v &lt;- rep(0, n)\n  for (i in 2:n){\n    v[i] &lt;- v[i-1] + rbinom(1, 1, 0.5)\n  }\n}\n\nmicrobenchmark(rw1(10000), rw2(10000), unit=\"ms\")\n#&gt; Unit: milliseconds\n#&gt;        expr  min   lq mean median   uq   max neval\n#&gt;  rw1(10000) 5.17 5.29 5.63   5.35 5.45 10.37   100\n#&gt;  rw2(10000) 4.29 4.34 4.58   4.38 4.44  6.54   100\n\nThis also works for data frames or matrices. In the following code chunk, we generate a random matrix in three ways. The first creates the matrix with a single line, the second initializes an empty matrix and then fills in each row, and the last dynamically updates the size of the matrix on each iteration.\n\nrandom_mat1 &lt;- function(n){\n  m &lt;- matrix(sample(1:3, n^2, replace = TRUE), nrow = n)\n  return(m)\n}\n\nrandom_mat2 &lt;- function(n){\n  m &lt;- matrix(nrow = n, ncol = n)\n  for (i in 1:n){\n    m[i,] &lt;- sample(1:3, n, replace = TRUE)\n  }\n}\n\nrandom_mat3 &lt;- function(n){\n  m &lt;- NULL\n  for (i in 1:n){\n    m &lt;- rbind(m, sample(1:3, n, replace = TRUE))\n  }\n  return(m)\n}\n\nmicrobenchmark(random_mat1(100),\n               random_mat2(100),\n               random_mat3(100),\n               unit = \"ms\")\n#&gt; Unit: milliseconds\n#&gt;              expr   min    lq  mean median    uq  max neval\n#&gt;  random_mat1(100) 0.233 0.240 0.289  0.243 0.251 3.49   100\n#&gt;  random_mat2(100) 0.503 0.514 0.543  0.518 0.530 2.25   100\n#&gt;  random_mat3(100) 1.065 1.148 1.330  1.188 1.213 4.94   100\n\nThis demonstrates that if you need to update the values of a vector, matrix, or data frame, try to do as much reassignment at once. For example, changing the a whole column at a time is better than changing the individual values. This avoids additional copies.\n\n20.2.1 Practice Question\nThe following code fits a linear model for each racial group and records the coefficient. Rewrite this code so that we pre-allocate the results vector and use the microbenchmark to compare the efficiency between the two approaches.\n\ncoefs &lt;- c()\npat_races &lt;- unique(pain$PAT_RACE)\nfor (r in pat_races){\n  df &lt;- pain[pain$PAT_RACE == r, ]\n  if (nrow(df) &gt; 3){\n    new_coef &lt;- lm(PROMIS_DEPRESSION ~ PROMIS_ANXIETY,\n                 data = df)$coefficients[2]\n    coefs &lt;- c(coefs, new_coef)\n  }\n}",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Writing Efficient Code</span>"
    ]
  },
  {
    "objectID": "book/efficiency.html#parallel-programming",
    "href": "book/efficiency.html#parallel-programming",
    "title": "20  Writing Efficient Code",
    "section": "20.3 Parallel Programming",
    "text": "20.3 Parallel Programming\nAnother approach to make our code more efficient is using parallel processing. When we run loops in R, only one iteration is run at a time. For example, the following code runs a random walk 100 times in serial. Parallel processing allows us to execute multiple calls to this function at the same time. This is done by running these processes on separate cores, or processors, on your machine. For example, if we had six cores available we would be able to run 1/6 of the replications on each processor and reduce our overall computation time. The parallel package (R Core Team 2024) contains functions to implement parallel processing on different operating systems. Unfortunately, this functionality is often not supported within RStudio and is not covered in this book. We recommend using the mclapply() function from the parallel package to implement parallel processing using forking. This does not work on Windows but is much simpler to implement. For parallel processing on Windows, we recommend looking into the socket approach using the parLapply() function in the parallel package.\n\nreplicate(100, rw1(1000))",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Writing Efficient Code</span>"
    ]
  },
  {
    "objectID": "book/efficiency.html#exercises",
    "href": "book/efficiency.html#exercises",
    "title": "20  Writing Efficient Code",
    "section": "20.4 Exercises",
    "text": "20.4 Exercises\n\nThe following code chunk includes four attempts to create a new column LOWER_BACK to the pain data. Note that the second and third attempt are vectorized whereas the first and fourth are not. Time each approach and order them from fastest to slowest.\n\n# Attempt 1: loop\npain$LOWER_BACK &lt;- vector(mode=\"logical\", length=nrow(pain))\nfor (i in 1:nrow(pain)) { # for every row\n  if ((pain$X218[i] == 1) | (pain$X219[i] == 1)) { \n    pain$LOWER_BACK[i] &lt;- TRUE \n  } else {\n    pain$LOWER_BACK[i] &lt;- FALSE\n  }\n}\n\n# Attempt 2: logic\npain$LOWER_BACK &lt;- ((pain$X218[i] == 1) | \n                      (pain$X219[i] == 1))\n\n# Attempt 3: which\npain$LOWER_BACK &lt;- FALSE\ntrue_ind &lt;- which((pain$X218[i] == 1) | (pain$X219[i] == 1))\npain$LOWER_BACK[true_ind] &lt;- TRUE\n\n# Attempt 4: apply\nback_pain &lt;- function(x){\n  if((x['X218'] == 1) | (x['X219'] == 1)){\n    return(TRUE)\n    } \n  return(FALSE)\n}\npain$BACK_PAIN &lt;- apply(pain, 1, back_pain)\n\nExamine the following code and determine what is being computed. Then, rewrite the code to make it more efficient. Use the microbenchmark package to compare the execution time, and explain why your approach is more efficient.\n\nn &lt;- 100000\nx1 &lt;- rnorm(n, 10, 1)\nx2 &lt;- rbinom(n, 1, 0.2)\ny &lt;- numeric(0)\n\nfor (i in 1:n){\n  if (x2[i] == 1){\n    y[i] &lt;- rnorm(1,2 *x1[i], 0.7)\n  } else{\n    y[i] &lt;- rnorm(1, 1+3*x1[i], 0.2)\n  }\n}\ndf &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\nSuppose we want to find the five most frequently reported pain regions by racial group. Code your solution (a) using at least one loop and (b) pivoting the data on the body region columns and then using dplyr functions to summarize. Compare the efficiency of both approaches.\n\n\n\n\n\nMersmann, Olaf. 2023. Microbenchmark: Accurate Timing Functions. https://CRAN.R-project.org/package=microbenchmark.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Writing Larger Programs",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Writing Efficient Code</span>"
    ]
  },
  {
    "objectID": "book/expanding_r_skills.html#reading-documentation-for-new-packages",
    "href": "book/expanding_r_skills.html#reading-documentation-for-new-packages",
    "title": "21  Expanding your R Skills",
    "section": "21.1 Reading Documentation for New Packages",
    "text": "21.1 Reading Documentation for New Packages\nAs you start to apply the tools from this book to your own work or in new settings, you may need to install and use new packages or encounter some unexpected errors. Practicing reading package documentation and responding to error messages helps you expand your R skills beyond the topics covered here. We demonstrate these skills using the stringr package (Wickham 2022), which is a package that is part of the tidyverse and has several functions for dealing with text data.\n\nlibrary(tidyverse)\nlibrary(HDSinRdata)\n\nEvery published package has a CRAN website. This website contains a reference manual that contains the documentation for the functions and data available in the package. Most often, the website also includes useful vignettes that give examples of how to use the functions in the package. The site also tells you what the requirements for using the package are, who the authors of the package are, and when the package was last updated. For example, take a look at the CRAN site for stringr and read the vignette “Introduction to String R”.\nWe use the stringr package to demonstrate cleaning up text related to a PubMed search query for a systematic review. An example search query is given in the following code chunk and is taken from Gue et al. (2021). Our first goal is to extract the actual search query from the text along with all the terms used in the query. We can assume that the search query is either fully contained in parentheses or is a sequence of parenthetical phrases connected with AND or OR. Our goal is to extract the search query as well as all the individual search terms used in the query, but we have to get there in a series of steps.\n\nsample_str &lt;- \" A systematic search will be performed in PubMed, \nEmbase, and the Cochrane Library, using the following search query:   \n('out-of-hospital cardiac arrest' OR 'OHCA') AND ('MIRACLE 2' OR \n'OHCA' OR 'CAHP' OR 'C-GRAPH' OR 'SOFA' OR 'APACHE' OR 'SAPS’ OR \n’SWAP’ OR ’TTM’).\"\n\nThe first thing we want to do with the text is clean up the white space by removing any trailing, leading, or repeated spaces. In our example, the string starts with a trailing space and there are also multiple spaces right before the search query. Searching for “white space” in the stringr reference manual, we find the str_trim() and str_squish() functions. Read the documentation for these two functions. You should find that str_squish() is the function we are looking for and that it takes a single argument.\n\nsample_str &lt;- str_squish(sample_str)\nsample_str\n#&gt; [1] \"A systematic search will be performed in PubMed, Embase, and the Cochrane Library, using the following search query: ('out-of-hospital cardiac arrest' OR 'OHCA') AND ('MIRACLE 2' OR 'OHCA' OR 'CAHP' OR 'C-GRAPH' OR 'SOFA' OR 'APACHE' OR 'SAPS’ OR ’SWAP’ OR ’TTM’).\"",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Expanding your R Skills</span>"
    ]
  },
  {
    "objectID": "book/expanding_r_skills.html#trying-simple-examples",
    "href": "book/expanding_r_skills.html#trying-simple-examples",
    "title": "21  Expanding your R Skills",
    "section": "21.2 Trying Simple Examples",
    "text": "21.2 Trying Simple Examples\nThe premise of testing a function on a single string is a good example of starting with a simple case. Rather than applying your function to your full dataset right away, you want to first make sure that you understand how it works on a simple example on which you can anticipate what the outcome should look like. Our next task is to split the text into words and store this as a character vector. Read the documentation to determine why we use the str_split_1() function. We then double check that the returned result is indeed a vector and print the result.\n\nsample_str_words &lt;- str_split_1(sample_str, \" \")\nclass(sample_str_words)\n#&gt; [1] \"character\"\nsample_str_words\n#&gt;  [1] \"A\"                 \"systematic\"        \"search\"           \n#&gt;  [4] \"will\"              \"be\"                \"performed\"        \n#&gt;  [7] \"in\"                \"PubMed,\"           \"Embase,\"          \n#&gt; [10] \"and\"               \"the\"               \"Cochrane\"         \n#&gt; [13] \"Library,\"          \"using\"             \"the\"              \n#&gt; [16] \"following\"         \"search\"            \"query:\"           \n#&gt; [19] \"('out-of-hospital\" \"cardiac\"           \"arrest'\"          \n#&gt; [22] \"OR\"                \"'OHCA')\"           \"AND\"              \n#&gt; [25] \"('MIRACLE\"         \"2'\"                \"OR\"               \n#&gt; [28] \"'OHCA'\"            \"OR\"                \"'CAHP'\"           \n#&gt; [31] \"OR\"                \"'C-GRAPH'\"         \"OR\"               \n#&gt; [34] \"'SOFA'\"            \"OR\"                \"'APACHE'\"         \n#&gt; [37] \"OR\"                \"'SAPS’\"            \"OR\"               \n#&gt; [40] \"’SWAP’\"            \"OR\"                \"’TTM’).\"\n\nWe now want to identify words in this vector that have starting and/or end parentheses. The function grepl() takes in a character vector x and a pattern to search for. It returns a logical vector for whether or not each element of x has a match for that pattern.\n\ngrepl(sample_str_words, \")\")\n#&gt; Warning in grepl(sample_str_words, \")\"): argument 'pattern' has length\n#&gt; &gt; 1 and only the first element will be used\n#&gt; [1] FALSE\n\nThat didn’t match what we expected. We expected to have multiple TRUE/FALSE values outputted - one for each word. Let’s read the documentation again.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Expanding your R Skills</span>"
    ]
  },
  {
    "objectID": "book/expanding_r_skills.html#deciphering-error-messages-and-warnings",
    "href": "book/expanding_r_skills.html#deciphering-error-messages-and-warnings",
    "title": "21  Expanding your R Skills",
    "section": "21.3 Deciphering Error Messages and Warnings",
    "text": "21.3 Deciphering Error Messages and Warnings\nThe previous warning message gives us a good clue for what went wrong. It says that the inputted pattern has length &gt; 1. However, the pattern we gave it is a single character. In fact, we specified the arguments in the wrong order. Let’s try again. This time we specify x and pattern.\n\ngrepl(x = sample_str_words, pattern = \")\")\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt; [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt; [23]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt; [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nThat fixed it. However, it won’t work if we change that to an opening parenthesis. Try it out for yourself to see this. The error message says that it is looking for an end parentheses. In this case, the documentation does not help us. Let’s try searching “stringr find start parentheses” using an online search engine. Our search results indicate that we may need to use backslashes to tell R to read the parentheses literally rather than as a special character used in a regular expression (a technique often referred to as “escaping” a character). Investigating the reason for an error, including using online material, is an important skill for a programmer to have.\n\ngrepl(x = sample_str_words, pattern = \"\\\\(\")\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt; [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#&gt; [23] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt; [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nWhen a function doesn’t return what we expect it to, it is a good idea to first test whether the arguments we gave it match what we expect, then re-read the documentation, and then look for other resources for help. For example, we could check that sample_str_words is indeed a character vector, then re-read the stringr documentation, and then search our problem.\n\n21.3.1 Debugging Code\nThe following code is supposed to extract the search query from the text as well as find the individual search terms used in the query. However, the code is incorrect. You can try out two test strings given to see why the code output is wrong. Practice reading through the code to understand what it is trying to do. The comments are there to help explain the steps, but you may also want to print the output to figure out what it is doing.\n\nsample_strA &lt;- \" A systematic search will be performed in PubMed, \nEmbase, and the Cochrane Library, using the following search query:   \n('out-of-hospital cardiac arrest' OR 'OHCA') AND ('MIRACLE 2' OR \n'OHCA' OR 'CAHP' OR 'C-GRAPH' OR 'SOFA' OR 'APACHE' OR 'SAPS’ OR \n’SWAP’ OR ’TTM’).\"\n\nsample_strB &lt;- \"Searches will be conducted in MEDLINE via PubMed, Web \nof Science, Scopus and Embase. The following search strategy will be \nused:(child OR infant OR preschool child OR preschool children OR \npreschooler OR pre-school child OR pre-school children OR pre school \nchild OR pre school children OR pre-schooler OR pre schooler OR \nchildren OR adolescent OR adolescents)AND(attention deficit disorder \nwith hyperactivity OR ADHD OR attention deficit disorder OR ADD OR \nhyperkinetic disorder OR minimal brain disorder) Submitted \"\n\n\nsample_str &lt;- sample_strB\n\n# separate parentheses, remove extra white space, and split into words\nsample_str &lt;- str_replace(sample_str, \"\\\\)\", \" \\\\) \")\nsample_str &lt;- str_replace(sample_str, \"\\\\(\", \" \\\\( \")\nsample_str &lt;- str_squish(sample_str)\nsample_str_words &lt;- str_split_1(sample_str, \" \")\n\n# find indices with parentheses \nend_ps &lt;- grepl(x = sample_str_words, pattern = \"\\\\)\")\nstart_ps &lt;- grepl(x = sample_str_words, pattern = \"\\\\(\")\n\n# find words between first and last parentheses \nsearch_query &lt;- sample_str_words[which(end_ps)[1]:which(start_ps)[1]]\nsearch_query &lt;- paste(search_query, collapse=\" \")\nsearch_query\n#&gt; [1] \") adolescents OR adolescent OR children OR schooler pre OR pre-schooler OR children school pre OR child school pre OR children pre-school OR child pre-school OR preschooler OR children preschool OR child preschool OR infant OR child (\"\n\n# find search terms\nsearch_terms &lt;- str_replace_all(search_query, \"\\\\)\", \"\")\nsearch_terms &lt;- str_replace_all(search_query, \"\\\\(\", \"\")\nsample_terms &lt;- str_squish(search_query)\nsearch_terms &lt;- str_split_1(search_terms, \" AND | OR \")\nsearch_terms\n#&gt;  [1] \") adolescents\"       \"adolescent\"          \"children\"           \n#&gt;  [4] \"schooler pre\"        \"pre-schooler\"        \"children school pre\"\n#&gt;  [7] \"child school pre\"    \"children pre-school\" \"child pre-school\"   \n#&gt; [10] \"preschooler\"         \"children preschool\"  \"child preschool\"    \n#&gt; [13] \"infant\"              \"child \"\n\n\n\n21.3.2 Video Solution\nWe shows how we can test and fix the previous code by using some simple debugging principles.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Expanding your R Skills</span>"
    ]
  },
  {
    "objectID": "book/expanding_r_skills.html#general-programming-tips",
    "href": "book/expanding_r_skills.html#general-programming-tips",
    "title": "21  Expanding your R Skills",
    "section": "21.4 General Programming Tips",
    "text": "21.4 General Programming Tips\nAs you write more complex code and functions, we want to focus on practicing good programming principles. This helps when you need to share or update your code or when you inevitably run into errors or unexpected behavior. Following are some general programming tips and how they relate to communication and debugging.\n\nConsistent naming. Use consistent and informative names for your objects and functions. For example, you can see that within this text we have only used lowercase letters, underscores (_), and occasionally numbers in our names. These names should also be informative and unique. This makes it easier to check for typos or duplicate names when debugging. When debugging, check that you haven’t used the same name for different objects or different names for the same object. You can do this by using the ls() function to find all current objects or by checking your environment pane.\n\n# Recommended\nages &lt;- c(65, 33, 27, 88)\nage_mean &lt;- mean(ages)\n\n# Not recommended\nx &lt;- c(65, 33, 27, 88)\nx1 &lt;- mean(ages)\n\nMake Your Code Readable. Readable code requires several elements of communication. As with writing an essay, we need to break our code into digestible and structured pieces. First, code should be broken into blocks, using white space to separate steps, and should use correct levels of indentation (one extra level of indentation for each new loop, if/else statement, or function). This means that closing curly braces should be on their own line indicating the end of the block. This makes it easy to check that all parentheses (), brackets [], and curly braces {} match. Additionally, you should use line breaks to avoid going over 80 characters on a single line of code. RStudio has an option to reformat or re-indent your code under the Code tab.\nBesides the structure of your code, writing helpful comments and function documentation are key for making your code readable. A good rule of thumb is to write comments for yourself a year from now - you might remember the project goal but you won’t remember what x represents. You likely do not need a comment for every line of code but you might need comments to explain the overall goal of a code block or to clarify lines that aren’t self-explanatory.\nIn the following code, we have not used indentation. This makes it hard to see the structure of the code such as what lines of code are in the loop or if statement. The function does not have any roxygen documentation. However, we have added too many comments. The comments here are repetitive with the code. Last, we named the function unique(), which is already a function in R.\n\n# Not recommended\nunique &lt;- function(x){\ny &lt;- c() # results\nif (length(x) == 0){ # check length 0\nreturn(NULL)} # return NULL\nfor(i in length(x)){  # loop through x\nif(!(x[i] %in% y)){  # check if x[i] in y\ny &lt;- c(y, x[i]) }} # add x[i] to y\nreturn(y) # return y\n}\n\nWe rewrite the function addressing these comments. The end result is much easier to read.\n\n#' Find unique elements of a vector \n#'\n#' @param x vector\n#' @return new vector with duplicates removed\nown_unique &lt;- function(x){\n\n  # check for empty vector\n  if (length(x) == 0){ \n    return(NULL)\n  }\n\n  # otherwise y will be unique values\n  y &lt;- c()\n\n  for(i in 1:length(x)){ \n    # if value of x is not in y, we add it\n    if(!(x[i] %in% y)){ \n      y &lt;- c(y, x[i]) \n    }\n  }\n\n  return(y) \n}\n\nDon’t Repeat Yourself. Repeating code increases the likelihood of errors. Additionally, it makes it hard to update our code later on. When we find ourselves repeating code, we should use a function. If you find yourself repeating constants you should define these values as an object. The subsequent code uses a single line of code to convert categorical variables to factors and stores a vector of which columns are categorical.\n\ndry_df &lt;- data.frame(age = ages,\n                    tb = c(1, 0, 0, 0),\n                    heart_rate = c(60, 82, 76, 72),\n                    gender = c(\"Female\", \"Male\", \"Nonbinary\", \n                               \"Female\"))\n\n# convert factor variables\ncat_vars &lt;- c(\"tb\", \"gender\")\ndry_df[cat_vars] &lt;- lapply(dry_df[cat_vars], factor)\n\nPractice Reading Documentation. Whenever we are using a new function, you should read the documentation first. When debugging, you should check that the input arguments to a function match what is expected and check the examples. Reading these examples also helps when writing your own documentation so you can better understand how to communicate to your audience.\nStart Simple, Build Up. If we write a large amount of code at once and then it fails to work, it’s hard to understand what went wrong. Instead, we should build up our code or functions in small steps and check it after each step. When it comes to testing code, a good mantra is test early and test often. So, try to avoid writing too much code before running and checking that the results match what you expect. If you do end up writing a big chunk of code, you can use localize your error by checking the values of objects at different points.\nGet Comfortable Asking for Help. In software engineering, it’s a known tip to have a rubber ducky (or other adorable object) at your desk to talk through your code. Having to verbalize and explain your approach can be really helpful for debugging. R’s error messages can sometimes hint at what the error might stem from, but they are not always direct. Searching for error messages you don’t understand might give you a better understanding of the problem.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Expanding your R Skills</span>"
    ]
  },
  {
    "objectID": "book/expanding_r_skills.html#exercises",
    "href": "book/expanding_r_skills.html#exercises",
    "title": "21  Expanding your R Skills",
    "section": "21.5 Exercises",
    "text": "21.5 Exercises\nThese exercises focus on reading function documentation and debugging.\n\nSuppose we want to replace the words “Thank you” in the following string with the word “Thanks”. Why does the following code fail? How can we correct it?\n\nstring &lt;- \"Congratulations on finishing the book! \nThank you for reading it.\"\nstr_sub(string, c(35, 42)) &lt;- \"Thanks\"\nstring\n#&gt; [1] \"Congratulations on finishing the bThanks\"        \n#&gt; [2] \"Congratulations on finishing the book! \\nTThanks\"\n\nThe subsequent code uses the NHANESsample data from the HDSinRdata package. The goal of the code is to plot the worst diastolic blood pressure reading against the worst systolic blood pressure reading for each patient, colored by hypertension status. However, the code currently generates an error message. What is wrong with the code? There are four errors for you to identify and fix.\n\ndata(NHANESsample)\n\nnhanes_df &lt;- NHANESsample %&gt;% \n  mutate(worst_DBP = max(DBP1, DBP2, DBP3, DBP4), \n     worst_SBP = max(SBP1, SBP2, SBP3, SBP4))\n\nggplot() %&gt;% \n  geom_point(data = nhanes_df, \n             aes(x = worst_SBP, y = worst_DBP), \n             color = HYP)\n\nThe following code uses the breastcancer data from the HDSinRdata package. The goal is to create a logistic regression model for whether or not the diagnosis is benign or malignant and then to create a calibration plot for the model, following the code from Chapter 14. Debug and fix the code. Hint: there are three separate errors.\n\ndata(breastcancer)\n\nmodel &lt;- glm(diagnosis ~ smoothness_worst + symmetry_mean + \n           texture_se + radius_mean, \n         data = breastcancer, family = binomial)\n\npred_probs &lt;- predict(model)\n\nnum_cuts &lt;- 10\ncalib_data &lt;-  data.frame(prob = pred_probs,\n                      bin = cut(pred_probs, breaks = num_cuts),\n                      class = mod_start$y)\n\ncalib_data &lt;- calib_data %&gt;% \n         group_by(bin) %&gt;% \n         summarize(observed = sum(class)/n(), \n                   expected = sum(prob)/n(), \n                   se = sqrt(observed * (1 - observed) / n()))\ncalib_data\n\nggplot(calib_data) + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  geom_errorbar(aes(x = expected, ymin = observed - 1.96 * se, \n                ymax = observed + 1.96 * se), \n            colour = \"black\", width = .01)+\n  geom_point(aes(x = expected, y = observed)) +\n  labs(x=\"Expected Proportion\", y=\"Observed Proportion\") +\n  theme_minimal()\n\n\n\n\n\n\nGue, Ying X., Krishma Adatia, Rahim Kanji, Tatjana Potpara, Gregory Y. H. Lip, and Diana A. Gorog. 2021. “Out-of-Hospital Cardiac Arrest: A Systematic Review of Current Risk Scores to Predict Survival.” American Heart Journal 234: 31–41. https://doi.org/https://doi.org/10.1016/j.ahj.2020.12.011.\n\n\nWickham, Hadley. 2022. stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Expanding your R Skills</span>"
    ]
  },
  {
    "objectID": "book/quarto_reports.html#starting-a-quarto-file",
    "href": "book/quarto_reports.html#starting-a-quarto-file",
    "title": "22  Writing Reports in Quarto",
    "section": "22.1 Starting a Quarto file",
    "text": "22.1 Starting a Quarto file\nTo create a Quarto file, you need to have RStudio installed as an application. For more recent versions of RStudio, Quarto is already installed. If you have an older version, you can install Quarto. We also recommend the kableExtra package (Zhu 2021) for formatting your tables.\nNow that you have these packages downloaded, opening a new Quarto file is very similar to opening a new R file, which was covered in Chapter 1. Just like opening a new R file, you’ll want to go to File -&gt; New File, but instead of selecting ‘R Script’, you’ll now select ‘Quarto Document…’. This should bring up a window that looks like Figure 22.1.\n\n\n\n\n\n\nFigure 22.1: Creating a New R Quarto Document.\n\n\n\nFirst, enter a title of your choosing for your report and type your name in the Author field - note that you can always change these later - and then click on OK. You should also choose which type of file you would like to generate, a PDF, HTML, or WORD document. This opens an Quarto file that has the extension .qmd. Make sure to save this file with a suitable name in your desired location on your computer by selecting File -&gt; Save, and then you’re ready to start writing your report! Your file should now look like Figure 22.2.\n\n\n\n\n\n\nFigure 22.2: A New Quarto Document.\n\n\n\nAt the top of this pane is a toggle between source and visual mode. In visual mode, we can see that we have a text toolbar including options to bold text or add a list. If we switch to source mode, our text reveals the underlying markdown in Figure 22.3 and the toolbar disappears. This chapter focuses on teaching you to edit in source mode, but you can always switch to visual mode if you prefer.\n\n\n\n\n\n\nFigure 22.3: Source Mode.\n\n\n\nWe write all of the text and code that we would like to include in your report in this .qmd file, and then produce a nicely formatted report from this file by ‘rendering’ the file. You can either render to HTML, PDF, or WORD by clicking on the Render button  from the toolbar at the top of the page. To update our format from PDF, we change the text in the top block to format: html or format: word.\n\n22.1.1 Adding Code Chunks\nEach of the darker gray rectangles is called a code chunk - all of the code used to generate your report goes in these chunks, and all of your text writing goes outside of them. Each code chunk starts with ```{r} and ends with ```. To create a chunk, you can either\n\nclick on this green “add chunk” symbol  in the toolbar at the top of the page,\ntype ```{r} and ```, or\nuse the keyboard shortcut Ctrl + Alt + I (Cmd + Option + I on Macs).\n\nTo run the code in a chunk, you can either use the keyboard shortcut Ctrl + Enter (Cmd + Return on Mac), or you can use one of the following buttons at the top right of the chunks:  runs all chunks above the current chunk and  runs the current chunk.\n\n\n22.1.2 Customizing Chunks\nYou can specify whether you want to include the code and/or its various output in your final report by adding the following commands, preceded by #|, at the top of the code chunk:\n\ninclude: false makes it so that neither code nor its output appears in your report.\necho: false makes it so that the output of the code but not the code itself appears in your report.\nmessage: false, warning: false, and error: false make it so that messages, warnings, and errors (respectively) that are generated from the code in the chunk won’t appear in your report.\n\nHere we can also specify a label for our code chunk. For example, if we wanted to include a chunk that displayed our code but did not execute it, we could include the following.\n  ```{r}\n#| label: example-chunk\n#| echo: true\n#| eval: false\n\nx &lt;- c(1,2,3)\n```\nTo apply the same options to all chunks in the document at once, you can add them to the first chunk at the very top of your Quarto that has the label label: setup using the knitr::opts_chunk$set() function. These are called the global settings. For example, using the following code for your first code chunk ensures that none of the errors, warnings, or messages from any of the code chunks appear in your final report. It is also good practice to load all the packages you are using for your report within this first code chunk using the library() function. For example, we load the tidyverse and HDSinRdata packages.\n  ```{r}\n|# label: setup\n|# include: false\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, \n error = FALSE, echo = FALSE)\nlibrary(tidyverse)\nlibrary(HDSinRdata)\n```\nIf you want to display the code for your report in a code appendix, you can easily do this by creating an empty code chunk at the end of your .qmd file that looks like the following. This finds all other chunks and displays the code.\n  ```{r ref.label = knitr::all_labels()}\n#| echo: true\n#| eval: false\n```",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Writing Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "book/quarto_reports.html#formatting-text-in-markdown",
    "href": "book/quarto_reports.html#formatting-text-in-markdown",
    "title": "22  Writing Reports in Quarto",
    "section": "22.2 Formatting Text in Markdown",
    "text": "22.2 Formatting Text in Markdown\nTo add text to your report, you can simply type directly into the Quarto file, between the code chunks. This code is formatted using markdown, which allows us to specify how to format and display the text when it is knit. For example, adding a single asterisk * on either side of some text italicizes it, while adding a double asterisk ** on either side of text makes it bold. To indicate code, you can use backticks `.\nregular text regular text\n*italicized text* italicized text\n**bold text** bold text\n`code text` code text\nTo create headers and sections, you can add the # symbol in front of your text. Adding more of these symbols makes the headers smaller, which is useful for making sub-headers.\n# Header\n## Smaller Header\n### Even Smaller Header\n\n\n\n\n\n\nFigure 22.4: Example header sizes.\n\n\n\nYou can also add links [text](www.example.com) and images ![alt text](#fig-label image.png). In the latter example, fig-label becomes the label of the image we can use to cross-reference it while image.png is the image file name.\nExample link.\n\n\n\nExample Image.\n\n\nThe Markdown Guide has a great cheat sheet as well as more resources for formatting markdown text.\nYou can also have inline R code by using single backticks around your code `{r} max(c(1,2,3))`. The code must start with r to be run when knit. This allows you to reference variables in your text. For example, we could display the variance of a column in our data without having to copy the value over `{r} round(var(cars$speed),2)`.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Writing Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "book/quarto_reports.html#formatting-figures-and-tables",
    "href": "book/quarto_reports.html#formatting-figures-and-tables",
    "title": "22  Writing Reports in Quarto",
    "section": "22.3 Formatting Figures and Tables",
    "text": "22.3 Formatting Figures and Tables\nOften, you’ll want to include figures generated by your code in your report, and you can customize these figures by changing the chunk options for the chunks that produce them. To change the size of a figure, you can add in the chunk option fig-width: 3 with your desired size in inches. To add a nice caption to a figure in your report, you can add fig-cap : 'Your Desired Caption.' option. To name a figure, you want to start your label with fig- such as fig-myfigure.\nBy default, the figures generated by your code chunks are allowed to ‘float’ in Quarto. This means that the figures might move away from where they were coded or referenced in the final report. To prevent this behavior, you can customize the chunk that contains the code to produce the figure by adding fig-pos : 'H' to that chunk’s options. If you want to prevent floating for all figures, add fig-pos : 'H' to the first code chunk in the file (the one that starts with the knitr::opts_chunk$set() function).\n  ```{r}\n  #| label: fig-myfigure\n  #| fig-width: 3\n  #| fig-pos: \"H\"\n  #| fig-cap: \"Blood Lead Level by Education\"\n\n  data(NHANESsample)\n  ggplot(subset(NHANESsample, !is.na(EDUCATION))) + \n  geom_boxplot(aes(x = EDUCATION, y = log(LEAD)), fill = 'lightblue') +\n  theme_bw() + \n  labs(y = \"Log Blood Lead Level\") + \n  scale_x_discrete(\"Education\")\n  ```\n\n\n\n\n\nIf you want to make data frames, matrices, or tables from your raw R output more polished and aesthetically pleasing, you can use the gt and kableExtra packages. Be sure to load the package you are using to the code chunk at the top of your Quarto file that contains all of your libraries.\nTo demonstrate the abilities of these packages, let’s suppose that we wanted to display the head of the first few columns of the NHANESsample data from the HDSinRdata package. The following code produces the corresponding output in the knitted pdf report. You can see that it essentially just copies the raw output from R, which is rather messy.\n\n\n\nTable 22.1\n\n\nNHANESsample %&gt;% \n  select(AGE, SEX, EDUCATION, INCOME, LEAD) %&gt;%   \n  head()\n#&gt;   AGE    SEX  EDUCATION INCOME LEAD\n#&gt; 1  77   Male MoreThanHS   5.00  5.0\n#&gt; 2  49   Male MoreThanHS   5.00  1.6\n#&gt; 3  37   Male MoreThanHS   4.93  2.4\n#&gt; 4  70   Male LessThanHS   1.07  1.6\n#&gt; 5  81   Male LessThanHS   2.67  5.5\n#&gt; 6  38 Female MoreThanHS   4.52  1.5\n\n\n\nWe use the kable() and kable_styling() functions from the kableExtra package to produce a more nicely formatted table. The kable() function generates a table from a data frame. The kable() function allows you to specify some display options for your table. For example, you can add a caption to your table using the caption argument, and you can change the names of the columns in the table using the col.names argument. The kable_styling() has additional options available. Similar to the fig.pos = H command described for figures in the previous section, adding “HOLD_position” to the kable_styling() function prevents the table from floating on the report; adding \"scale_down\" scales the table so that it fits in the margins of the paper. The updated code and output are shown in the following code chunk. See the documentation for the kable() and kable_styling() functions for more options available.\n\nNHANESsample %&gt;% \n  select(AGE, SEX, EDUCATION, INCOME, LEAD) %&gt;% \n  head() %&gt;% \n  kable(col.names = c(\"Age\", \"Sex\", \"Education Level\", \n                      \"Poverty Income Ratio\", \"Lead Level\")) %&gt;%   \n  kable_styling(latex_options = c(\"scale_down\", \"HOLD_position\"))\n\n\n\nTable 22.2: Head of the NHANES Sample Data\n\n\n\n\n\n\nAge\nSex\nEducation Level\nPoverty Income Ratio\nLead Level\n\n\n\n\n77\nMale\nMoreThanHS\n5.00\n5.0\n\n\n49\nMale\nMoreThanHS\n5.00\n1.6\n\n\n37\nMale\nMoreThanHS\n4.93\n2.4\n\n\n70\nMale\nLessThanHS\n1.07\n1.6\n\n\n81\nMale\nLessThanHS\n2.67\n5.5\n\n\n38\nFemale\nMoreThanHS\n4.52\n1.5\n\n\n\n\n\n\n\n\n\n\nIn the previous could chunk, we saw that kable() produces a much nicer table in the knitted pdf that is more suitable for a data analysis report. In Chapter 4, we also introduced the gt package. This package is an alternative package to kableExtra that allows you to format each part of the table and includes options for formatting the columns, adding footers or subtitles, or grouping your table. See the package introduction for more details about this package. An example gt table is given in the following code and output. Note that for tables, we want to start our label with tbl- and can include a caption using the tbl-cap option.\n  ```{r}\n  #| label: tbl-gt-ex\n  #| tbl-cap: \"Head of the NHANES Sample Data\"\n\n  NHANESsample %&gt;% \n  select(AGE, SEX, EDUCATION, INCOME, LEAD) %&gt;% \n  head() %&gt;% \n  gt() %&gt;%\n  tab_header(title = \"Head of the NHANES Sample Data\") %&gt;%\n  cols_label(AGE ~ \"Age\", \n      SEX ~ \"Sex\", \n      EDUCATION ~ \"Education Level\", \n      INCOME ~ \"Poverty Income Ratio\",\n      LEAD ~ \"Lead Level\")\n  ```\n\n\n\n\n\n\n22.3.1 Using References\nQuarto automatically adds figure and table numbers to the figures and tables in your report. By using the label options, we can also reference our figures easily by using their names: @fig-figname or @tab-tablename. The knitted pdf substitutes the appropriate figure or table number into your text. Additionally, we can reference sections by adding in labels to the section header. For example, we added the tag #sec-awesome for the section in the following text and can now reference it using @sec-awesome.\n## Awesome Stuff {#sec-awesome}",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Writing Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "book/quarto_reports.html#adding-in-equations",
    "href": "book/quarto_reports.html#adding-in-equations",
    "title": "22  Writing Reports in Quarto",
    "section": "22.4 Adding in Equations",
    "text": "22.4 Adding in Equations\nAnother useful option in markdown is the option to add in mathematical equations. If you want to insert math equations, you can do so by writing LaTeX expressions. To write a math equation inline, you put a single dollar sign $ on either side of your equation, and to write a math equation on its own line, you put a double dollar sign $$ on either side of the equation, like so:\nHere’s an equation that is inline with the text: $5x^2 + 9x^3$ produces \\(5x^2 + 9x^3\\). On the other hand, here’s an equation that is on its own line: $$5x^2 + 9x^3$$ produces \\[5x^2 + 9x^3\\]\nHere is some other LaTeX notation that you should know in order to write common equations: * To create a fraction, type \\frac{numerator}{denominator}. For example, \\frac{2}{3} produces \\(\\frac{2}{3}\\). * To create a subscript, type _. For example, x_{2} produces \\(x_2\\). * To create a superscript, type ^. For example, x^{2} produces \\(x^2\\).\nIf you want to learn more about how to write in LaTeX, Art of Problem Solving provides a great reference for LaTeX symbols and Overleaf provides a helpful introduction to LaTeX in general.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Writing Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "book/quarto_reports.html#exercises",
    "href": "book/quarto_reports.html#exercises",
    "title": "22  Writing Reports in Quarto",
    "section": "22.5 Exercises",
    "text": "22.5 Exercises\nThe exercise for this chapter is to recreate this example pdf created from an Quarto file. You will need to use the NHANESsample data from the HDSinRdata package.\n\n\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with Kable and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Extra Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Writing Reports in Quarto</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "AJMC Staff. 2021. “A Timeline of COVID-19 Developments in\n2020.” https://www.ajmc.com/view/a-timeline-of-covid19-developments-in-2020.\n\n\nAlter, Benedict J, Nathan P Anderson, Andrea G Gillman, Qing Yin,\nJong-Hyeon Jeong, and Ajay D Wasan. 2021. “Hierarchical Clustering\nby Patient-Reported Pain Distribution Alone Identifies Distinct Chronic\nPain Subgroups Differing by Pain Intensity, Quality, and Clinical\nOutcomes.” PLoS One 16 (8): e0254862.\n\n\nBaik, Yeonsoo, Hannah M Rickman, Colleen F Hanrahan, Lesego Mmolawa,\nPeter J Kitonsa, Tsundzukana Sewelana, Annet Nalutaaya, et al. 2020.\n“A Clinical Score for Identifying Active Tuberculosis While\nAwaiting Microbiological Results: Development and Validation of a\nMultivariable Prediction Model in Sub-Saharan Africa.” PLoS\nMedicine 17 (11): e1003420.\n\n\nCenters for Disease Control and Prevention (CDC). 1999-2018.\n“National Health and Nutrition Examination\nSurvey Data (NHANES).” U.S. Department of Health; Human\nServices. http://www.cdc.gov/nchs/nhanes.htm.\n\n\n———. 2021. “National Youth Tobacco Survey\n(NYTS).” U.S. Department of Health; Human Services. https://www.cdc.gov/tobacco/data_statistics/surveys/nyts/index.htm.\n\n\nDi Lorenzo, Paolo. 2024. Usmap: US Maps Including Alaska and\nHawaii. https://CRAN.R-project.org/package=usmap.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2023. car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2010.\n“Regularization Paths for Generalized Linear Models via Coordinate\nDescent.” Journal of Statistical Software 33 (1): 1–22.\nhttps://doi.org/10.18637/jss.v033.i01.\n\n\nGue, Ying X., Krishma Adatia, Rahim Kanji, Tatjana Potpara, Gregory Y.\nH. Lip, and Diana A. Gorog. 2021. “Out-of-Hospital Cardiac Arrest:\nA Systematic Review of Current Risk Scores to Predict Survival.”\nAmerican Heart Journal 234: 31–41. https://doi.org/https://doi.org/10.1016/j.ahj.2020.12.011.\n\n\nGuidotti, Emanuele. 2022. “A Worldwide Epidemiological Database\nfor COVID-19 at Fine-Grained Spatial Resolution.” Scientific\nData 9 (1): 112. https://doi.org/10.1038/s41597-022-01245-1.\n\n\nGuidotti, Emanuele, and David Ardia. 2020. “COVID-19 Data\nHub.” Journal of Open Source Software 5 (51): 2376. https://doi.org/10.21105/joss.02376.\n\n\nHastie, Trevor, Robert Tibshirani, and Ryan Tibshirani. 2020.\n“Best Subset, Forward Stepwise or Lasso? Analysis and\nRecommendations Based on Extensive Comparisons.” Statistical\nScience 35 (4).\n\n\nHazimeh, Hussein, Rahul Mazumder, and Tim Nonet. 2023. L0Learn: Fast\nAlgorithms for Best Subset Selection. https://CRAN.R-project.org/package=L0Learn.\n\n\nHothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint\nCummins. 2022. lmtest: Testing Linear\nRegression Models. https://CRAN.R-project.org/package=lmtest.\n\n\nHuang, Ziyao. 2022. “Association Between\nBlood Lead Level with High Blood Pressure in US (NHANES\n1999–2018).” Frontiers in Public Health 10:\n836357.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra\nLauer, and JooYoung Seo. 2023. gt: Easily\nCreate Presentation-Ready Display Tables, Url = https://CRAN.R-project.org/package=gt.\n\n\nKortsmit, Katherine. 2023. “Abortion Surveillance—United States,\n2021.” MMWR. Surveillance Summaries 72.\n\n\nMersmann, Olaf. 2023. Microbenchmark: Accurate Timing\nFunctions. https://CRAN.R-project.org/package=microbenchmark.\n\n\nMüller, Kirill. 2023. Hms: Pretty Time of Day. https://CRAN.R-project.org/package=hms.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nPark-Lee, Eunice, Andrea S Gentzke, Chunfeng Ren, Maria Cooper, Michael\nD Sawdey, S Sean Hu, and Karen A Cullen. 2023. “Impact of Survey\nSetting on Current Tobacco Product Use: National Youth Tobacco Survey,\n2021.” Journal of Adolescent Health 72 (3): 365–74.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The\nComposer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRaifman, Julia, Kristen Nocka, David Jones, Jacob Bor, Sarah Lipson,\nJonathan Jay, Megan Cole, et al. 2022. “COVID-19 US State\nPolicy Database.” Inter-university Consortium for\nPolitical; Social Research. https://doi.org/10.3886/E119446V143.\n\n\nRobin, Xavier, Natacha Turck, Alexandre Hainard, Natalia Tiberti,\nFrédérique Lisacek, Jean-Charles Sanchez, and Markus Müller. 2023.\npROC: Display and Analyze ROC Curves. http://expasy.org/tools/pROC/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. broom: Convert Statistical Objects into Tidy\nTibbles. https://CRAN.R-project.org/package=broom.\n\n\nRoser, Max, and Hannah Ritchie. 2013. “Maternal Mortality.”\nhttps://ourworldindata.org/maternal-mortality.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally.\n\n\nSjoberg, Daniel D., Joseph Larmarange, Michael Curry, Jessica Lavery,\nKarissa Whiting, and Emily C. Zabor. 2023. gtsummary: Presentation-Ready Data Summary and Analytic\nResult Tables. https://CRAN.R-project.org/package=gtsummary.\n\n\nSpinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2023. lubridate: Make Dealing with Dates a Little\nEasier. https://CRAN.R-project.org/package=lubridate.\n\n\nTexas Health & Human Services Commission. 2016-2021. “Induced\nTerminations of Pregnancy.” Texas Department of State Health\nServices. https://www.hhs.texas.gov/about/records-statistics/data-statistics/itop-statistics.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics\nwith s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nWarren, Michael S, and Samuel W Skillman. 2020. “Mobility Changes\nin Response to COVID-19.” arXiv Preprint\narXiv:2003.14228.\n\n\nWickham, Hadley. 2011. “Testthat: Get Started with\nTesting.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2022. stringr: Simple, Consistent Wrappers\nfor Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023. tidyverse: Easily Install and Load\nthe Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. dplyr: A Grammar of Data\nManipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’\nFiles. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nYoshida, Kazuki, and Alexander Bartel. 2022. Tableone: Create ’Table\n1’ to Describe Baseline Characteristics with or Without Propensity Score\nWeights. https://CRAN.R-project.org/package=tableone.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with Kable and\nPipe Syntax. https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "References"
    ]
  }
]